{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000009\n",
      "000017\n",
      "000021\n",
      "000023\n",
      "000030\n",
      "000032\n",
      "000035\n",
      "000041\n",
      "000048\n",
      "000050\n",
      "000051\n",
      "000066\n",
      "000073\n",
      "000081\n",
      "000083\n",
      "000089\n",
      "000101\n",
      "000104\n",
      "000110\n",
      "000113\n",
      "000125\n",
      "000129\n",
      "000131\n",
      "000133\n",
      "000138\n",
      "000146\n",
      "000150\n",
      "000159\n",
      "000162\n",
      "000163\n",
      "000164\n",
      "000165\n",
      "000169\n",
      "000170\n",
      "000171\n",
      "000173\n",
      "000174\n",
      "000177\n",
      "000190\n",
      "000192\n",
      "000193\n",
      "000194\n",
      "000200\n",
      "000210\n",
      "000218\n",
      "000220\n",
      "000222\n",
      "000229\n",
      "000232\n",
      "000245\n",
      "000251\n",
      "000257\n",
      "000259\n",
      "000269\n",
      "000275\n",
      "000276\n",
      "000278\n",
      "000282\n",
      "000285\n",
      "000288\n",
      "000298\n",
      "000302\n",
      "000305\n",
      "000308\n",
      "000320\n",
      "000321\n",
      "000322\n",
      "000323\n",
      "000328\n",
      "000331\n",
      "000337\n",
      "000338\n",
      "000352\n",
      "000359\n",
      "000367\n",
      "000372\n",
      "000374\n",
      "000382\n",
      "000394\n",
      "000406\n",
      "000407\n",
      "000411\n",
      "000419\n",
      "000428\n",
      "000433\n",
      "000435\n",
      "000438\n",
      "000443\n",
      "000446\n",
      "000448\n",
      "000463\n",
      "000468\n",
      "000470\n",
      "000476\n",
      "000477\n",
      "000480\n",
      "000482\n",
      "000483\n",
      "000498\n",
      "000499\n",
      "000500\n",
      "000515\n",
      "000516\n",
      "000518\n",
      "000520\n",
      "000523\n",
      "000524\n",
      "000525\n",
      "000526\n",
      "000530\n",
      "000531\n",
      "000535\n",
      "000541\n",
      "000545\n",
      "000554\n",
      "000555\n",
      "000579\n",
      "000583\n",
      "000589\n",
      "000591\n",
      "000597\n",
      "000612\n",
      "000613\n",
      "000625\n",
      "000626\n",
      "000628\n",
      "000633\n",
      "000648\n",
      "000654\n",
      "000677\n",
      "000684\n",
      "000688\n",
      "000690\n",
      "000694\n",
      "000695\n",
      "000702\n",
      "000709\n",
      "000717\n",
      "000726\n",
      "000731\n",
      "000733\n",
      "000739\n",
      "000742\n",
      "000750\n",
      "000752\n",
      "000753\n",
      "000755\n",
      "000760\n",
      "000770\n",
      "000777\n",
      "000782\n",
      "000786\n",
      "000793\n",
      "000797\n",
      "000799\n",
      "000802\n",
      "000805\n",
      "000806\n",
      "000810\n",
      "000812\n",
      "000814\n",
      "000816\n",
      "000828\n",
      "000829\n",
      "000834\n",
      "000842\n",
      "000843\n",
      "000847\n",
      "000848\n",
      "000851\n",
      "000854\n",
      "000855\n",
      "000859\n",
      "000860\n",
      "000862\n",
      "000865\n",
      "000874\n",
      "000878\n",
      "000879\n",
      "000885\n",
      "000892\n",
      "000895\n",
      "000898\n",
      "000902\n",
      "000903\n",
      "000904\n",
      "000906\n",
      "000911\n",
      "000915\n",
      "000918\n",
      "000920\n",
      "000926\n",
      "000937\n",
      "000943\n",
      "000948\n",
      "000949\n",
      "000966\n",
      "000967\n",
      "000971\n",
      "000982\n",
      "000987\n",
      "000991\n",
      "000996\n",
      "000999\n",
      "001001\n",
      "001011\n",
      "001014\n",
      "001017\n",
      "001024\n",
      "001028\n",
      "001036\n",
      "001042\n",
      "001050\n",
      "001057\n",
      "001060\n",
      "001061\n",
      "001064\n",
      "001066\n",
      "001071\n",
      "001072\n",
      "001079\n",
      "001084\n",
      "001091\n",
      "001092\n",
      "001097\n",
      "001101\n",
      "001109\n",
      "001113\n",
      "001125\n",
      "001129\n",
      "001137\n",
      "001140\n",
      "001145\n",
      "001147\n",
      "001151\n",
      "001152\n",
      "001164\n",
      "001168\n",
      "001170\n",
      "001171\n",
      "001175\n",
      "001185\n",
      "001186\n",
      "001201\n",
      "001206\n",
      "001212\n",
      "001221\n",
      "001224\n",
      "001229\n",
      "001234\n",
      "001236\n",
      "001240\n",
      "001241\n",
      "001248\n",
      "001254\n",
      "001259\n",
      "001265\n",
      "001266\n",
      "001269\n",
      "001272\n",
      "001279\n",
      "001281\n",
      "001284\n",
      "001287\n",
      "001292\n",
      "001298\n",
      "001304\n",
      "001309\n",
      "001310\n",
      "001311\n",
      "001315\n",
      "001325\n",
      "001327\n",
      "001330\n",
      "001333\n",
      "001337\n",
      "001346\n",
      "001350\n",
      "001352\n",
      "001362\n",
      "001378\n",
      "001388\n",
      "001390\n",
      "001393\n",
      "001405\n",
      "001406\n",
      "001408\n",
      "001409\n",
      "001414\n",
      "001420\n",
      "001421\n",
      "001426\n",
      "001427\n",
      "001430\n",
      "001434\n",
      "001445\n",
      "001450\n",
      "001451\n",
      "001455\n",
      "001460\n",
      "001463\n",
      "001472\n",
      "001475\n",
      "001479\n",
      "001480\n",
      "001484\n",
      "001485\n",
      "001493\n",
      "001498\n",
      "001499\n",
      "001501\n",
      "001504\n",
      "001509\n",
      "001510\n",
      "001514\n",
      "001521\n",
      "001523\n",
      "001524\n",
      "001526\n",
      "001531\n",
      "001532\n",
      "001536\n",
      "001537\n",
      "001544\n",
      "001548\n",
      "001554\n",
      "001557\n",
      "001561\n",
      "001563\n",
      "001571\n",
      "001577\n",
      "001579\n",
      "001580\n",
      "001586\n",
      "001593\n",
      "001594\n",
      "001603\n",
      "001608\n",
      "001611\n",
      "001617\n",
      "001622\n",
      "001627\n",
      "001628\n",
      "001630\n",
      "001640\n",
      "001649\n",
      "001651\n",
      "001654\n",
      "001678\n",
      "001682\n",
      "001686\n",
      "001689\n",
      "001690\n",
      "001691\n",
      "001711\n",
      "001717\n",
      "001721\n",
      "001723\n",
      "001725\n",
      "001726\n",
      "001727\n",
      "001729\n",
      "001730\n",
      "001749\n",
      "001754\n",
      "001756\n",
      "001758\n",
      "001759\n",
      "001766\n",
      "001772\n",
      "001775\n",
      "001784\n",
      "001785\n",
      "001789\n",
      "001793\n",
      "001795\n",
      "001797\n",
      "001799\n",
      "001800\n",
      "001801\n",
      "001806\n",
      "001807\n",
      "001809\n",
      "001810\n",
      "001828\n",
      "001830\n",
      "001832\n",
      "001833\n",
      "001836\n",
      "001837\n",
      "001841\n",
      "001843\n",
      "001849\n",
      "001854\n",
      "001861\n",
      "001864\n",
      "001881\n",
      "001887\n",
      "001894\n",
      "001899\n",
      "001902\n",
      "001903\n",
      "001904\n",
      "001918\n",
      "001920\n",
      "001927\n",
      "001930\n",
      "001932\n",
      "001938\n",
      "001941\n",
      "001944\n",
      "001945\n",
      "001950\n",
      "001958\n",
      "001963\n",
      "001977\n",
      "001978\n",
      "001980\n",
      "001989\n",
      "001999\n",
      "002000\n",
      "002012\n",
      "002022\n",
      "002024\n",
      "002030\n",
      "002051\n",
      "002055\n",
      "002061\n",
      "002069\n",
      "002082\n",
      "002090\n",
      "002091\n",
      "002096\n",
      "002104\n",
      "002108\n",
      "002114\n",
      "002116\n",
      "002117\n",
      "002120\n",
      "002142\n",
      "002145\n",
      "002156\n",
      "002163\n",
      "002169\n",
      "002170\n",
      "002171\n",
      "002172\n",
      "002174\n",
      "002176\n",
      "002180\n",
      "002183\n",
      "002186\n",
      "002187\n",
      "002190\n",
      "002193\n",
      "002194\n",
      "002213\n",
      "002214\n",
      "002219\n",
      "002220\n",
      "002221\n",
      "002224\n",
      "002226\n",
      "002233\n",
      "002238\n",
      "002241\n",
      "002244\n",
      "002256\n",
      "002261\n",
      "002265\n",
      "002272\n",
      "002273\n",
      "002277\n",
      "002281\n",
      "002285\n",
      "002288\n",
      "002290\n",
      "002293\n",
      "002302\n",
      "002306\n",
      "002307\n",
      "002311\n",
      "002324\n",
      "002329\n",
      "002332\n",
      "002337\n",
      "002342\n",
      "002361\n",
      "002362\n",
      "002367\n",
      "002372\n",
      "002378\n",
      "002382\n",
      "002384\n",
      "002385\n",
      "002387\n",
      "002401\n",
      "002403\n",
      "002405\n",
      "002410\n",
      "002413\n",
      "002419\n",
      "002425\n",
      "002433\n",
      "002443\n",
      "002444\n",
      "002448\n",
      "002450\n",
      "002458\n",
      "002459\n",
      "002460\n",
      "002472\n",
      "002476\n",
      "002477\n",
      "002481\n",
      "002483\n",
      "002491\n",
      "002497\n",
      "002501\n",
      "002504\n",
      "002514\n",
      "002518\n",
      "002520\n",
      "002524\n",
      "002529\n",
      "002533\n",
      "002539\n",
      "002540\n",
      "002542\n",
      "002545\n",
      "002554\n",
      "002555\n",
      "002558\n",
      "002561\n",
      "002563\n",
      "002564\n",
      "002565\n",
      "002566\n",
      "002572\n",
      "002584\n",
      "002585\n",
      "002589\n",
      "002595\n",
      "002611\n",
      "002613\n",
      "002625\n",
      "002632\n",
      "002633\n",
      "002636\n",
      "002637\n",
      "002641\n",
      "002643\n",
      "002649\n",
      "002657\n",
      "002658\n",
      "002662\n",
      "002668\n",
      "002669\n",
      "002675\n",
      "002678\n",
      "002689\n",
      "002691\n",
      "002693\n",
      "002696\n",
      "002710\n",
      "002713\n",
      "002714\n",
      "002715\n",
      "002723\n",
      "002727\n",
      "002730\n",
      "002735\n",
      "002741\n",
      "002744\n",
      "002751\n",
      "002755\n",
      "002759\n",
      "002767\n",
      "002776\n",
      "002786\n",
      "002795\n",
      "002798\n",
      "002803\n",
      "002804\n",
      "002807\n",
      "002815\n",
      "002826\n",
      "002827\n",
      "002834\n",
      "002838\n",
      "002841\n",
      "002842\n",
      "002855\n",
      "002858\n",
      "002859\n",
      "002864\n",
      "002867\n",
      "002870\n",
      "002879\n",
      "002881\n",
      "002884\n",
      "002886\n",
      "002889\n",
      "002899\n",
      "002901\n",
      "002912\n",
      "002913\n",
      "002914\n",
      "002915\n",
      "002916\n",
      "002931\n",
      "002932\n",
      "002933\n",
      "002935\n",
      "002939\n",
      "002940\n",
      "002954\n",
      "002958\n",
      "002960\n",
      "002962\n",
      "002965\n",
      "002975\n",
      "002976\n",
      "002987\n",
      "002988\n",
      "002990\n",
      "002992\n",
      "003002\n",
      "003003\n",
      "003017\n",
      "003023\n",
      "003024\n",
      "003027\n",
      "003031\n",
      "003034\n",
      "003039\n",
      "003042\n",
      "003044\n",
      "003057\n",
      "003058\n",
      "003063\n",
      "003064\n",
      "003065\n",
      "003077\n",
      "003078\n",
      "003086\n",
      "003088\n",
      "003089\n",
      "003100\n",
      "003102\n",
      "003106\n",
      "003107\n",
      "003118\n",
      "003121\n",
      "003122\n",
      "003124\n",
      "003129\n",
      "003134\n",
      "003138\n",
      "003140\n",
      "003145\n",
      "003147\n",
      "003150\n",
      "003159\n",
      "003162\n",
      "003170\n",
      "003175\n",
      "003181\n",
      "003185\n",
      "003188\n",
      "003189\n",
      "003205\n",
      "003210\n",
      "003213\n",
      "003218\n",
      "003228\n",
      "003236\n",
      "003247\n",
      "003253\n",
      "003259\n",
      "003260\n",
      "003261\n",
      "003270\n",
      "003280\n",
      "003282\n",
      "003284\n",
      "003290\n",
      "003292\n",
      "003296\n",
      "003301\n",
      "003303\n",
      "003311\n",
      "003313\n",
      "003320\n",
      "003327\n",
      "003331\n",
      "003335\n",
      "003344\n",
      "003350\n",
      "003355\n",
      "003360\n",
      "003362\n",
      "003367\n",
      "003369\n",
      "003373\n",
      "003376\n",
      "003380\n",
      "003382\n",
      "003386\n",
      "003390\n",
      "003391\n",
      "003395\n",
      "003396\n",
      "003397\n",
      "003403\n",
      "003406\n",
      "003407\n",
      "003412\n",
      "003416\n",
      "003417\n",
      "003422\n",
      "003424\n",
      "003429\n",
      "003433\n",
      "003435\n",
      "003436\n",
      "003450\n",
      "003453\n",
      "003455\n",
      "003462\n",
      "003465\n",
      "003466\n",
      "003468\n",
      "003469\n",
      "003470\n",
      "003484\n",
      "003487\n",
      "003492\n",
      "003496\n",
      "003510\n",
      "003518\n",
      "003521\n",
      "003528\n",
      "003529\n",
      "003530\n",
      "003536\n",
      "003539\n",
      "003548\n",
      "003550\n",
      "003556\n",
      "003566\n",
      "003567\n",
      "003575\n",
      "003576\n",
      "003577\n",
      "003580\n",
      "003585\n",
      "003589\n",
      "003603\n",
      "003611\n",
      "003618\n",
      "003621\n",
      "003625\n",
      "003628\n",
      "003638\n",
      "003644\n",
      "003645\n",
      "003648\n",
      "003651\n",
      "003654\n",
      "003655\n",
      "003662\n",
      "003663\n",
      "003664\n",
      "003669\n",
      "003678\n",
      "003685\n",
      "003688\n",
      "003690\n",
      "003691\n",
      "003694\n",
      "003696\n",
      "003699\n",
      "003700\n",
      "003703\n",
      "003706\n",
      "003708\n",
      "003711\n",
      "003717\n",
      "003732\n",
      "003735\n",
      "003740\n",
      "003749\n",
      "003751\n",
      "003759\n",
      "003760\n",
      "003779\n",
      "003780\n",
      "003781\n",
      "003797\n",
      "003811\n",
      "003817\n",
      "003820\n",
      "003824\n",
      "003826\n",
      "003827\n",
      "003838\n",
      "003847\n",
      "003849\n",
      "003855\n",
      "003856\n",
      "003859\n",
      "003860\n",
      "003863\n",
      "003865\n",
      "003868\n",
      "003869\n",
      "003877\n",
      "003879\n",
      "003889\n",
      "003891\n",
      "003899\n",
      "003913\n",
      "003915\n",
      "003923\n",
      "003932\n",
      "003939\n",
      "003945\n",
      "003946\n",
      "003948\n",
      "003953\n",
      "003960\n",
      "003961\n",
      "003963\n",
      "003966\n",
      "003970\n",
      "003973\n",
      "003974\n",
      "003986\n",
      "003992\n",
      "003994\n",
      "003996\n",
      "004008\n",
      "004010\n",
      "004012\n",
      "004014\n",
      "004016\n",
      "004017\n",
      "004019\n",
      "004020\n",
      "004028\n",
      "004031\n",
      "004033\n",
      "004039\n",
      "004046\n",
      "004047\n",
      "004051\n",
      "004058\n",
      "004060\n",
      "004067\n",
      "004073\n",
      "004075\n",
      "004077\n",
      "004085\n",
      "004089\n",
      "004091\n",
      "004092\n",
      "004110\n",
      "004111\n",
      "004117\n",
      "004121\n",
      "004122\n",
      "004133\n",
      "004136\n",
      "004137\n",
      "004138\n",
      "004141\n",
      "004142\n",
      "004145\n",
      "004158\n",
      "004168\n",
      "004169\n",
      "004170\n",
      "004191\n",
      "004195\n",
      "004201\n",
      "004205\n",
      "004209\n",
      "004223\n",
      "004224\n",
      "004229\n",
      "004230\n",
      "004231\n",
      "004232\n",
      "004237\n",
      "004244\n",
      "004246\n",
      "004257\n",
      "004258\n",
      "004259\n",
      "004265\n",
      "004269\n",
      "004270\n",
      "004273\n",
      "004275\n",
      "004279\n",
      "004281\n",
      "004287\n",
      "004291\n",
      "004292\n",
      "004296\n",
      "004298\n",
      "004304\n",
      "004321\n",
      "004322\n",
      "004323\n",
      "004327\n",
      "004329\n",
      "004339\n",
      "004346\n",
      "004356\n",
      "004359\n",
      "004360\n",
      "004361\n",
      "004370\n",
      "004372\n",
      "004380\n",
      "004384\n",
      "004387\n",
      "004389\n",
      "004392\n",
      "004397\n",
      "004404\n",
      "004424\n",
      "004433\n",
      "004437\n",
      "004438\n",
      "004457\n",
      "004459\n",
      "004463\n",
      "004464\n",
      "004466\n",
      "004470\n",
      "004471\n",
      "004479\n",
      "004481\n",
      "004487\n",
      "004494\n",
      "004495\n",
      "004496\n",
      "004499\n",
      "004508\n",
      "004509\n",
      "004512\n",
      "004517\n",
      "004518\n",
      "004520\n",
      "004528\n",
      "004530\n",
      "004535\n",
      "004537\n",
      "004542\n",
      "004553\n",
      "004562\n",
      "004563\n",
      "004571\n",
      "004574\n",
      "004588\n",
      "004595\n",
      "004601\n",
      "004604\n",
      "004605\n",
      "004607\n",
      "004609\n",
      "004611\n",
      "004622\n",
      "004630\n",
      "004631\n",
      "004647\n",
      "004648\n",
      "004653\n",
      "004656\n",
      "004660\n",
      "004662\n",
      "004672\n",
      "004673\n",
      "004676\n",
      "004679\n",
      "004685\n",
      "004686\n",
      "004687\n",
      "004692\n",
      "004693\n",
      "004694\n",
      "004699\n",
      "004706\n",
      "004707\n",
      "004715\n",
      "004718\n",
      "004722\n",
      "004723\n",
      "004727\n",
      "004732\n",
      "004742\n",
      "004750\n",
      "004753\n",
      "004760\n",
      "004761\n",
      "004770\n",
      "004779\n",
      "004782\n",
      "004788\n",
      "004789\n",
      "004790\n",
      "004796\n",
      "004797\n",
      "004801\n",
      "004812\n",
      "004815\n",
      "004826\n",
      "004828\n",
      "004830\n",
      "004831\n",
      "004832\n",
      "004834\n",
      "004840\n",
      "004846\n",
      "004852\n",
      "004857\n",
      "004859\n",
      "004868\n",
      "004872\n",
      "004879\n",
      "004882\n",
      "004886\n",
      "004897\n",
      "004898\n",
      "004902\n",
      "004913\n",
      "004916\n",
      "004928\n",
      "004931\n",
      "004935\n",
      "004939\n",
      "004943\n",
      "004946\n",
      "004950\n",
      "004953\n",
      "004954\n",
      "004968\n",
      "004976\n",
      "004977\n",
      "004985\n",
      "004987\n",
      "004990\n",
      "004994\n",
      "004995\n",
      "005014\n",
      "005024\n",
      "005027\n",
      "005029\n",
      "005036\n",
      "005037\n",
      "005039\n",
      "005042\n",
      "005045\n",
      "005052\n",
      "005054\n",
      "005057\n",
      "005061\n",
      "005063\n",
      "005064\n",
      "005081\n",
      "005086\n",
      "005090\n",
      "005093\n",
      "005097\n",
      "005102\n",
      "005107\n",
      "005111\n",
      "005114\n",
      "005116\n",
      "005122\n",
      "005124\n",
      "005129\n",
      "005134\n",
      "005144\n",
      "005146\n",
      "005156\n",
      "005159\n",
      "005160\n",
      "005161\n",
      "005173\n",
      "005176\n",
      "005189\n",
      "005190\n",
      "005195\n",
      "005208\n",
      "005219\n",
      "005220\n",
      "005222\n",
      "005229\n",
      "005230\n",
      "005231\n",
      "005236\n",
      "005239\n",
      "005242\n",
      "005245\n",
      "005248\n",
      "005257\n",
      "005260\n",
      "005268\n",
      "005269\n",
      "005273\n",
      "005274\n",
      "005278\n",
      "005281\n",
      "005283\n",
      "005297\n",
      "005298\n",
      "005303\n",
      "005304\n",
      "005306\n",
      "005307\n",
      "005310\n",
      "005311\n",
      "005315\n",
      "005326\n",
      "005327\n",
      "005331\n",
      "005336\n",
      "005343\n",
      "005344\n",
      "005346\n",
      "005348\n",
      "005351\n",
      "005368\n",
      "005371\n",
      "005374\n",
      "005383\n",
      "005384\n",
      "005385\n",
      "005396\n",
      "005398\n",
      "005405\n",
      "005410\n",
      "005413\n",
      "005414\n",
      "005417\n",
      "005419\n",
      "005421\n",
      "005424\n",
      "005429\n",
      "005430\n",
      "005439\n",
      "005441\n",
      "005448\n",
      "005450\n",
      "005451\n",
      "005457\n",
      "005461\n",
      "005467\n",
      "005471\n",
      "005475\n",
      "005478\n",
      "005485\n",
      "005487\n",
      "005496\n",
      "005499\n",
      "005507\n",
      "005508\n",
      "005509\n",
      "005511\n",
      "005522\n",
      "005526\n",
      "005527\n",
      "005536\n",
      "005541\n",
      "005542\n",
      "005547\n",
      "005550\n",
      "005552\n",
      "005554\n",
      "005563\n",
      "005573\n",
      "005574\n",
      "005582\n",
      "005583\n",
      "005585\n",
      "005593\n",
      "005599\n",
      "005600\n",
      "005606\n",
      "005608\n",
      "005611\n",
      "005613\n",
      "005615\n",
      "005618\n",
      "005631\n",
      "005636\n",
      "005639\n",
      "005644\n",
      "005645\n",
      "005648\n",
      "005652\n",
      "005653\n",
      "005657\n",
      "005658\n",
      "005660\n",
      "005662\n",
      "005664\n",
      "005669\n",
      "005672\n",
      "005680\n",
      "005682\n",
      "005693\n",
      "005695\n",
      "005700\n",
      "005702\n",
      "005704\n",
      "005714\n",
      "005716\n",
      "005719\n",
      "005728\n",
      "005729\n",
      "005731\n",
      "005732\n",
      "005738\n",
      "005740\n",
      "005741\n",
      "005747\n",
      "005757\n",
      "005761\n",
      "005764\n",
      "005765\n",
      "005768\n",
      "005773\n",
      "005780\n",
      "005784\n",
      "005786\n",
      "005788\n",
      "005790\n",
      "005796\n",
      "005799\n",
      "005811\n",
      "005814\n",
      "005818\n",
      "005824\n",
      "005826\n",
      "005829\n",
      "005836\n",
      "005840\n",
      "005845\n",
      "005850\n",
      "005852\n",
      "005856\n",
      "005859\n",
      "005863\n",
      "005873\n",
      "005881\n",
      "005884\n",
      "005885\n",
      "005889\n",
      "005903\n",
      "005905\n",
      "005906\n",
      "005908\n",
      "005910\n",
      "005919\n",
      "005923\n",
      "005928\n",
      "005930\n",
      "005938\n",
      "005940\n",
      "005948\n",
      "005951\n",
      "005956\n",
      "005961\n",
      "005971\n",
      "005975\n",
      "005979\n",
      "005984\n",
      "005985\n",
      "005988\n",
      "005989\n",
      "005990\n",
      "005991\n",
      "005992\n",
      "006004\n",
      "006005\n",
      "006011\n",
      "006028\n",
      "006029\n",
      "006033\n",
      "006038\n",
      "006041\n",
      "006043\n",
      "006071\n",
      "006073\n",
      "006095\n",
      "006103\n",
      "006104\n",
      "006117\n",
      "006128\n",
      "006130\n",
      "006133\n",
      "006140\n",
      "006148\n",
      "006150\n",
      "006151\n",
      "006156\n",
      "006158\n",
      "006161\n",
      "006163\n",
      "006171\n",
      "006172\n",
      "006175\n",
      "006176\n",
      "006177\n",
      "006179\n",
      "006180\n",
      "006185\n",
      "006187\n",
      "006198\n",
      "006209\n",
      "006210\n",
      "006221\n",
      "006225\n",
      "006229\n",
      "006233\n",
      "006235\n",
      "006236\n",
      "006238\n",
      "006240\n",
      "006243\n",
      "006259\n",
      "006272\n",
      "006276\n",
      "006282\n",
      "006284\n",
      "006285\n",
      "006286\n",
      "006289\n",
      "006296\n",
      "006300\n",
      "006320\n",
      "006339\n",
      "006341\n",
      "006344\n",
      "006346\n",
      "006351\n",
      "006352\n",
      "006355\n",
      "006369\n",
      "006381\n",
      "006387\n",
      "006391\n",
      "006392\n",
      "006395\n",
      "006398\n",
      "006409\n",
      "006427\n",
      "006428\n",
      "006433\n",
      "006440\n",
      "006442\n",
      "006444\n",
      "006445\n",
      "006450\n",
      "006455\n",
      "006459\n",
      "006463\n",
      "006465\n",
      "006475\n",
      "006476\n",
      "006482\n",
      "006483\n",
      "006486\n",
      "006488\n",
      "006492\n",
      "006497\n",
      "006503\n",
      "006506\n",
      "006519\n",
      "006529\n",
      "006532\n",
      "006536\n",
      "006542\n",
      "006550\n",
      "006551\n",
      "006560\n",
      "006572\n",
      "006576\n",
      "006578\n",
      "006583\n",
      "006584\n",
      "006585\n",
      "006587\n",
      "006588\n",
      "006599\n",
      "006602\n",
      "006609\n",
      "006610\n",
      "006611\n",
      "006612\n",
      "006617\n",
      "006618\n",
      "006621\n",
      "006625\n",
      "006626\n",
      "006631\n",
      "006637\n",
      "006645\n",
      "006647\n",
      "006648\n",
      "006658\n",
      "006661\n",
      "006664\n",
      "006666\n",
      "006667\n",
      "006668\n",
      "006670\n",
      "006677\n",
      "006682\n",
      "006690\n",
      "006695\n",
      "006696\n",
      "006697\n",
      "006698\n",
      "006699\n",
      "006703\n",
      "006707\n",
      "006709\n",
      "006714\n",
      "006718\n",
      "006722\n",
      "006727\n",
      "006731\n",
      "006735\n",
      "006738\n",
      "006739\n",
      "006748\n",
      "006753\n",
      "006762\n",
      "006766\n",
      "006768\n",
      "006772\n",
      "006777\n",
      "006782\n",
      "006783\n",
      "006784\n",
      "006786\n",
      "006789\n",
      "006797\n",
      "006800\n",
      "006802\n",
      "006803\n",
      "006808\n",
      "006810\n",
      "006821\n",
      "006824\n",
      "006833\n",
      "006835\n",
      "006836\n",
      "006838\n",
      "006840\n",
      "006841\n",
      "006847\n",
      "006850\n",
      "006855\n",
      "006858\n",
      "006862\n",
      "006864\n",
      "006866\n",
      "006868\n",
      "006874\n",
      "006876\n",
      "006880\n",
      "006887\n",
      "006896\n",
      "006903\n",
      "006908\n",
      "006910\n",
      "006911\n",
      "006912\n",
      "006919\n",
      "006924\n",
      "006932\n",
      "006934\n",
      "006939\n",
      "006943\n",
      "006945\n",
      "006948\n",
      "006950\n",
      "006956\n",
      "006958\n",
      "006959\n",
      "006963\n",
      "006965\n",
      "006968\n",
      "006981\n",
      "006983\n",
      "006987\n",
      "006988\n",
      "006989\n",
      "006990\n",
      "007004\n",
      "007007\n",
      "007016\n",
      "007020\n",
      "007023\n",
      "007031\n",
      "007035\n",
      "007036\n",
      "007040\n",
      "007045\n",
      "007046\n",
      "007048\n",
      "007049\n",
      "007052\n",
      "007054\n",
      "007059\n",
      "007073\n",
      "007079\n",
      "007080\n",
      "007093\n",
      "007108\n",
      "007109\n",
      "007114\n",
      "007117\n",
      "007121\n",
      "007122\n",
      "007128\n",
      "007129\n",
      "007138\n",
      "007139\n",
      "007140\n",
      "007147\n",
      "007148\n",
      "007150\n",
      "007153\n",
      "007154\n",
      "007159\n",
      "007163\n",
      "007167\n",
      "007172\n",
      "007182\n",
      "007184\n",
      "007187\n",
      "007189\n",
      "007200\n",
      "007208\n",
      "007213\n",
      "007214\n",
      "007216\n",
      "007227\n",
      "007234\n",
      "007236\n",
      "007243\n",
      "007244\n",
      "007256\n",
      "007260\n",
      "007261\n",
      "007266\n",
      "007270\n",
      "007271\n",
      "007276\n",
      "007284\n",
      "007292\n",
      "007296\n",
      "007298\n",
      "007305\n",
      "007308\n",
      "007314\n",
      "007318\n",
      "007325\n",
      "007330\n",
      "007336\n",
      "007346\n",
      "007350\n",
      "007359\n",
      "007369\n",
      "007375\n",
      "007381\n",
      "007388\n",
      "007398\n",
      "007408\n",
      "007410\n",
      "007413\n",
      "007414\n",
      "007436\n",
      "007438\n",
      "007445\n",
      "007446\n",
      "007448\n",
      "007449\n",
      "007457\n",
      "007466\n",
      "007475\n",
      "007480\n",
      "007482\n",
      "007483\n",
      "007484\n",
      "007493\n",
      "007497\n",
      "007498\n",
      "007511\n",
      "007526\n",
      "007535\n",
      "007540\n",
      "007543\n",
      "007547\n",
      "007558\n",
      "007559\n",
      "007563\n",
      "007566\n",
      "007568\n",
      "007570\n",
      "007571\n",
      "007572\n",
      "007576\n",
      "007578\n",
      "007579\n",
      "007586\n",
      "007590\n",
      "007592\n",
      "007594\n",
      "007605\n",
      "007606\n",
      "007611\n",
      "007618\n",
      "007619\n",
      "007629\n",
      "007631\n",
      "007639\n",
      "007647\n",
      "007649\n",
      "007653\n",
      "007656\n",
      "007657\n",
      "007668\n",
      "007675\n",
      "007683\n",
      "007687\n",
      "007694\n",
      "007697\n",
      "007699\n",
      "007709\n",
      "007712\n",
      "007718\n",
      "007723\n",
      "007727\n",
      "007735\n",
      "007745\n",
      "007746\n",
      "007748\n",
      "007751\n",
      "007760\n",
      "007763\n",
      "007768\n",
      "007777\n",
      "007786\n",
      "007793\n",
      "007795\n",
      "007799\n",
      "007809\n",
      "007810\n",
      "007813\n",
      "007814\n",
      "007815\n",
      "007821\n",
      "007836\n",
      "007841\n",
      "007856\n",
      "007872\n",
      "007877\n",
      "007878\n",
      "007884\n",
      "007889\n",
      "007897\n",
      "007901\n",
      "007902\n",
      "007910\n",
      "007911\n",
      "007915\n",
      "007919\n",
      "007928\n",
      "007931\n",
      "007932\n",
      "007940\n",
      "007943\n",
      "007946\n",
      "007953\n",
      "007963\n",
      "007964\n",
      "007970\n",
      "007971\n",
      "007979\n",
      "007984\n",
      "007987\n",
      "007996\n",
      "007997\n",
      "007998\n",
      "008002\n",
      "008008\n",
      "008019\n",
      "008023\n",
      "008024\n",
      "008026\n",
      "008033\n",
      "008040\n",
      "008042\n",
      "008043\n",
      "008048\n",
      "008053\n",
      "008069\n",
      "008072\n",
      "008079\n",
      "008082\n",
      "008083\n",
      "008086\n",
      "008091\n",
      "008096\n",
      "008103\n",
      "008105\n",
      "008108\n",
      "008122\n",
      "008125\n",
      "008127\n",
      "008130\n",
      "008132\n",
      "008137\n",
      "008139\n",
      "008142\n",
      "008160\n",
      "008163\n",
      "008164\n",
      "008171\n",
      "008175\n",
      "008177\n",
      "008180\n",
      "008188\n",
      "008190\n",
      "008199\n",
      "008204\n",
      "008209\n",
      "008211\n",
      "008216\n",
      "008218\n",
      "008220\n",
      "008224\n",
      "008225\n",
      "008232\n",
      "008241\n",
      "008248\n",
      "008253\n",
      "008261\n",
      "008263\n",
      "008272\n",
      "008279\n",
      "008281\n",
      "008284\n",
      "008285\n",
      "008293\n",
      "008294\n",
      "008301\n",
      "008302\n",
      "008307\n",
      "008312\n",
      "008313\n",
      "008315\n",
      "008327\n",
      "008332\n",
      "008341\n",
      "008360\n",
      "008368\n",
      "008374\n",
      "008376\n",
      "008381\n",
      "008385\n",
      "008387\n",
      "008390\n",
      "008391\n",
      "008397\n",
      "008398\n",
      "008410\n",
      "008413\n",
      "008430\n",
      "008433\n",
      "008437\n",
      "008444\n",
      "008445\n",
      "008449\n",
      "008452\n",
      "008453\n",
      "008461\n",
      "008467\n",
      "008470\n",
      "008475\n",
      "008482\n",
      "008485\n",
      "008503\n",
      "008509\n",
      "008513\n",
      "008514\n",
      "008526\n",
      "008529\n",
      "008541\n",
      "008550\n",
      "008559\n",
      "008562\n",
      "008573\n",
      "008576\n",
      "008582\n",
      "008585\n",
      "008587\n",
      "008592\n",
      "008596\n",
      "008602\n",
      "008604\n",
      "008612\n",
      "008617\n",
      "008618\n",
      "008628\n",
      "008638\n",
      "008644\n",
      "008647\n",
      "008663\n",
      "008665\n",
      "008676\n",
      "008690\n",
      "008692\n",
      "008698\n",
      "008699\n",
      "008701\n",
      "008702\n",
      "008706\n",
      "008718\n",
      "008728\n",
      "008731\n",
      "008738\n",
      "008739\n",
      "008742\n",
      "008744\n",
      "008748\n",
      "008752\n",
      "008766\n",
      "008768\n",
      "008769\n",
      "008771\n",
      "008775\n",
      "008794\n",
      "008801\n",
      "008805\n",
      "008810\n",
      "008813\n",
      "008817\n",
      "008833\n",
      "008836\n",
      "008838\n",
      "008862\n",
      "008865\n",
      "008867\n",
      "008872\n",
      "008876\n",
      "008879\n",
      "008884\n",
      "008886\n",
      "008891\n",
      "008892\n",
      "008900\n",
      "008911\n",
      "008920\n",
      "008921\n",
      "008931\n",
      "008932\n",
      "008943\n",
      "008944\n",
      "008953\n",
      "008960\n",
      "008961\n",
      "008965\n",
      "008975\n",
      "008979\n",
      "008983\n",
      "009000\n",
      "009005\n",
      "009020\n",
      "009022\n",
      "009024\n",
      "009034\n",
      "009036\n",
      "009045\n",
      "009049\n",
      "009064\n",
      "009068\n",
      "009073\n",
      "009078\n",
      "009079\n",
      "009087\n",
      "009089\n",
      "009091\n",
      "009114\n",
      "009123\n",
      "009128\n",
      "009136\n",
      "009141\n",
      "009153\n",
      "009160\n",
      "009161\n",
      "009174\n",
      "009180\n",
      "009184\n",
      "009193\n",
      "009197\n",
      "009202\n",
      "009208\n",
      "009209\n",
      "009214\n",
      "009224\n",
      "009230\n",
      "009236\n",
      "009238\n",
      "009239\n",
      "009242\n",
      "009251\n",
      "009259\n",
      "009273\n",
      "009283\n",
      "009286\n",
      "009287\n",
      "009288\n",
      "009290\n",
      "009291\n",
      "009295\n",
      "009296\n",
      "009299\n",
      "009303\n",
      "009306\n",
      "009308\n",
      "009312\n",
      "009316\n",
      "009318\n",
      "009327\n",
      "009330\n",
      "009331\n",
      "009336\n",
      "009342\n",
      "009343\n",
      "009349\n",
      "009358\n",
      "009362\n",
      "009368\n",
      "009374\n",
      "009375\n",
      "009377\n",
      "009382\n",
      "009393\n",
      "009394\n",
      "009407\n",
      "009410\n",
      "009411\n",
      "009412\n",
      "009413\n",
      "009414\n",
      "009418\n",
      "009420\n",
      "009421\n",
      "009424\n",
      "009433\n",
      "009434\n",
      "009437\n",
      "009440\n",
      "009448\n",
      "009456\n",
      "009460\n",
      "009468\n",
      "009469\n",
      "009477\n",
      "009479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009481\n",
      "009490\n",
      "009494\n",
      "009496\n",
      "009497\n",
      "009499\n",
      "009500\n",
      "009504\n",
      "009512\n",
      "009517\n",
      "009518\n",
      "009520\n",
      "009524\n",
      "009526\n",
      "009531\n",
      "009532\n",
      "009537\n",
      "009541\n",
      "009542\n",
      "009546\n",
      "009551\n",
      "009557\n",
      "009558\n",
      "009565\n",
      "009567\n",
      "009568\n",
      "009573\n",
      "009577\n",
      "009579\n",
      "009585\n",
      "009591\n",
      "009596\n",
      "009600\n",
      "009603\n",
      "009609\n",
      "009611\n",
      "009613\n",
      "009614\n",
      "009617\n",
      "009618\n",
      "009638\n",
      "009641\n",
      "009647\n",
      "009649\n",
      "009654\n",
      "009655\n",
      "009656\n",
      "009659\n",
      "009666\n",
      "009668\n",
      "009671\n",
      "009676\n",
      "009684\n",
      "009687\n",
      "009691\n",
      "009693\n",
      "009698\n",
      "009703\n",
      "009711\n",
      "009712\n",
      "009713\n",
      "009717\n",
      "009718\n",
      "009721\n",
      "009726\n",
      "009732\n",
      "009734\n",
      "009738\n",
      "009747\n",
      "009754\n",
      "009755\n",
      "009756\n",
      "009762\n",
      "009767\n",
      "009773\n",
      "009776\n",
      "009780\n",
      "009781\n",
      "009789\n",
      "009792\n",
      "009796\n",
      "009800\n",
      "009809\n",
      "009813\n",
      "009822\n",
      "009828\n",
      "009841\n",
      "009845\n",
      "009848\n",
      "009851\n",
      "009859\n",
      "009867\n",
      "009868\n",
      "009869\n",
      "009874\n",
      "009877\n",
      "009878\n",
      "009879\n",
      "009880\n",
      "009881\n",
      "009882\n",
      "009900\n",
      "009902\n",
      "009917\n",
      "009918\n",
      "009926\n",
      "009935\n",
      "009942\n",
      "009944\n",
      "009946\n",
      "009947\n",
      "009949\n",
      "009950\n",
      "009954\n",
      "009958\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 128  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n",
      "32\n",
      "torch.Size([6, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3773, 0.2824, 0.7485, 1.0000, 0.0000],\n",
       "        [0.8037, 0.3389, 0.9356, 0.5000, 0.0000],\n",
       "        [0.8896, 0.3368, 0.9540, 0.4979, 0.0000],\n",
       "        [0.3650, 0.3410, 0.4479, 0.4874, 0.0000],\n",
       "        [0.3006, 0.3410, 0.3405, 0.3954, 0.0000],\n",
       "        [0.0767, 0.3410, 0.1166, 0.4079, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazeFace(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (4): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (5): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (7): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (8): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (9): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BlazeFaceExtra(\n",
      "  (features): Sequential(\n",
      "    (0): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.blazeface import SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 128,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [4, 8],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 128],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=2, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-3\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface128_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 25.8795 || 10iter: 9.9486 sec.\n",
      "Iteration 20 || Loss: 18.7910 || 10iter: 4.5899 sec.\n",
      "Iteration 30 || Loss: 15.2810 || 10iter: 4.9723 sec.\n",
      "Iteration 40 || Loss: 16.1553 || 10iter: 5.2477 sec.\n",
      "Iteration 50 || Loss: 16.6958 || 10iter: 3.9402 sec.\n",
      "Iteration 60 || Loss: 18.2191 || 10iter: 3.1149 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:1324.4515 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.0132 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 70 || Loss: 13.3653 || 10iter: 5.0666 sec.\n",
      "Iteration 80 || Loss: 13.0765 || 10iter: 6.5523 sec.\n",
      "Iteration 90 || Loss: 11.3279 || 10iter: 4.7422 sec.\n",
      "Iteration 100 || Loss: 12.5673 || 10iter: 5.2613 sec.\n",
      "Iteration 110 || Loss: 11.8278 || 10iter: 4.9162 sec.\n",
      "Iteration 120 || Loss: 11.2398 || 10iter: 4.4973 sec.\n",
      "Iteration 130 || Loss: 12.2265 || 10iter: 3.3242 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:805.6247 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.0463 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 140 || Loss: 10.7685 || 10iter: 5.6259 sec.\n",
      "Iteration 150 || Loss: 9.7290 || 10iter: 3.6519 sec.\n",
      "Iteration 160 || Loss: 12.3020 || 10iter: 6.6921 sec.\n",
      "Iteration 170 || Loss: 11.9427 || 10iter: 5.0195 sec.\n",
      "Iteration 180 || Loss: 11.1173 || 10iter: 5.0264 sec.\n",
      "Iteration 190 || Loss: 10.3843 || 10iter: 4.1260 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:703.1291 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.0807 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 200 || Loss: 10.8327 || 10iter: 4.0271 sec.\n",
      "Iteration 210 || Loss: 8.8087 || 10iter: 5.2883 sec.\n",
      "Iteration 220 || Loss: 10.0528 || 10iter: 3.3112 sec.\n",
      "Iteration 230 || Loss: 10.3157 || 10iter: 3.2061 sec.\n",
      "Iteration 240 || Loss: 10.2391 || 10iter: 5.7237 sec.\n",
      "Iteration 250 || Loss: 7.1589 || 10iter: 5.2836 sec.\n",
      "Iteration 260 || Loss: 7.7072 || 10iter: 3.2359 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:641.1161 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.6062 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 270 || Loss: 7.2601 || 10iter: 6.6124 sec.\n",
      "Iteration 280 || Loss: 7.3709 || 10iter: 5.4572 sec.\n",
      "Iteration 290 || Loss: 8.1717 || 10iter: 5.7193 sec.\n",
      "Iteration 300 || Loss: 8.6521 || 10iter: 4.2515 sec.\n",
      "Iteration 310 || Loss: 8.2011 || 10iter: 3.2180 sec.\n",
      "Iteration 320 || Loss: 7.6175 || 10iter: 5.1015 sec.\n",
      "Iteration 330 || Loss: 9.2340 || 10iter: 3.6966 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:544.5281 ||Epoch_VAL_Loss:242.2106\n",
      "timer:  44.8985 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 340 || Loss: 8.2508 || 10iter: 9.2721 sec.\n",
      "Iteration 350 || Loss: 9.9854 || 10iter: 3.7627 sec.\n",
      "Iteration 360 || Loss: 7.4307 || 10iter: 3.3371 sec.\n",
      "Iteration 370 || Loss: 7.9943 || 10iter: 3.0612 sec.\n",
      "Iteration 380 || Loss: 9.4275 || 10iter: 4.2242 sec.\n",
      "Iteration 390 || Loss: 8.0927 || 10iter: 2.4869 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:517.1980 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.5385 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 400 || Loss: 7.0648 || 10iter: 3.8540 sec.\n",
      "Iteration 410 || Loss: 6.9953 || 10iter: 6.5921 sec.\n",
      "Iteration 420 || Loss: 7.9019 || 10iter: 5.5803 sec.\n",
      "Iteration 430 || Loss: 7.4807 || 10iter: 5.2795 sec.\n",
      "Iteration 440 || Loss: 9.7014 || 10iter: 4.6376 sec.\n",
      "Iteration 450 || Loss: 6.6180 || 10iter: 4.2126 sec.\n",
      "Iteration 460 || Loss: 7.8972 || 10iter: 3.3594 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:478.8341 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.2616 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 470 || Loss: 6.6639 || 10iter: 6.3484 sec.\n",
      "Iteration 480 || Loss: 7.4798 || 10iter: 3.1776 sec.\n",
      "Iteration 490 || Loss: 7.5482 || 10iter: 4.2056 sec.\n",
      "Iteration 500 || Loss: 6.9327 || 10iter: 7.0107 sec.\n",
      "Iteration 510 || Loss: 6.6687 || 10iter: 4.9347 sec.\n",
      "Iteration 520 || Loss: 6.3543 || 10iter: 3.6463 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:456.9837 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1639 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 530 || Loss: 6.4922 || 10iter: 3.8718 sec.\n",
      "Iteration 540 || Loss: 5.7576 || 10iter: 5.6735 sec.\n",
      "Iteration 550 || Loss: 6.5739 || 10iter: 4.1413 sec.\n",
      "Iteration 560 || Loss: 6.6022 || 10iter: 3.3806 sec.\n",
      "Iteration 570 || Loss: 6.3238 || 10iter: 3.0095 sec.\n",
      "Iteration 580 || Loss: 6.8098 || 10iter: 6.7843 sec.\n",
      "Iteration 590 || Loss: 6.2868 || 10iter: 3.7419 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:446.1537 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1102 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 600 || Loss: 6.4656 || 10iter: 6.2102 sec.\n",
      "Iteration 610 || Loss: 5.5487 || 10iter: 5.1610 sec.\n",
      "Iteration 620 || Loss: 6.8220 || 10iter: 5.3730 sec.\n",
      "Iteration 630 || Loss: 6.6036 || 10iter: 5.3966 sec.\n",
      "Iteration 640 || Loss: 7.1408 || 10iter: 3.9145 sec.\n",
      "Iteration 650 || Loss: 7.4338 || 10iter: 3.2417 sec.\n",
      "Iteration 660 || Loss: 7.6151 || 10iter: 4.1261 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:440.5737 ||Epoch_VAL_Loss:213.4986\n",
      "timer:  44.3708 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 670 || Loss: 5.8515 || 10iter: 9.1723 sec.\n",
      "Iteration 680 || Loss: 6.2586 || 10iter: 5.1558 sec.\n",
      "Iteration 690 || Loss: 7.3706 || 10iter: 3.8605 sec.\n",
      "Iteration 700 || Loss: 5.8025 || 10iter: 3.5161 sec.\n",
      "Iteration 710 || Loss: 5.6700 || 10iter: 5.3408 sec.\n",
      "Iteration 720 || Loss: 6.6332 || 10iter: 4.2995 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:424.0156 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5344 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 730 || Loss: 7.0253 || 10iter: 5.3216 sec.\n",
      "Iteration 740 || Loss: 5.6395 || 10iter: 5.7265 sec.\n",
      "Iteration 750 || Loss: 6.7595 || 10iter: 4.9738 sec.\n",
      "Iteration 760 || Loss: 7.5677 || 10iter: 5.4720 sec.\n",
      "Iteration 770 || Loss: 6.5891 || 10iter: 3.3755 sec.\n",
      "Iteration 780 || Loss: 6.7641 || 10iter: 3.4099 sec.\n",
      "Iteration 790 || Loss: 5.9704 || 10iter: 3.1389 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:415.2046 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1938 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 800 || Loss: 6.1334 || 10iter: 5.7760 sec.\n",
      "Iteration 810 || Loss: 5.5588 || 10iter: 3.3376 sec.\n",
      "Iteration 820 || Loss: 5.5267 || 10iter: 4.9574 sec.\n",
      "Iteration 830 || Loss: 5.9919 || 10iter: 6.2410 sec.\n",
      "Iteration 840 || Loss: 8.7084 || 10iter: 5.3641 sec.\n",
      "Iteration 850 || Loss: 6.6301 || 10iter: 3.9203 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:420.9409 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.3530 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n",
      "Iteration 860 || Loss: 6.6726 || 10iter: 4.3353 sec.\n",
      "Iteration 870 || Loss: 5.2726 || 10iter: 5.7338 sec.\n",
      "Iteration 880 || Loss: 7.5904 || 10iter: 3.7019 sec.\n",
      "Iteration 890 || Loss: 7.8420 || 10iter: 3.3688 sec.\n",
      "Iteration 900 || Loss: 5.7804 || 10iter: 3.6468 sec.\n",
      "Iteration 910 || Loss: 6.0927 || 10iter: 6.9583 sec.\n",
      "Iteration 920 || Loss: 5.6841 || 10iter: 3.3362 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:416.0687 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.6829 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 930 || Loss: 6.0400 || 10iter: 7.1326 sec.\n",
      "Iteration 940 || Loss: 6.4721 || 10iter: 5.2178 sec.\n",
      "Iteration 950 || Loss: 5.5958 || 10iter: 5.1213 sec.\n",
      "Iteration 960 || Loss: 5.4022 || 10iter: 4.3271 sec.\n",
      "Iteration 970 || Loss: 5.4034 || 10iter: 3.4308 sec.\n",
      "Iteration 980 || Loss: 5.5873 || 10iter: 3.4289 sec.\n",
      "Iteration 990 || Loss: 5.8415 || 10iter: 4.9720 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:401.5338 ||Epoch_VAL_Loss:192.5541\n",
      "timer:  44.4224 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1000 || Loss: 6.1793 || 10iter: 9.3493 sec.\n",
      "Iteration 1010 || Loss: 5.3828 || 10iter: 4.4378 sec.\n",
      "Iteration 1020 || Loss: 6.6914 || 10iter: 3.5746 sec.\n",
      "Iteration 1030 || Loss: 5.8635 || 10iter: 3.6503 sec.\n",
      "Iteration 1040 || Loss: 6.7932 || 10iter: 5.3526 sec.\n",
      "Iteration 1050 || Loss: 6.4678 || 10iter: 4.5094 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:398.9087 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8995 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 17/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1060 || Loss: 6.3088 || 10iter: 6.1092 sec.\n",
      "Iteration 1070 || Loss: 6.4473 || 10iter: 4.9865 sec.\n",
      "Iteration 1080 || Loss: 6.3467 || 10iter: 5.4380 sec.\n",
      "Iteration 1090 || Loss: 6.7570 || 10iter: 5.4601 sec.\n",
      "Iteration 1100 || Loss: 5.7319 || 10iter: 3.3467 sec.\n",
      "Iteration 1110 || Loss: 6.3377 || 10iter: 3.3797 sec.\n",
      "Iteration 1120 || Loss: 5.1928 || 10iter: 3.5788 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:392.6125 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.3167 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 18/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1130 || Loss: 5.5649 || 10iter: 8.1390 sec.\n",
      "Iteration 1140 || Loss: 6.3331 || 10iter: 4.7450 sec.\n",
      "Iteration 1150 || Loss: 5.8808 || 10iter: 5.2080 sec.\n",
      "Iteration 1160 || Loss: 5.2652 || 10iter: 5.1664 sec.\n",
      "Iteration 1170 || Loss: 5.7686 || 10iter: 4.6720 sec.\n",
      "Iteration 1180 || Loss: 5.4543 || 10iter: 3.7601 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:389.4132 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.3516 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 19/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1190 || Loss: 5.5595 || 10iter: 2.7328 sec.\n",
      "Iteration 1200 || Loss: 5.5658 || 10iter: 5.4823 sec.\n",
      "Iteration 1210 || Loss: 5.7950 || 10iter: 3.6225 sec.\n",
      "Iteration 1220 || Loss: 6.2505 || 10iter: 3.1597 sec.\n",
      "Iteration 1230 || Loss: 5.6614 || 10iter: 4.4618 sec.\n",
      "Iteration 1240 || Loss: 6.5752 || 10iter: 6.1236 sec.\n",
      "Iteration 1250 || Loss: 5.2142 || 10iter: 3.3976 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:386.2327 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  30.4977 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 20/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1260 || Loss: 5.9823 || 10iter: 6.7652 sec.\n",
      "Iteration 1270 || Loss: 5.1596 || 10iter: 5.5793 sec.\n",
      "Iteration 1280 || Loss: 5.6961 || 10iter: 4.8892 sec.\n",
      "Iteration 1290 || Loss: 5.7149 || 10iter: 4.0129 sec.\n",
      "Iteration 1300 || Loss: 5.2345 || 10iter: 3.4510 sec.\n",
      "Iteration 1310 || Loss: 5.4694 || 10iter: 3.2741 sec.\n",
      "Iteration 1320 || Loss: 6.1532 || 10iter: 4.3561 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:382.3223 ||Epoch_VAL_Loss:187.4955\n",
      "timer:  43.9403 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 21/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1330 || Loss: 4.9288 || 10iter: 9.4470 sec.\n",
      "Iteration 1340 || Loss: 6.5707 || 10iter: 4.4353 sec.\n",
      "Iteration 1350 || Loss: 5.9210 || 10iter: 3.9586 sec.\n",
      "Iteration 1360 || Loss: 7.5194 || 10iter: 3.2146 sec.\n",
      "Iteration 1370 || Loss: 5.3998 || 10iter: 5.6374 sec.\n",
      "Iteration 1380 || Loss: 5.4592 || 10iter: 4.2065 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:378.8376 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.0354 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 22/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1390 || Loss: 5.3266 || 10iter: 5.3403 sec.\n",
      "Iteration 1400 || Loss: 6.1523 || 10iter: 5.3057 sec.\n",
      "Iteration 1410 || Loss: 5.1363 || 10iter: 5.0902 sec.\n",
      "Iteration 1420 || Loss: 6.4343 || 10iter: 4.5453 sec.\n",
      "Iteration 1430 || Loss: 6.0088 || 10iter: 4.1579 sec.\n",
      "Iteration 1440 || Loss: 5.7316 || 10iter: 3.2590 sec.\n",
      "Iteration 1450 || Loss: 5.8061 || 10iter: 3.3241 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:384.5692 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.2577 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 23/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1460 || Loss: 5.7357 || 10iter: 8.6217 sec.\n",
      "Iteration 1470 || Loss: 6.2165 || 10iter: 5.1524 sec.\n",
      "Iteration 1480 || Loss: 6.1212 || 10iter: 5.1946 sec.\n",
      "Iteration 1490 || Loss: 5.8004 || 10iter: 4.8788 sec.\n",
      "Iteration 1500 || Loss: 5.0608 || 10iter: 5.0505 sec.\n",
      "Iteration 1510 || Loss: 5.3588 || 10iter: 3.5559 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:381.5916 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.9155 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 24/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1520 || Loss: 5.4180 || 10iter: 2.7525 sec.\n",
      "Iteration 1530 || Loss: 5.7105 || 10iter: 5.0895 sec.\n",
      "Iteration 1540 || Loss: 5.4976 || 10iter: 6.0899 sec.\n",
      "Iteration 1550 || Loss: 5.0720 || 10iter: 5.1462 sec.\n",
      "Iteration 1560 || Loss: 5.2782 || 10iter: 5.0031 sec.\n",
      "Iteration 1570 || Loss: 5.8252 || 10iter: 4.5880 sec.\n",
      "Iteration 1580 || Loss: 5.5732 || 10iter: 3.4628 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:381.3399 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4583 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 25/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1590 || Loss: 5.0552 || 10iter: 6.1435 sec.\n",
      "Iteration 1600 || Loss: 4.7963 || 10iter: 3.3632 sec.\n",
      "Iteration 1610 || Loss: 5.3508 || 10iter: 3.7563 sec.\n",
      "Iteration 1620 || Loss: 7.0870 || 10iter: 3.7468 sec.\n",
      "Iteration 1630 || Loss: 5.8675 || 10iter: 3.5181 sec.\n",
      "Iteration 1640 || Loss: 5.3088 || 10iter: 2.5971 sec.\n",
      "Iteration 1650 || Loss: 5.3172 || 10iter: 3.0222 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:365.5579 ||Epoch_VAL_Loss:184.8248\n",
      "timer:  38.7357 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 26/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1660 || Loss: 4.8578 || 10iter: 8.8868 sec.\n",
      "Iteration 1670 || Loss: 5.2420 || 10iter: 5.1873 sec.\n",
      "Iteration 1680 || Loss: 5.5439 || 10iter: 4.3436 sec.\n",
      "Iteration 1690 || Loss: 6.1031 || 10iter: 3.4541 sec.\n",
      "Iteration 1700 || Loss: 5.2477 || 10iter: 3.0855 sec.\n",
      "Iteration 1710 || Loss: 5.8355 || 10iter: 5.0525 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:366.9629 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1487 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 27/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1720 || Loss: 5.2654 || 10iter: 5.4799 sec.\n",
      "Iteration 1730 || Loss: 5.5610 || 10iter: 5.2955 sec.\n",
      "Iteration 1740 || Loss: 5.0454 || 10iter: 5.0286 sec.\n",
      "Iteration 1750 || Loss: 5.7276 || 10iter: 4.6172 sec.\n",
      "Iteration 1760 || Loss: 5.2292 || 10iter: 4.8926 sec.\n",
      "Iteration 1770 || Loss: 5.9957 || 10iter: 3.2737 sec.\n",
      "Iteration 1780 || Loss: 5.2651 || 10iter: 3.0604 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:366.4626 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1580 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 28/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1790 || Loss: 5.2801 || 10iter: 10.1243 sec.\n",
      "Iteration 1800 || Loss: 6.0089 || 10iter: 4.5823 sec.\n",
      "Iteration 1810 || Loss: 5.7026 || 10iter: 5.4635 sec.\n",
      "Iteration 1820 || Loss: 5.5549 || 10iter: 4.8162 sec.\n",
      "Iteration 1830 || Loss: 5.7697 || 10iter: 5.0125 sec.\n",
      "Iteration 1840 || Loss: 5.9535 || 10iter: 3.7791 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:366.4569 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.3446 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 29/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1850 || Loss: 4.6406 || 10iter: 2.9735 sec.\n",
      "Iteration 1860 || Loss: 4.7780 || 10iter: 4.1291 sec.\n",
      "Iteration 1870 || Loss: 5.2086 || 10iter: 6.6189 sec.\n",
      "Iteration 1880 || Loss: 4.9633 || 10iter: 5.5217 sec.\n",
      "Iteration 1890 || Loss: 6.8127 || 10iter: 4.6656 sec.\n",
      "Iteration 1900 || Loss: 5.7328 || 10iter: 4.9967 sec.\n",
      "Iteration 1910 || Loss: 5.4714 || 10iter: 3.4010 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:369.2575 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.6685 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 30/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1920 || Loss: 4.9580 || 10iter: 6.3926 sec.\n",
      "Iteration 1930 || Loss: 4.4719 || 10iter: 3.2495 sec.\n",
      "Iteration 1940 || Loss: 5.2667 || 10iter: 3.6654 sec.\n",
      "Iteration 1950 || Loss: 6.7963 || 10iter: 6.2738 sec.\n",
      "Iteration 1960 || Loss: 5.5036 || 10iter: 4.8406 sec.\n",
      "Iteration 1970 || Loss: 5.4529 || 10iter: 3.9179 sec.\n",
      "Iteration 1980 || Loss: 6.4265 || 10iter: 3.3549 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:365.4949 ||Epoch_VAL_Loss:180.7464\n",
      "timer:  42.3265 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 31/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1990 || Loss: 5.4180 || 10iter: 7.0712 sec.\n",
      "Iteration 2000 || Loss: 5.4505 || 10iter: 3.2126 sec.\n",
      "Iteration 2010 || Loss: 5.8504 || 10iter: 4.4209 sec.\n",
      "Iteration 2020 || Loss: 5.6003 || 10iter: 3.6866 sec.\n",
      "Iteration 2030 || Loss: 5.6165 || 10iter: 3.1054 sec.\n",
      "Iteration 2040 || Loss: 5.5729 || 10iter: 3.2461 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:361.5799 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.6330 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 32/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2050 || Loss: 5.3543 || 10iter: 5.5833 sec.\n",
      "Iteration 2060 || Loss: 5.0644 || 10iter: 5.7068 sec.\n",
      "Iteration 2070 || Loss: 5.0810 || 10iter: 4.7907 sec.\n",
      "Iteration 2080 || Loss: 5.6853 || 10iter: 5.5479 sec.\n",
      "Iteration 2090 || Loss: 5.5329 || 10iter: 4.6458 sec.\n",
      "Iteration 2100 || Loss: 5.6177 || 10iter: 3.8039 sec.\n",
      "Iteration 2110 || Loss: 6.0615 || 10iter: 3.0984 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:360.4325 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8714 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 33/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2120 || Loss: 5.3887 || 10iter: 9.6099 sec.\n",
      "Iteration 2130 || Loss: 6.1761 || 10iter: 5.3010 sec.\n",
      "Iteration 2140 || Loss: 4.8753 || 10iter: 5.2826 sec.\n",
      "Iteration 2150 || Loss: 4.7907 || 10iter: 5.2660 sec.\n",
      "Iteration 2160 || Loss: 6.0817 || 10iter: 4.7120 sec.\n",
      "Iteration 2170 || Loss: 5.7536 || 10iter: 3.6680 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:355.0768 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.4611 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 34/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2180 || Loss: 5.1250 || 10iter: 2.9772 sec.\n",
      "Iteration 2190 || Loss: 5.7580 || 10iter: 4.0049 sec.\n",
      "Iteration 2200 || Loss: 5.2643 || 10iter: 6.6321 sec.\n",
      "Iteration 2210 || Loss: 5.4131 || 10iter: 5.8710 sec.\n",
      "Iteration 2220 || Loss: 6.4108 || 10iter: 4.7266 sec.\n",
      "Iteration 2230 || Loss: 5.8163 || 10iter: 5.1801 sec.\n",
      "Iteration 2240 || Loss: 5.4080 || 10iter: 3.3746 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:358.6712 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.2534 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 35/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2250 || Loss: 5.6126 || 10iter: 6.0583 sec.\n",
      "Iteration 2260 || Loss: 4.9249 || 10iter: 3.4376 sec.\n",
      "Iteration 2270 || Loss: 7.6695 || 10iter: 3.2240 sec.\n",
      "Iteration 2280 || Loss: 5.7683 || 10iter: 6.3380 sec.\n",
      "Iteration 2290 || Loss: 5.1297 || 10iter: 5.7756 sec.\n",
      "Iteration 2300 || Loss: 6.0194 || 10iter: 4.4146 sec.\n",
      "Iteration 2310 || Loss: 5.5488 || 10iter: 3.2134 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:364.5961 ||Epoch_VAL_Loss:181.4992\n",
      "timer:  43.2789 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 36/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2320 || Loss: 5.2237 || 10iter: 6.7191 sec.\n",
      "Iteration 2330 || Loss: 4.8899 || 10iter: 3.8858 sec.\n",
      "Iteration 2340 || Loss: 5.3958 || 10iter: 6.8510 sec.\n",
      "Iteration 2350 || Loss: 4.9342 || 10iter: 4.7131 sec.\n",
      "Iteration 2360 || Loss: 5.7008 || 10iter: 5.0622 sec.\n",
      "Iteration 2370 || Loss: 5.1598 || 10iter: 3.4606 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:356.1712 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.7494 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 37/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2380 || Loss: 4.9333 || 10iter: 5.3368 sec.\n",
      "Iteration 2390 || Loss: 5.9493 || 10iter: 5.0888 sec.\n",
      "Iteration 2400 || Loss: 7.8270 || 10iter: 3.4782 sec.\n",
      "Iteration 2410 || Loss: 5.9149 || 10iter: 3.3087 sec.\n",
      "Iteration 2420 || Loss: 5.3231 || 10iter: 4.5829 sec.\n",
      "Iteration 2430 || Loss: 5.5423 || 10iter: 3.1096 sec.\n",
      "Iteration 2440 || Loss: 5.2271 || 10iter: 2.1098 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:367.5158 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.5699 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 38/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2450 || Loss: 5.1361 || 10iter: 7.9097 sec.\n",
      "Iteration 2460 || Loss: 4.7827 || 10iter: 6.3864 sec.\n",
      "Iteration 2470 || Loss: 6.2014 || 10iter: 5.0117 sec.\n",
      "Iteration 2480 || Loss: 5.2777 || 10iter: 4.5772 sec.\n",
      "Iteration 2490 || Loss: 8.4233 || 10iter: 4.8459 sec.\n",
      "Iteration 2500 || Loss: 5.4167 || 10iter: 4.0948 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:356.8190 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.5434 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 39/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2510 || Loss: 5.8447 || 10iter: 3.2870 sec.\n",
      "Iteration 2520 || Loss: 5.9601 || 10iter: 3.6261 sec.\n",
      "Iteration 2530 || Loss: 4.9754 || 10iter: 4.0128 sec.\n",
      "Iteration 2540 || Loss: 4.9014 || 10iter: 6.9842 sec.\n",
      "Iteration 2550 || Loss: 5.6806 || 10iter: 5.1428 sec.\n",
      "Iteration 2560 || Loss: 5.6837 || 10iter: 4.9789 sec.\n",
      "Iteration 2570 || Loss: 4.9486 || 10iter: 3.4669 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:352.0901 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.9079 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 40/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2580 || Loss: 5.7503 || 10iter: 6.7886 sec.\n",
      "Iteration 2590 || Loss: 5.0906 || 10iter: 3.7605 sec.\n",
      "Iteration 2600 || Loss: 5.0887 || 10iter: 3.7474 sec.\n",
      "Iteration 2610 || Loss: 5.0072 || 10iter: 3.9220 sec.\n",
      "Iteration 2620 || Loss: 5.5237 || 10iter: 6.4861 sec.\n",
      "Iteration 2630 || Loss: 5.7005 || 10iter: 4.6812 sec.\n",
      "Iteration 2640 || Loss: 4.8860 || 10iter: 3.3165 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:354.9532 ||Epoch_VAL_Loss:177.8393\n",
      "timer:  43.2566 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 41/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2650 || Loss: 5.2297 || 10iter: 7.4914 sec.\n",
      "Iteration 2660 || Loss: 5.7614 || 10iter: 3.2947 sec.\n",
      "Iteration 2670 || Loss: 5.6950 || 10iter: 6.6289 sec.\n",
      "Iteration 2680 || Loss: 5.4453 || 10iter: 5.8508 sec.\n",
      "Iteration 2690 || Loss: 5.3481 || 10iter: 5.8546 sec.\n",
      "Iteration 2700 || Loss: 5.7286 || 10iter: 3.4740 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:354.5597 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.7468 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 42/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2710 || Loss: 5.0544 || 10iter: 5.6700 sec.\n",
      "Iteration 2720 || Loss: 5.6800 || 10iter: 4.9689 sec.\n",
      "Iteration 2730 || Loss: 5.1074 || 10iter: 3.3989 sec.\n",
      "Iteration 2740 || Loss: 5.9363 || 10iter: 3.4313 sec.\n",
      "Iteration 2750 || Loss: 5.5328 || 10iter: 4.1447 sec.\n",
      "Iteration 2760 || Loss: 5.0932 || 10iter: 5.8157 sec.\n",
      "Iteration 2770 || Loss: 5.3019 || 10iter: 3.3937 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:349.6082 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.6828 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 43/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2780 || Loss: 5.6477 || 10iter: 7.9185 sec.\n",
      "Iteration 2790 || Loss: 5.8422 || 10iter: 4.7547 sec.\n",
      "Iteration 2800 || Loss: 5.7884 || 10iter: 5.1953 sec.\n",
      "Iteration 2810 || Loss: 5.1798 || 10iter: 4.2396 sec.\n",
      "Iteration 2820 || Loss: 5.0856 || 10iter: 3.4996 sec.\n",
      "Iteration 2830 || Loss: 5.1996 || 10iter: 3.3603 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:355.6068 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.0134 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 44/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2840 || Loss: 5.4686 || 10iter: 3.6529 sec.\n",
      "Iteration 2850 || Loss: 4.5685 || 10iter: 3.3667 sec.\n",
      "Iteration 2860 || Loss: 5.8541 || 10iter: 4.0273 sec.\n",
      "Iteration 2870 || Loss: 5.1226 || 10iter: 6.6791 sec.\n",
      "Iteration 2880 || Loss: 5.0530 || 10iter: 5.5397 sec.\n",
      "Iteration 2890 || Loss: 5.3407 || 10iter: 4.9963 sec.\n",
      "Iteration 2900 || Loss: 4.7600 || 10iter: 3.3587 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:350.3588 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2005 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 45/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2910 || Loss: 4.7596 || 10iter: 6.6786 sec.\n",
      "Iteration 2920 || Loss: 4.5961 || 10iter: 4.2309 sec.\n",
      "Iteration 2930 || Loss: 5.3109 || 10iter: 3.6288 sec.\n",
      "Iteration 2940 || Loss: 5.7007 || 10iter: 4.1852 sec.\n",
      "Iteration 2950 || Loss: 5.2479 || 10iter: 6.3376 sec.\n",
      "Iteration 2960 || Loss: 5.2149 || 10iter: 4.0652 sec.\n",
      "Iteration 2970 || Loss: 5.0296 || 10iter: 3.0543 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:352.8707 ||Epoch_VAL_Loss:177.4520\n",
      "timer:  42.8679 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 46/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2980 || Loss: 4.8800 || 10iter: 7.3886 sec.\n",
      "Iteration 2990 || Loss: 5.2242 || 10iter: 3.3650 sec.\n",
      "Iteration 3000 || Loss: 6.0672 || 10iter: 5.0151 sec.\n",
      "Iteration 3010 || Loss: 6.0584 || 10iter: 6.3270 sec.\n",
      "Iteration 3020 || Loss: 4.9178 || 10iter: 5.9638 sec.\n",
      "Iteration 3030 || Loss: 4.6860 || 10iter: 3.4580 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:354.7771 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.7086 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 47/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3040 || Loss: 5.0337 || 10iter: 5.6084 sec.\n",
      "Iteration 3050 || Loss: 5.6788 || 10iter: 5.5502 sec.\n",
      "Iteration 3060 || Loss: 4.6614 || 10iter: 4.1321 sec.\n",
      "Iteration 3070 || Loss: 5.5690 || 10iter: 3.3272 sec.\n",
      "Iteration 3080 || Loss: 4.8240 || 10iter: 3.5583 sec.\n",
      "Iteration 3090 || Loss: 5.0934 || 10iter: 6.2379 sec.\n",
      "Iteration 3100 || Loss: 4.9811 || 10iter: 3.4308 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:347.8138 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.6341 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 48/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3110 || Loss: 4.7705 || 10iter: 7.9157 sec.\n",
      "Iteration 3120 || Loss: 5.6901 || 10iter: 5.1697 sec.\n",
      "Iteration 3130 || Loss: 4.8844 || 10iter: 4.9790 sec.\n",
      "Iteration 3140 || Loss: 5.0884 || 10iter: 4.6546 sec.\n",
      "Iteration 3150 || Loss: 5.5543 || 10iter: 3.5086 sec.\n",
      "Iteration 3160 || Loss: 4.6845 || 10iter: 3.1377 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:348.7904 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.6648 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 49/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3170 || Loss: 5.1814 || 10iter: 5.1039 sec.\n",
      "Iteration 3180 || Loss: 5.6592 || 10iter: 6.2243 sec.\n",
      "Iteration 3190 || Loss: 6.0801 || 10iter: 5.2291 sec.\n",
      "Iteration 3200 || Loss: 5.3549 || 10iter: 5.5239 sec.\n",
      "Iteration 3210 || Loss: 5.4534 || 10iter: 4.8087 sec.\n",
      "Iteration 3220 || Loss: 5.2018 || 10iter: 3.4829 sec.\n",
      "Iteration 3230 || Loss: 6.2092 || 10iter: 3.0859 sec.\n",
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:356.2334 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.5465 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 50/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3240 || Loss: 5.4426 || 10iter: 6.0703 sec.\n",
      "Iteration 3250 || Loss: 4.9422 || 10iter: 3.5880 sec.\n",
      "Iteration 3260 || Loss: 5.6458 || 10iter: 2.8721 sec.\n",
      "Iteration 3270 || Loss: 5.2662 || 10iter: 6.1502 sec.\n",
      "Iteration 3280 || Loss: 4.8227 || 10iter: 5.5038 sec.\n",
      "Iteration 3290 || Loss: 5.2385 || 10iter: 4.4660 sec.\n",
      "Iteration 3300 || Loss: 4.7495 || 10iter: 3.2601 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:349.8790 ||Epoch_VAL_Loss:177.0397\n",
      "timer:  42.8806 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 51/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3310 || Loss: 4.8131 || 10iter: 6.4352 sec.\n",
      "Iteration 3320 || Loss: 5.2130 || 10iter: 4.9288 sec.\n",
      "Iteration 3330 || Loss: 5.0125 || 10iter: 6.2862 sec.\n",
      "Iteration 3340 || Loss: 5.3325 || 10iter: 5.2298 sec.\n",
      "Iteration 3350 || Loss: 6.1677 || 10iter: 5.7167 sec.\n",
      "Iteration 3360 || Loss: 6.4144 || 10iter: 3.5995 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:352.0928 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.4347 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 52/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3370 || Loss: 5.3685 || 10iter: 5.0435 sec.\n",
      "Iteration 3380 || Loss: 5.2958 || 10iter: 4.4796 sec.\n",
      "Iteration 3390 || Loss: 5.1836 || 10iter: 3.5520 sec.\n",
      "Iteration 3400 || Loss: 5.1270 || 10iter: 4.4768 sec.\n",
      "Iteration 3410 || Loss: 5.6136 || 10iter: 6.6790 sec.\n",
      "Iteration 3420 || Loss: 5.4502 || 10iter: 4.7239 sec.\n",
      "Iteration 3430 || Loss: 5.4065 || 10iter: 3.3441 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:351.9484 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.0921 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 53/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3440 || Loss: 5.7801 || 10iter: 7.5945 sec.\n",
      "Iteration 3450 || Loss: 4.9099 || 10iter: 4.3459 sec.\n",
      "Iteration 3460 || Loss: 5.3797 || 10iter: 4.5096 sec.\n",
      "Iteration 3470 || Loss: 5.0178 || 10iter: 3.5237 sec.\n",
      "Iteration 3480 || Loss: 6.3640 || 10iter: 3.4338 sec.\n",
      "Iteration 3490 || Loss: 5.3404 || 10iter: 4.9517 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:344.7505 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.4532 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 54/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3500 || Loss: 5.2564 || 10iter: 4.3007 sec.\n",
      "Iteration 3510 || Loss: 5.6002 || 10iter: 5.3967 sec.\n",
      "Iteration 3520 || Loss: 5.2027 || 10iter: 4.7767 sec.\n",
      "Iteration 3530 || Loss: 5.3606 || 10iter: 5.1994 sec.\n",
      "Iteration 3540 || Loss: 5.5068 || 10iter: 4.7407 sec.\n",
      "Iteration 3550 || Loss: 4.8117 || 10iter: 3.3548 sec.\n",
      "Iteration 3560 || Loss: 4.9740 || 10iter: 2.8499 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:348.9963 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1331 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 55/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3570 || Loss: 4.8572 || 10iter: 8.8692 sec.\n",
      "Iteration 3580 || Loss: 5.2974 || 10iter: 4.6025 sec.\n",
      "Iteration 3590 || Loss: 5.6444 || 10iter: 5.3425 sec.\n",
      "Iteration 3600 || Loss: 5.9936 || 10iter: 4.7078 sec.\n",
      "Iteration 3610 || Loss: 5.6217 || 10iter: 4.8591 sec.\n",
      "Iteration 3620 || Loss: 5.6882 || 10iter: 4.1281 sec.\n",
      "Iteration 3630 || Loss: 5.1282 || 10iter: 3.0089 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:351.1389 ||Epoch_VAL_Loss:176.9738\n",
      "timer:  43.4520 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 56/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3640 || Loss: 5.1493 || 10iter: 6.0389 sec.\n",
      "Iteration 3650 || Loss: 5.7002 || 10iter: 5.3376 sec.\n",
      "Iteration 3660 || Loss: 5.7151 || 10iter: 5.6216 sec.\n",
      "Iteration 3670 || Loss: 7.0023 || 10iter: 5.3019 sec.\n",
      "Iteration 3680 || Loss: 5.2158 || 10iter: 5.0985 sec.\n",
      "Iteration 3690 || Loss: 5.7901 || 10iter: 3.9331 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:349.6541 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.6324 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 57/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3700 || Loss: 5.4109 || 10iter: 5.3509 sec.\n",
      "Iteration 3710 || Loss: 4.5522 || 10iter: 4.2563 sec.\n",
      "Iteration 3720 || Loss: 4.4660 || 10iter: 3.5088 sec.\n",
      "Iteration 3730 || Loss: 5.3574 || 10iter: 4.1351 sec.\n",
      "Iteration 3740 || Loss: 4.8352 || 10iter: 6.8816 sec.\n",
      "Iteration 3750 || Loss: 5.1277 || 10iter: 3.9960 sec.\n",
      "Iteration 3760 || Loss: 4.4972 || 10iter: 3.4988 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:343.8282 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.4552 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 58/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3770 || Loss: 5.2132 || 10iter: 7.7586 sec.\n",
      "Iteration 3780 || Loss: 5.2331 || 10iter: 4.9314 sec.\n",
      "Iteration 3790 || Loss: 5.4201 || 10iter: 4.2935 sec.\n",
      "Iteration 3800 || Loss: 4.8619 || 10iter: 3.6488 sec.\n",
      "Iteration 3810 || Loss: 5.1525 || 10iter: 3.8465 sec.\n",
      "Iteration 3820 || Loss: 5.1937 || 10iter: 4.9032 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:344.1412 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.2466 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 59/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3830 || Loss: 5.0166 || 10iter: 4.4793 sec.\n",
      "Iteration 3840 || Loss: 5.6741 || 10iter: 5.9270 sec.\n",
      "Iteration 3850 || Loss: 5.0706 || 10iter: 5.1654 sec.\n",
      "Iteration 3860 || Loss: 4.9701 || 10iter: 4.9765 sec.\n",
      "Iteration 3870 || Loss: 4.6089 || 10iter: 4.5723 sec.\n",
      "Iteration 3880 || Loss: 4.7449 || 10iter: 3.3318 sec.\n",
      "Iteration 3890 || Loss: 4.8053 || 10iter: 3.0226 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:346.0818 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.3161 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 60/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3900 || Loss: 6.2097 || 10iter: 7.8456 sec.\n",
      "Iteration 3910 || Loss: 4.6645 || 10iter: 5.5541 sec.\n",
      "Iteration 3920 || Loss: 5.2183 || 10iter: 5.0363 sec.\n",
      "Iteration 3930 || Loss: 6.3011 || 10iter: 5.6644 sec.\n",
      "Iteration 3940 || Loss: 4.4512 || 10iter: 5.2452 sec.\n",
      "Iteration 3950 || Loss: 4.6046 || 10iter: 3.5762 sec.\n",
      "Iteration 3960 || Loss: 5.3552 || 10iter: 2.9866 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:344.2694 ||Epoch_VAL_Loss:174.7491\n",
      "timer:  47.1834 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 61/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3970 || Loss: 5.1377 || 10iter: 8.7595 sec.\n",
      "Iteration 3980 || Loss: 4.9801 || 10iter: 4.4292 sec.\n",
      "Iteration 3990 || Loss: 5.9310 || 10iter: 5.6541 sec.\n",
      "Iteration 4000 || Loss: 5.1214 || 10iter: 4.4158 sec.\n",
      "Iteration 4010 || Loss: 5.8641 || 10iter: 3.5830 sec.\n",
      "Iteration 4020 || Loss: 4.9499 || 10iter: 3.2540 sec.\n",
      "-------------\n",
      "epoch 61 || Epoch_TRAIN_Loss:353.3635 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.0279 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 62/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4030 || Loss: 5.1570 || 10iter: 4.4013 sec.\n",
      "Iteration 4040 || Loss: 4.8749 || 10iter: 3.5894 sec.\n",
      "Iteration 4050 || Loss: 4.8957 || 10iter: 2.8829 sec.\n",
      "Iteration 4060 || Loss: 6.5538 || 10iter: 5.9753 sec.\n",
      "Iteration 4070 || Loss: 6.6630 || 10iter: 5.6060 sec.\n",
      "Iteration 4080 || Loss: 4.8712 || 10iter: 4.4585 sec.\n",
      "Iteration 4090 || Loss: 5.1795 || 10iter: 3.3735 sec.\n",
      "-------------\n",
      "epoch 62 || Epoch_TRAIN_Loss:348.7929 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  30.9974 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 63/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4100 || Loss: 4.8563 || 10iter: 8.3291 sec.\n",
      "Iteration 4110 || Loss: 5.1610 || 10iter: 5.1267 sec.\n",
      "Iteration 4120 || Loss: 4.5962 || 10iter: 3.5432 sec.\n",
      "Iteration 4130 || Loss: 5.1685 || 10iter: 3.5776 sec.\n",
      "Iteration 4140 || Loss: 5.5348 || 10iter: 5.6613 sec.\n",
      "Iteration 4150 || Loss: 4.8240 || 10iter: 4.6734 sec.\n",
      "-------------\n",
      "epoch 63 || Epoch_TRAIN_Loss:347.9291 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8659 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 64/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4160 || Loss: 4.8917 || 10iter: 4.0370 sec.\n",
      "Iteration 4170 || Loss: 4.9202 || 10iter: 6.1530 sec.\n",
      "Iteration 4180 || Loss: 5.4993 || 10iter: 4.9141 sec.\n",
      "Iteration 4190 || Loss: 5.1613 || 10iter: 4.8620 sec.\n",
      "Iteration 4200 || Loss: 5.7871 || 10iter: 3.7619 sec.\n",
      "Iteration 4210 || Loss: 5.4421 || 10iter: 3.3163 sec.\n",
      "Iteration 4220 || Loss: 4.8469 || 10iter: 3.6090 sec.\n",
      "-------------\n",
      "epoch 64 || Epoch_TRAIN_Loss:340.8212 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8422 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 65/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4230 || Loss: 5.4939 || 10iter: 6.8056 sec.\n",
      "Iteration 4240 || Loss: 5.2311 || 10iter: 5.0919 sec.\n",
      "Iteration 4250 || Loss: 5.5800 || 10iter: 4.9113 sec.\n",
      "Iteration 4260 || Loss: 5.4981 || 10iter: 5.2577 sec.\n",
      "Iteration 4270 || Loss: 5.3561 || 10iter: 5.1521 sec.\n",
      "Iteration 4280 || Loss: 4.9830 || 10iter: 3.4323 sec.\n",
      "Iteration 4290 || Loss: 5.1192 || 10iter: 2.9277 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 65 || Epoch_TRAIN_Loss:341.3625 ||Epoch_VAL_Loss:173.2099\n",
      "timer:  45.6405 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 66/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4300 || Loss: 4.7466 || 10iter: 8.5946 sec.\n",
      "Iteration 4310 || Loss: 4.7963 || 10iter: 5.0224 sec.\n",
      "Iteration 4320 || Loss: 5.0785 || 10iter: 5.1820 sec.\n",
      "Iteration 4330 || Loss: 6.1385 || 10iter: 4.2181 sec.\n",
      "Iteration 4340 || Loss: 4.7994 || 10iter: 3.3485 sec.\n",
      "Iteration 4350 || Loss: 4.7037 || 10iter: 3.0367 sec.\n",
      "-------------\n",
      "epoch 66 || Epoch_TRAIN_Loss:345.6071 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.3211 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 67/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4360 || Loss: 4.7195 || 10iter: 6.2388 sec.\n",
      "Iteration 4370 || Loss: 4.4682 || 10iter: 5.7150 sec.\n",
      "Iteration 4380 || Loss: 4.8345 || 10iter: 5.2121 sec.\n",
      "Iteration 4390 || Loss: 5.8251 || 10iter: 4.9744 sec.\n",
      "Iteration 4400 || Loss: 5.3446 || 10iter: 5.0216 sec.\n",
      "Iteration 4410 || Loss: 5.7839 || 10iter: 4.4251 sec.\n",
      "Iteration 4420 || Loss: 5.6350 || 10iter: 3.1568 sec.\n",
      "-------------\n",
      "epoch 67 || Epoch_TRAIN_Loss:340.8186 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.4993 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 68/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4430 || Loss: 4.9273 || 10iter: 6.8620 sec.\n",
      "Iteration 4440 || Loss: 5.7496 || 10iter: 3.5831 sec.\n",
      "Iteration 4450 || Loss: 4.9222 || 10iter: 3.3934 sec.\n",
      "Iteration 4460 || Loss: 5.0977 || 10iter: 3.1086 sec.\n",
      "Iteration 4470 || Loss: 4.7018 || 10iter: 6.4948 sec.\n",
      "Iteration 4480 || Loss: 5.4030 || 10iter: 3.9996 sec.\n",
      "-------------\n",
      "epoch 68 || Epoch_TRAIN_Loss:338.2930 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  30.0761 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 69/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4490 || Loss: 4.9756 || 10iter: 4.5600 sec.\n",
      "Iteration 4500 || Loss: 4.8089 || 10iter: 5.7942 sec.\n",
      "Iteration 4510 || Loss: 5.5466 || 10iter: 4.8508 sec.\n",
      "Iteration 4520 || Loss: 4.8402 || 10iter: 4.8573 sec.\n",
      "Iteration 4530 || Loss: 5.4539 || 10iter: 3.3416 sec.\n",
      "Iteration 4540 || Loss: 5.2210 || 10iter: 3.3505 sec.\n",
      "Iteration 4550 || Loss: 5.7508 || 10iter: 4.5538 sec.\n",
      "-------------\n",
      "epoch 69 || Epoch_TRAIN_Loss:341.7771 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.0454 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 70/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4560 || Loss: 5.5645 || 10iter: 6.3026 sec.\n",
      "Iteration 4570 || Loss: 4.9914 || 10iter: 5.5546 sec.\n",
      "Iteration 4580 || Loss: 5.0145 || 10iter: 5.0610 sec.\n",
      "Iteration 4590 || Loss: 6.5766 || 10iter: 5.2299 sec.\n",
      "Iteration 4600 || Loss: 5.1676 || 10iter: 4.7121 sec.\n",
      "Iteration 4610 || Loss: 5.0607 || 10iter: 3.1640 sec.\n",
      "Iteration 4620 || Loss: 4.7444 || 10iter: 2.7902 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 70 || Epoch_TRAIN_Loss:340.6823 ||Epoch_VAL_Loss:174.0113\n",
      "timer:  46.1791 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 71/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4630 || Loss: 4.7128 || 10iter: 8.7469 sec.\n",
      "Iteration 4640 || Loss: 5.3534 || 10iter: 4.8985 sec.\n",
      "Iteration 4650 || Loss: 4.6193 || 10iter: 4.9526 sec.\n",
      "Iteration 4660 || Loss: 5.1873 || 10iter: 3.6845 sec.\n",
      "Iteration 4670 || Loss: 5.5465 || 10iter: 3.4497 sec.\n",
      "Iteration 4680 || Loss: 4.7155 || 10iter: 4.7303 sec.\n",
      "-------------\n",
      "epoch 71 || Epoch_TRAIN_Loss:333.5612 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8602 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 72/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4690 || Loss: 5.3515 || 10iter: 6.1522 sec.\n",
      "Iteration 4700 || Loss: 4.7394 || 10iter: 5.4687 sec.\n",
      "Iteration 4710 || Loss: 4.6901 || 10iter: 5.3797 sec.\n",
      "Iteration 4720 || Loss: 5.6658 || 10iter: 5.5858 sec.\n",
      "Iteration 4730 || Loss: 5.0619 || 10iter: 4.5581 sec.\n",
      "Iteration 4740 || Loss: 5.1763 || 10iter: 3.2296 sec.\n",
      "Iteration 4750 || Loss: 5.0608 || 10iter: 2.9776 sec.\n",
      "-------------\n",
      "epoch 72 || Epoch_TRAIN_Loss:339.0524 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.0236 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 73/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4760 || Loss: 4.7861 || 10iter: 10.2272 sec.\n",
      "Iteration 4770 || Loss: 5.1413 || 10iter: 5.0324 sec.\n",
      "Iteration 4780 || Loss: 4.8133 || 10iter: 5.3379 sec.\n",
      "Iteration 4790 || Loss: 5.1090 || 10iter: 5.0695 sec.\n",
      "Iteration 4800 || Loss: 5.0317 || 10iter: 4.9951 sec.\n",
      "Iteration 4810 || Loss: 4.7817 || 10iter: 3.9917 sec.\n",
      "-------------\n",
      "epoch 73 || Epoch_TRAIN_Loss:334.3250 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.0855 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 74/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4820 || Loss: 5.2293 || 10iter: 3.3438 sec.\n",
      "Iteration 4830 || Loss: 5.2332 || 10iter: 4.8944 sec.\n",
      "Iteration 4840 || Loss: 4.9289 || 10iter: 3.4373 sec.\n",
      "Iteration 4850 || Loss: 5.3336 || 10iter: 3.1694 sec.\n",
      "Iteration 4860 || Loss: 4.7891 || 10iter: 2.8694 sec.\n",
      "Iteration 4870 || Loss: 4.9612 || 10iter: 6.0485 sec.\n",
      "Iteration 4880 || Loss: 5.8370 || 10iter: 3.5208 sec.\n",
      "-------------\n",
      "epoch 74 || Epoch_TRAIN_Loss:338.8012 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  28.8809 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 75/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4890 || Loss: 5.1219 || 10iter: 6.4502 sec.\n",
      "Iteration 4900 || Loss: 4.7595 || 10iter: 5.4641 sec.\n",
      "Iteration 4910 || Loss: 5.1478 || 10iter: 4.5648 sec.\n",
      "Iteration 4920 || Loss: 4.9742 || 10iter: 5.1641 sec.\n",
      "Iteration 4930 || Loss: 6.0915 || 10iter: 3.7179 sec.\n",
      "Iteration 4940 || Loss: 4.9875 || 10iter: 3.2122 sec.\n",
      "Iteration 4950 || Loss: 4.8368 || 10iter: 3.7322 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 75 || Epoch_TRAIN_Loss:341.5465 ||Epoch_VAL_Loss:172.9328\n",
      "timer:  43.9605 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 76/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4960 || Loss: 4.6449 || 10iter: 9.1209 sec.\n",
      "Iteration 4970 || Loss: 4.4199 || 10iter: 5.1865 sec.\n",
      "Iteration 4980 || Loss: 5.4924 || 10iter: 4.2406 sec.\n",
      "Iteration 4990 || Loss: 5.6862 || 10iter: 3.4736 sec.\n",
      "Iteration 5000 || Loss: 4.6844 || 10iter: 4.6102 sec.\n",
      "Iteration 5010 || Loss: 4.6910 || 10iter: 4.4390 sec.\n",
      "-------------\n",
      "epoch 76 || Epoch_TRAIN_Loss:342.0746 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.1889 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 77/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5020 || Loss: 5.2343 || 10iter: 5.4421 sec.\n",
      "Iteration 5030 || Loss: 4.6338 || 10iter: 6.3887 sec.\n",
      "Iteration 5040 || Loss: 5.1906 || 10iter: 5.4529 sec.\n",
      "Iteration 5050 || Loss: 4.9108 || 10iter: 5.3055 sec.\n",
      "Iteration 5060 || Loss: 5.1298 || 10iter: 4.1504 sec.\n",
      "Iteration 5070 || Loss: 5.9110 || 10iter: 3.2635 sec.\n",
      "Iteration 5080 || Loss: 5.4489 || 10iter: 3.0357 sec.\n",
      "-------------\n",
      "epoch 77 || Epoch_TRAIN_Loss:338.1949 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.0111 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 78/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5090 || Loss: 4.8331 || 10iter: 8.9565 sec.\n",
      "Iteration 5100 || Loss: 5.5726 || 10iter: 5.1120 sec.\n",
      "Iteration 5110 || Loss: 4.9913 || 10iter: 5.2715 sec.\n",
      "Iteration 5120 || Loss: 6.6463 || 10iter: 5.2572 sec.\n",
      "Iteration 5130 || Loss: 5.0894 || 10iter: 5.3678 sec.\n",
      "Iteration 5140 || Loss: 4.8595 || 10iter: 3.5222 sec.\n",
      "-------------\n",
      "epoch 78 || Epoch_TRAIN_Loss:344.4926 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.0905 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 79/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5150 || Loss: 5.0118 || 10iter: 3.4805 sec.\n",
      "Iteration 5160 || Loss: 5.2183 || 10iter: 6.1553 sec.\n",
      "Iteration 5170 || Loss: 5.2767 || 10iter: 5.7118 sec.\n",
      "Iteration 5180 || Loss: 4.6637 || 10iter: 5.2331 sec.\n",
      "Iteration 5190 || Loss: 7.3490 || 10iter: 5.2505 sec.\n",
      "Iteration 5200 || Loss: 6.0314 || 10iter: 5.0709 sec.\n",
      "Iteration 5210 || Loss: 4.9682 || 10iter: 3.4147 sec.\n",
      "-------------\n",
      "epoch 79 || Epoch_TRAIN_Loss:336.4194 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.7223 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 80/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5220 || Loss: 5.0552 || 10iter: 5.0848 sec.\n",
      "Iteration 5230 || Loss: 4.9468 || 10iter: 3.4792 sec.\n",
      "Iteration 5240 || Loss: 5.2297 || 10iter: 4.3132 sec.\n",
      "Iteration 5250 || Loss: 5.8468 || 10iter: 3.6028 sec.\n",
      "Iteration 5260 || Loss: 6.0256 || 10iter: 3.2669 sec.\n",
      "Iteration 5270 || Loss: 4.9725 || 10iter: 2.5601 sec.\n",
      "Iteration 5280 || Loss: 4.9544 || 10iter: 4.6078 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 80 || Epoch_TRAIN_Loss:337.5905 ||Epoch_VAL_Loss:173.5689\n",
      "timer:  37.8698 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 81/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5290 || Loss: 5.8236 || 10iter: 9.1433 sec.\n",
      "Iteration 5300 || Loss: 5.6233 || 10iter: 4.6711 sec.\n",
      "Iteration 5310 || Loss: 4.9910 || 10iter: 3.8720 sec.\n",
      "Iteration 5320 || Loss: 4.9847 || 10iter: 3.4416 sec.\n",
      "Iteration 5330 || Loss: 4.9147 || 10iter: 5.3576 sec.\n",
      "Iteration 5340 || Loss: 5.2100 || 10iter: 4.1819 sec.\n",
      "-------------\n",
      "epoch 81 || Epoch_TRAIN_Loss:341.8613 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8916 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 82/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5350 || Loss: 5.3285 || 10iter: 5.0570 sec.\n",
      "Iteration 5360 || Loss: 4.8386 || 10iter: 6.1383 sec.\n",
      "Iteration 5370 || Loss: 4.9861 || 10iter: 5.4103 sec.\n",
      "Iteration 5380 || Loss: 5.0347 || 10iter: 5.4113 sec.\n",
      "Iteration 5390 || Loss: 5.3398 || 10iter: 3.6432 sec.\n",
      "Iteration 5400 || Loss: 5.9817 || 10iter: 3.4042 sec.\n",
      "Iteration 5410 || Loss: 5.5117 || 10iter: 3.4888 sec.\n",
      "-------------\n",
      "epoch 82 || Epoch_TRAIN_Loss:336.0098 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8359 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 83/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5420 || Loss: 4.9495 || 10iter: 8.3471 sec.\n",
      "Iteration 5430 || Loss: 4.9070 || 10iter: 4.9381 sec.\n",
      "Iteration 5440 || Loss: 4.5039 || 10iter: 5.3131 sec.\n",
      "Iteration 5450 || Loss: 5.7295 || 10iter: 5.1615 sec.\n",
      "Iteration 5460 || Loss: 6.3982 || 10iter: 4.7632 sec.\n",
      "Iteration 5470 || Loss: 5.7027 || 10iter: 3.5465 sec.\n",
      "-------------\n",
      "epoch 83 || Epoch_TRAIN_Loss:340.5975 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.6274 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 84/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5480 || Loss: 5.2976 || 10iter: 2.8774 sec.\n",
      "Iteration 5490 || Loss: 5.0577 || 10iter: 7.3096 sec.\n",
      "Iteration 5500 || Loss: 4.5250 || 10iter: 5.1300 sec.\n",
      "Iteration 5510 || Loss: 5.4894 || 10iter: 5.5410 sec.\n",
      "Iteration 5520 || Loss: 6.0531 || 10iter: 5.2708 sec.\n",
      "Iteration 5530 || Loss: 5.0587 || 10iter: 4.6439 sec.\n",
      "Iteration 5540 || Loss: 4.9620 || 10iter: 3.6415 sec.\n",
      "-------------\n",
      "epoch 84 || Epoch_TRAIN_Loss:334.6212 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.6405 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 85/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5550 || Loss: 4.8955 || 10iter: 4.9346 sec.\n",
      "Iteration 5560 || Loss: 4.8062 || 10iter: 3.2510 sec.\n",
      "Iteration 5570 || Loss: 4.9394 || 10iter: 5.9677 sec.\n",
      "Iteration 5580 || Loss: 5.4122 || 10iter: 5.4377 sec.\n",
      "Iteration 5590 || Loss: 5.2928 || 10iter: 5.0007 sec.\n",
      "Iteration 5600 || Loss: 5.6717 || 10iter: 4.2874 sec.\n",
      "Iteration 5610 || Loss: 5.8170 || 10iter: 3.2859 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 85 || Epoch_TRAIN_Loss:338.0629 ||Epoch_VAL_Loss:172.5573\n",
      "timer:  42.4903 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 86/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5620 || Loss: 5.1200 || 10iter: 7.4126 sec.\n",
      "Iteration 5630 || Loss: 4.5945 || 10iter: 3.4497 sec.\n",
      "Iteration 5640 || Loss: 5.0037 || 10iter: 3.2887 sec.\n",
      "Iteration 5650 || Loss: 4.8417 || 10iter: 2.9024 sec.\n",
      "Iteration 5660 || Loss: 5.2255 || 10iter: 6.0671 sec.\n",
      "Iteration 5670 || Loss: 4.8154 || 10iter: 3.9238 sec.\n",
      "-------------\n",
      "epoch 86 || Epoch_TRAIN_Loss:336.8758 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  29.2700 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 87/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5680 || Loss: 4.7374 || 10iter: 5.4732 sec.\n",
      "Iteration 5690 || Loss: 4.8569 || 10iter: 5.1279 sec.\n",
      "Iteration 5700 || Loss: 4.8640 || 10iter: 5.5711 sec.\n",
      "Iteration 5710 || Loss: 5.5602 || 10iter: 5.6563 sec.\n",
      "Iteration 5720 || Loss: 5.1328 || 10iter: 3.9049 sec.\n",
      "Iteration 5730 || Loss: 5.3267 || 10iter: 3.2728 sec.\n",
      "Iteration 5740 || Loss: 5.4693 || 10iter: 3.5816 sec.\n",
      "-------------\n",
      "epoch 87 || Epoch_TRAIN_Loss:334.8111 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.7676 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 88/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5750 || Loss: 5.4755 || 10iter: 8.3807 sec.\n",
      "Iteration 5760 || Loss: 5.8350 || 10iter: 4.6243 sec.\n",
      "Iteration 5770 || Loss: 5.5156 || 10iter: 5.2607 sec.\n",
      "Iteration 5780 || Loss: 4.7519 || 10iter: 4.6863 sec.\n",
      "Iteration 5790 || Loss: 5.3283 || 10iter: 4.2615 sec.\n",
      "Iteration 5800 || Loss: 5.0859 || 10iter: 3.3559 sec.\n",
      "-------------\n",
      "epoch 88 || Epoch_TRAIN_Loss:337.4256 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2632 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 89/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5810 || Loss: 4.9953 || 10iter: 3.1067 sec.\n",
      "Iteration 5820 || Loss: 4.6990 || 10iter: 6.2994 sec.\n",
      "Iteration 5830 || Loss: 4.5934 || 10iter: 5.3030 sec.\n",
      "Iteration 5840 || Loss: 5.1632 || 10iter: 5.5666 sec.\n",
      "Iteration 5850 || Loss: 4.6676 || 10iter: 5.2079 sec.\n",
      "Iteration 5860 || Loss: 4.9067 || 10iter: 4.5190 sec.\n",
      "Iteration 5870 || Loss: 5.9178 || 10iter: 3.3211 sec.\n",
      "-------------\n",
      "epoch 89 || Epoch_TRAIN_Loss:334.3678 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.8471 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 90/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5880 || Loss: 5.0294 || 10iter: 5.2701 sec.\n",
      "Iteration 5890 || Loss: 4.9581 || 10iter: 3.2411 sec.\n",
      "Iteration 5900 || Loss: 4.9600 || 10iter: 4.3960 sec.\n",
      "Iteration 5910 || Loss: 5.3870 || 10iter: 6.0122 sec.\n",
      "Iteration 5920 || Loss: 5.2884 || 10iter: 4.8083 sec.\n",
      "Iteration 5930 || Loss: 5.3948 || 10iter: 4.3214 sec.\n",
      "Iteration 5940 || Loss: 5.3709 || 10iter: 3.3472 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 90 || Epoch_TRAIN_Loss:337.5317 ||Epoch_VAL_Loss:171.2715\n",
      "timer:  41.7772 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 91/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5950 || Loss: 4.2548 || 10iter: 6.3120 sec.\n",
      "Iteration 5960 || Loss: 4.7073 || 10iter: 6.0663 sec.\n",
      "Iteration 5970 || Loss: 4.8797 || 10iter: 5.4598 sec.\n",
      "Iteration 5980 || Loss: 4.9508 || 10iter: 5.1641 sec.\n",
      "Iteration 5990 || Loss: 5.4431 || 10iter: 4.7166 sec.\n",
      "Iteration 6000 || Loss: 5.0022 || 10iter: 3.4619 sec.\n",
      "-------------\n",
      "epoch 91 || Epoch_TRAIN_Loss:336.4180 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4656 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 92/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6010 || Loss: 4.6823 || 10iter: 5.7091 sec.\n",
      "Iteration 6020 || Loss: 5.4404 || 10iter: 3.9878 sec.\n",
      "Iteration 6030 || Loss: 4.7842 || 10iter: 3.2724 sec.\n",
      "Iteration 6040 || Loss: 5.1063 || 10iter: 4.8803 sec.\n",
      "Iteration 6050 || Loss: 5.6645 || 10iter: 4.1723 sec.\n",
      "Iteration 6060 || Loss: 4.5470 || 10iter: 2.7274 sec.\n",
      "Iteration 6070 || Loss: 4.8243 || 10iter: 2.1949 sec.\n",
      "-------------\n",
      "epoch 92 || Epoch_TRAIN_Loss:334.5057 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.8695 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 93/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6080 || Loss: 6.6558 || 10iter: 9.8518 sec.\n",
      "Iteration 6090 || Loss: 4.6927 || 10iter: 5.1101 sec.\n",
      "Iteration 6100 || Loss: 4.6548 || 10iter: 4.8906 sec.\n",
      "Iteration 6110 || Loss: 4.7806 || 10iter: 5.2352 sec.\n",
      "Iteration 6120 || Loss: 4.8750 || 10iter: 5.3759 sec.\n",
      "Iteration 6130 || Loss: 5.2543 || 10iter: 3.6066 sec.\n",
      "-------------\n",
      "epoch 93 || Epoch_TRAIN_Loss:335.4084 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.5124 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 94/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6140 || Loss: 5.1814 || 10iter: 3.4529 sec.\n",
      "Iteration 6150 || Loss: 5.9273 || 10iter: 5.4250 sec.\n",
      "Iteration 6160 || Loss: 5.3562 || 10iter: 6.0791 sec.\n",
      "Iteration 6170 || Loss: 4.9083 || 10iter: 5.4690 sec.\n",
      "Iteration 6180 || Loss: 4.7102 || 10iter: 5.1927 sec.\n",
      "Iteration 6190 || Loss: 5.0967 || 10iter: 5.4304 sec.\n",
      "Iteration 6200 || Loss: 4.8121 || 10iter: 3.3208 sec.\n",
      "-------------\n",
      "epoch 94 || Epoch_TRAIN_Loss:337.5486 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.9487 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 95/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6210 || Loss: 5.9020 || 10iter: 5.0176 sec.\n",
      "Iteration 6220 || Loss: 4.4267 || 10iter: 3.2387 sec.\n",
      "Iteration 6230 || Loss: 4.9361 || 10iter: 6.1477 sec.\n",
      "Iteration 6240 || Loss: 5.8300 || 10iter: 5.7687 sec.\n",
      "Iteration 6250 || Loss: 5.2793 || 10iter: 4.6580 sec.\n",
      "Iteration 6260 || Loss: 5.0982 || 10iter: 4.0263 sec.\n",
      "Iteration 6270 || Loss: 4.8961 || 10iter: 3.2202 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 95 || Epoch_TRAIN_Loss:332.9794 ||Epoch_VAL_Loss:169.5926\n",
      "timer:  42.5456 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 96/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6280 || Loss: 5.1402 || 10iter: 6.9456 sec.\n",
      "Iteration 6290 || Loss: 4.9130 || 10iter: 6.2726 sec.\n",
      "Iteration 6300 || Loss: 4.9558 || 10iter: 5.6648 sec.\n",
      "Iteration 6310 || Loss: 5.3072 || 10iter: 4.9827 sec.\n",
      "Iteration 6320 || Loss: 5.6104 || 10iter: 4.8902 sec.\n",
      "Iteration 6330 || Loss: 4.5515 || 10iter: 3.4065 sec.\n",
      "-------------\n",
      "epoch 96 || Epoch_TRAIN_Loss:335.4803 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.3423 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 97/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6340 || Loss: 5.0237 || 10iter: 5.4696 sec.\n",
      "Iteration 6350 || Loss: 4.9915 || 10iter: 3.5655 sec.\n",
      "Iteration 6360 || Loss: 5.2389 || 10iter: 3.2122 sec.\n",
      "Iteration 6370 || Loss: 4.4454 || 10iter: 6.3995 sec.\n",
      "Iteration 6380 || Loss: 4.9309 || 10iter: 5.4992 sec.\n",
      "Iteration 6390 || Loss: 4.4740 || 10iter: 5.1999 sec.\n",
      "Iteration 6400 || Loss: 5.0778 || 10iter: 3.3934 sec.\n",
      "-------------\n",
      "epoch 97 || Epoch_TRAIN_Loss:335.5139 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5124 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 98/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6410 || Loss: 4.8398 || 10iter: 8.0204 sec.\n",
      "Iteration 6420 || Loss: 6.6632 || 10iter: 4.9559 sec.\n",
      "Iteration 6430 || Loss: 5.1607 || 10iter: 3.8757 sec.\n",
      "Iteration 6440 || Loss: 5.1572 || 10iter: 3.3153 sec.\n",
      "Iteration 6450 || Loss: 4.6252 || 10iter: 4.0564 sec.\n",
      "Iteration 6460 || Loss: 5.3590 || 10iter: 2.9433 sec.\n",
      "-------------\n",
      "epoch 98 || Epoch_TRAIN_Loss:328.7714 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  29.0120 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 99/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6470 || Loss: 4.5742 || 10iter: 3.0719 sec.\n",
      "Iteration 6480 || Loss: 4.8794 || 10iter: 5.6338 sec.\n",
      "Iteration 6490 || Loss: 4.6937 || 10iter: 5.6832 sec.\n",
      "Iteration 6500 || Loss: 4.9117 || 10iter: 5.0465 sec.\n",
      "Iteration 6510 || Loss: 4.3895 || 10iter: 5.0255 sec.\n",
      "Iteration 6520 || Loss: 4.5658 || 10iter: 5.0813 sec.\n",
      "Iteration 6530 || Loss: 4.8086 || 10iter: 3.3167 sec.\n",
      "-------------\n",
      "epoch 99 || Epoch_TRAIN_Loss:330.9227 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.2576 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 100/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6540 || Loss: 4.9971 || 10iter: 5.4874 sec.\n",
      "Iteration 6550 || Loss: 4.8287 || 10iter: 3.4478 sec.\n",
      "Iteration 6560 || Loss: 4.8661 || 10iter: 4.4028 sec.\n",
      "Iteration 6570 || Loss: 5.3905 || 10iter: 6.8666 sec.\n",
      "Iteration 6580 || Loss: 5.6367 || 10iter: 4.6232 sec.\n",
      "Iteration 6590 || Loss: 4.6382 || 10iter: 4.2300 sec.\n",
      "Iteration 6600 || Loss: 5.5395 || 10iter: 3.3596 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 100 || Epoch_TRAIN_Loss:331.9443 ||Epoch_VAL_Loss:169.0103\n",
      "timer:  42.9537 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 101/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6610 || Loss: 4.9355 || 10iter: 6.2269 sec.\n",
      "Iteration 6620 || Loss: 5.6058 || 10iter: 6.2844 sec.\n",
      "Iteration 6630 || Loss: 5.2638 || 10iter: 5.4506 sec.\n",
      "Iteration 6640 || Loss: 5.2067 || 10iter: 5.1660 sec.\n",
      "Iteration 6650 || Loss: 5.5145 || 10iter: 4.2625 sec.\n",
      "Iteration 6660 || Loss: 4.4785 || 10iter: 3.4485 sec.\n",
      "-------------\n",
      "epoch 101 || Epoch_TRAIN_Loss:330.5635 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.9767 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 102/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6670 || Loss: 4.7508 || 10iter: 5.4230 sec.\n",
      "Iteration 6680 || Loss: 4.6686 || 10iter: 4.1122 sec.\n",
      "Iteration 6690 || Loss: 5.3955 || 10iter: 3.5257 sec.\n",
      "Iteration 6700 || Loss: 5.6397 || 10iter: 4.0138 sec.\n",
      "Iteration 6710 || Loss: 5.0526 || 10iter: 6.6971 sec.\n",
      "Iteration 6720 || Loss: 4.5929 || 10iter: 5.0256 sec.\n",
      "Iteration 6730 || Loss: 4.8563 || 10iter: 3.8072 sec.\n",
      "-------------\n",
      "epoch 102 || Epoch_TRAIN_Loss:333.3340 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.3831 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 103/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6740 || Loss: 5.5137 || 10iter: 7.9643 sec.\n",
      "Iteration 6750 || Loss: 5.0668 || 10iter: 5.2441 sec.\n",
      "Iteration 6760 || Loss: 5.1078 || 10iter: 4.4494 sec.\n",
      "Iteration 6770 || Loss: 4.7882 || 10iter: 3.3781 sec.\n",
      "Iteration 6780 || Loss: 5.0428 || 10iter: 3.9304 sec.\n",
      "Iteration 6790 || Loss: 4.3760 || 10iter: 5.1823 sec.\n",
      "-------------\n",
      "epoch 103 || Epoch_TRAIN_Loss:333.4575 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.1438 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 104/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6800 || Loss: 4.3919 || 10iter: 4.3891 sec.\n",
      "Iteration 6810 || Loss: 5.0948 || 10iter: 5.7394 sec.\n",
      "Iteration 6820 || Loss: 5.3607 || 10iter: 5.0084 sec.\n",
      "Iteration 6830 || Loss: 4.7311 || 10iter: 4.8912 sec.\n",
      "Iteration 6840 || Loss: 5.3978 || 10iter: 4.0408 sec.\n",
      "Iteration 6850 || Loss: 5.2914 || 10iter: 3.4769 sec.\n",
      "Iteration 6860 || Loss: 5.2416 || 10iter: 2.7029 sec.\n",
      "-------------\n",
      "epoch 104 || Epoch_TRAIN_Loss:333.0180 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.7183 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 105/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6870 || Loss: 4.4593 || 10iter: 5.0232 sec.\n",
      "Iteration 6880 || Loss: 5.2025 || 10iter: 3.5207 sec.\n",
      "Iteration 6890 || Loss: 4.9982 || 10iter: 3.1906 sec.\n",
      "Iteration 6900 || Loss: 4.8588 || 10iter: 6.6228 sec.\n",
      "Iteration 6910 || Loss: 6.1843 || 10iter: 5.2012 sec.\n",
      "Iteration 6920 || Loss: 5.3836 || 10iter: 3.8120 sec.\n",
      "Iteration 6930 || Loss: 5.3900 || 10iter: 3.0424 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 105 || Epoch_TRAIN_Loss:336.5094 ||Epoch_VAL_Loss:170.2864\n",
      "timer:  41.5066 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 106/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6940 || Loss: 4.6495 || 10iter: 6.1145 sec.\n",
      "Iteration 6950 || Loss: 5.0203 || 10iter: 4.0045 sec.\n",
      "Iteration 6960 || Loss: 4.8834 || 10iter: 7.0836 sec.\n",
      "Iteration 6970 || Loss: 5.5671 || 10iter: 5.1134 sec.\n",
      "Iteration 6980 || Loss: 5.2720 || 10iter: 4.6466 sec.\n",
      "Iteration 6990 || Loss: 4.9018 || 10iter: 3.6766 sec.\n",
      "-------------\n",
      "epoch 106 || Epoch_TRAIN_Loss:332.8630 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.6921 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 107/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7000 || Loss: 6.0594 || 10iter: 5.6236 sec.\n",
      "Iteration 7010 || Loss: 4.9491 || 10iter: 5.1456 sec.\n",
      "Iteration 7020 || Loss: 4.7626 || 10iter: 3.5586 sec.\n",
      "Iteration 7030 || Loss: 5.0104 || 10iter: 3.7390 sec.\n",
      "Iteration 7040 || Loss: 4.4156 || 10iter: 6.7149 sec.\n",
      "Iteration 7050 || Loss: 4.9877 || 10iter: 4.1752 sec.\n",
      "Iteration 7060 || Loss: 4.4686 || 10iter: 3.5453 sec.\n",
      "-------------\n",
      "epoch 107 || Epoch_TRAIN_Loss:336.8686 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2719 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 108/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7070 || Loss: 4.8921 || 10iter: 8.0707 sec.\n",
      "Iteration 7080 || Loss: 4.6129 || 10iter: 5.1127 sec.\n",
      "Iteration 7090 || Loss: 4.8098 || 10iter: 4.6288 sec.\n",
      "Iteration 7100 || Loss: 4.8157 || 10iter: 4.1226 sec.\n",
      "Iteration 7110 || Loss: 5.5100 || 10iter: 3.5287 sec.\n",
      "Iteration 7120 || Loss: 5.2948 || 10iter: 3.9033 sec.\n",
      "-------------\n",
      "epoch 108 || Epoch_TRAIN_Loss:334.2056 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.7161 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 109/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7130 || Loss: 5.4780 || 10iter: 4.0695 sec.\n",
      "Iteration 7140 || Loss: 4.5472 || 10iter: 5.5749 sec.\n",
      "Iteration 7150 || Loss: 5.0245 || 10iter: 5.5728 sec.\n",
      "Iteration 7160 || Loss: 5.4282 || 10iter: 5.3420 sec.\n",
      "Iteration 7170 || Loss: 5.0028 || 10iter: 5.1784 sec.\n",
      "Iteration 7180 || Loss: 5.2446 || 10iter: 3.4889 sec.\n",
      "Iteration 7190 || Loss: 4.5875 || 10iter: 2.9178 sec.\n",
      "-------------\n",
      "epoch 109 || Epoch_TRAIN_Loss:330.4913 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5196 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 110/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7200 || Loss: 5.2397 || 10iter: 8.5654 sec.\n",
      "Iteration 7210 || Loss: 5.2916 || 10iter: 5.0503 sec.\n",
      "Iteration 7220 || Loss: 5.0323 || 10iter: 5.2099 sec.\n",
      "Iteration 7230 || Loss: 5.1599 || 10iter: 5.0529 sec.\n",
      "Iteration 7240 || Loss: 5.1267 || 10iter: 4.8851 sec.\n",
      "Iteration 7250 || Loss: 4.1343 || 10iter: 4.2154 sec.\n",
      "Iteration 7260 || Loss: 5.3095 || 10iter: 2.9902 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 110 || Epoch_TRAIN_Loss:326.5050 ||Epoch_VAL_Loss:168.0451\n",
      "timer:  43.7669 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 111/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7270 || Loss: 4.4922 || 10iter: 6.3114 sec.\n",
      "Iteration 7280 || Loss: 4.9392 || 10iter: 3.8765 sec.\n",
      "Iteration 7290 || Loss: 4.9321 || 10iter: 6.8705 sec.\n",
      "Iteration 7300 || Loss: 4.7295 || 10iter: 5.0108 sec.\n",
      "Iteration 7310 || Loss: 5.3353 || 10iter: 5.3194 sec.\n",
      "Iteration 7320 || Loss: 5.7434 || 10iter: 3.4463 sec.\n",
      "-------------\n",
      "epoch 111 || Epoch_TRAIN_Loss:338.0759 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8962 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 112/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7330 || Loss: 4.7596 || 10iter: 5.7147 sec.\n",
      "Iteration 7340 || Loss: 5.3802 || 10iter: 4.4023 sec.\n",
      "Iteration 7350 || Loss: 4.8468 || 10iter: 3.3830 sec.\n",
      "Iteration 7360 || Loss: 4.8417 || 10iter: 3.7631 sec.\n",
      "Iteration 7370 || Loss: 4.8659 || 10iter: 6.9046 sec.\n",
      "Iteration 7380 || Loss: 5.1680 || 10iter: 4.7725 sec.\n",
      "Iteration 7390 || Loss: 4.5872 || 10iter: 3.4376 sec.\n",
      "-------------\n",
      "epoch 112 || Epoch_TRAIN_Loss:327.4916 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.1651 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 113/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7400 || Loss: 4.9684 || 10iter: 8.4274 sec.\n",
      "Iteration 7410 || Loss: 4.9709 || 10iter: 5.6534 sec.\n",
      "Iteration 7420 || Loss: 4.4108 || 10iter: 4.5757 sec.\n",
      "Iteration 7430 || Loss: 4.7467 || 10iter: 3.1042 sec.\n",
      "Iteration 7440 || Loss: 5.2273 || 10iter: 3.4762 sec.\n",
      "Iteration 7450 || Loss: 4.9126 || 10iter: 5.6674 sec.\n",
      "-------------\n",
      "epoch 113 || Epoch_TRAIN_Loss:326.4717 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8519 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 114/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7460 || Loss: 5.1688 || 10iter: 3.9603 sec.\n",
      "Iteration 7470 || Loss: 5.1433 || 10iter: 5.7243 sec.\n",
      "Iteration 7480 || Loss: 4.6256 || 10iter: 4.3318 sec.\n",
      "Iteration 7490 || Loss: 4.3604 || 10iter: 5.1535 sec.\n",
      "Iteration 7500 || Loss: 4.5564 || 10iter: 4.8400 sec.\n",
      "Iteration 7510 || Loss: 4.6913 || 10iter: 3.5670 sec.\n",
      "Iteration 7520 || Loss: 4.6262 || 10iter: 3.1823 sec.\n",
      "-------------\n",
      "epoch 114 || Epoch_TRAIN_Loss:332.6653 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.9722 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 115/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7530 || Loss: 5.0072 || 10iter: 8.9969 sec.\n",
      "Iteration 7540 || Loss: 4.2424 || 10iter: 5.0946 sec.\n",
      "Iteration 7550 || Loss: 4.9723 || 10iter: 5.7136 sec.\n",
      "Iteration 7560 || Loss: 4.6370 || 10iter: 5.4032 sec.\n",
      "Iteration 7570 || Loss: 5.4033 || 10iter: 4.7831 sec.\n",
      "Iteration 7580 || Loss: 5.0113 || 10iter: 4.1396 sec.\n",
      "Iteration 7590 || Loss: 6.6519 || 10iter: 2.9827 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 115 || Epoch_TRAIN_Loss:325.5828 ||Epoch_VAL_Loss:169.8055\n",
      "timer:  47.8087 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 116/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7600 || Loss: 5.4692 || 10iter: 9.2908 sec.\n",
      "Iteration 7610 || Loss: 5.2017 || 10iter: 5.1157 sec.\n",
      "Iteration 7620 || Loss: 5.2028 || 10iter: 5.6309 sec.\n",
      "Iteration 7630 || Loss: 5.1837 || 10iter: 4.5834 sec.\n",
      "Iteration 7640 || Loss: 4.9744 || 10iter: 3.5106 sec.\n",
      "Iteration 7650 || Loss: 5.1002 || 10iter: 3.0108 sec.\n",
      "-------------\n",
      "epoch 116 || Epoch_TRAIN_Loss:329.6948 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2594 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 117/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7660 || Loss: 4.7307 || 10iter: 4.7035 sec.\n",
      "Iteration 7670 || Loss: 4.7950 || 10iter: 3.2154 sec.\n",
      "Iteration 7680 || Loss: 4.8183 || 10iter: 3.5833 sec.\n",
      "Iteration 7690 || Loss: 4.9228 || 10iter: 6.4500 sec.\n",
      "Iteration 7700 || Loss: 5.4957 || 10iter: 5.2174 sec.\n",
      "Iteration 7710 || Loss: 4.5624 || 10iter: 4.2980 sec.\n",
      "Iteration 7720 || Loss: 4.3377 || 10iter: 3.5008 sec.\n",
      "-------------\n",
      "epoch 117 || Epoch_TRAIN_Loss:329.4826 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.7236 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 118/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7730 || Loss: 4.4781 || 10iter: 8.2989 sec.\n",
      "Iteration 7740 || Loss: 4.8111 || 10iter: 5.1113 sec.\n",
      "Iteration 7750 || Loss: 4.7985 || 10iter: 3.5295 sec.\n",
      "Iteration 7760 || Loss: 4.9348 || 10iter: 3.1688 sec.\n",
      "Iteration 7770 || Loss: 4.6834 || 10iter: 6.3182 sec.\n",
      "Iteration 7780 || Loss: 5.1668 || 10iter: 4.2536 sec.\n",
      "-------------\n",
      "epoch 118 || Epoch_TRAIN_Loss:322.8650 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4372 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 119/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7790 || Loss: 5.0337 || 10iter: 4.3253 sec.\n",
      "Iteration 7800 || Loss: 4.7537 || 10iter: 5.6284 sec.\n",
      "Iteration 7810 || Loss: 5.7854 || 10iter: 4.7876 sec.\n",
      "Iteration 7820 || Loss: 4.5696 || 10iter: 4.8117 sec.\n",
      "Iteration 7830 || Loss: 5.0531 || 10iter: 4.1561 sec.\n",
      "Iteration 7840 || Loss: 4.7093 || 10iter: 3.3896 sec.\n",
      "Iteration 7850 || Loss: 4.2648 || 10iter: 3.3982 sec.\n",
      "-------------\n",
      "epoch 119 || Epoch_TRAIN_Loss:326.8371 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.7453 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 120/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7860 || Loss: 4.2437 || 10iter: 7.1116 sec.\n",
      "Iteration 7870 || Loss: 4.6080 || 10iter: 4.6111 sec.\n",
      "Iteration 7880 || Loss: 4.6190 || 10iter: 5.2532 sec.\n",
      "Iteration 7890 || Loss: 4.8239 || 10iter: 5.3287 sec.\n",
      "Iteration 7900 || Loss: 4.7408 || 10iter: 4.6642 sec.\n",
      "Iteration 7910 || Loss: 5.5149 || 10iter: 3.7719 sec.\n",
      "Iteration 7920 || Loss: 4.8264 || 10iter: 3.0067 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 120 || Epoch_TRAIN_Loss:333.5950 ||Epoch_VAL_Loss:166.9986\n",
      "timer:  46.0800 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 121/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7930 || Loss: 5.4190 || 10iter: 8.5365 sec.\n",
      "Iteration 7940 || Loss: 4.9114 || 10iter: 4.9706 sec.\n",
      "Iteration 7950 || Loss: 5.3791 || 10iter: 4.9458 sec.\n",
      "Iteration 7960 || Loss: 5.3346 || 10iter: 4.3222 sec.\n",
      "Iteration 7970 || Loss: 4.7099 || 10iter: 3.3868 sec.\n",
      "Iteration 7980 || Loss: 4.4216 || 10iter: 2.8758 sec.\n",
      "-------------\n",
      "epoch 121 || Epoch_TRAIN_Loss:320.6859 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1428 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 122/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7990 || Loss: 5.1955 || 10iter: 6.8344 sec.\n",
      "Iteration 8000 || Loss: 5.4693 || 10iter: 5.2584 sec.\n",
      "Iteration 8010 || Loss: 5.3011 || 10iter: 4.6302 sec.\n",
      "Iteration 8020 || Loss: 4.3763 || 10iter: 5.4127 sec.\n",
      "Iteration 8030 || Loss: 4.8311 || 10iter: 5.2171 sec.\n",
      "Iteration 8040 || Loss: 4.8131 || 10iter: 3.9192 sec.\n",
      "Iteration 8050 || Loss: 4.7755 || 10iter: 3.0831 sec.\n",
      "-------------\n",
      "epoch 122 || Epoch_TRAIN_Loss:320.6242 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.1287 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 123/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8060 || Loss: 5.5585 || 10iter: 6.8604 sec.\n",
      "Iteration 8070 || Loss: 5.0747 || 10iter: 4.0845 sec.\n",
      "Iteration 8080 || Loss: 5.8026 || 10iter: 3.5472 sec.\n",
      "Iteration 8090 || Loss: 4.3248 || 10iter: 2.8531 sec.\n",
      "Iteration 8100 || Loss: 5.3263 || 10iter: 6.4370 sec.\n",
      "Iteration 8110 || Loss: 5.0262 || 10iter: 3.9376 sec.\n",
      "-------------\n",
      "epoch 123 || Epoch_TRAIN_Loss:326.7986 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  30.6124 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 124/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8120 || Loss: 4.5906 || 10iter: 4.4444 sec.\n",
      "Iteration 8130 || Loss: 5.4890 || 10iter: 4.9126 sec.\n",
      "Iteration 8140 || Loss: 5.6998 || 10iter: 5.1189 sec.\n",
      "Iteration 8150 || Loss: 4.1188 || 10iter: 5.0133 sec.\n",
      "Iteration 8160 || Loss: 4.6416 || 10iter: 3.6271 sec.\n",
      "Iteration 8170 || Loss: 5.0896 || 10iter: 3.3264 sec.\n",
      "Iteration 8180 || Loss: 4.7809 || 10iter: 3.2399 sec.\n",
      "-------------\n",
      "epoch 124 || Epoch_TRAIN_Loss:319.0992 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.2937 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 125/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8190 || Loss: 4.7300 || 10iter: 6.6461 sec.\n",
      "Iteration 8200 || Loss: 4.6912 || 10iter: 5.8778 sec.\n",
      "Iteration 8210 || Loss: 4.4385 || 10iter: 5.1536 sec.\n",
      "Iteration 8220 || Loss: 6.1893 || 10iter: 5.5065 sec.\n",
      "Iteration 8230 || Loss: 4.6899 || 10iter: 5.0624 sec.\n",
      "Iteration 8240 || Loss: 4.9891 || 10iter: 3.2349 sec.\n",
      "Iteration 8250 || Loss: 5.2263 || 10iter: 2.9126 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 125 || Epoch_TRAIN_Loss:324.4987 ||Epoch_VAL_Loss:165.8006\n",
      "timer:  46.3794 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 126/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8260 || Loss: 5.0909 || 10iter: 8.7726 sec.\n",
      "Iteration 8270 || Loss: 4.7142 || 10iter: 4.9213 sec.\n",
      "Iteration 8280 || Loss: 4.3833 || 10iter: 5.4611 sec.\n",
      "Iteration 8290 || Loss: 5.1296 || 10iter: 4.1346 sec.\n",
      "Iteration 8300 || Loss: 4.7497 || 10iter: 3.3461 sec.\n",
      "Iteration 8310 || Loss: 5.2000 || 10iter: 3.0704 sec.\n",
      "-------------\n",
      "epoch 126 || Epoch_TRAIN_Loss:322.9627 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5995 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 127/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8320 || Loss: 4.3719 || 10iter: 5.5644 sec.\n",
      "Iteration 8330 || Loss: 4.8118 || 10iter: 5.4483 sec.\n",
      "Iteration 8340 || Loss: 4.8637 || 10iter: 4.8186 sec.\n",
      "Iteration 8350 || Loss: 4.9313 || 10iter: 4.6554 sec.\n",
      "Iteration 8360 || Loss: 5.2658 || 10iter: 4.8803 sec.\n",
      "Iteration 8370 || Loss: 4.8711 || 10iter: 4.3820 sec.\n",
      "Iteration 8380 || Loss: 4.9715 || 10iter: 3.1466 sec.\n",
      "-------------\n",
      "epoch 127 || Epoch_TRAIN_Loss:327.4274 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.6798 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 128/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8390 || Loss: 4.5989 || 10iter: 5.3425 sec.\n",
      "Iteration 8400 || Loss: 5.2385 || 10iter: 7.0484 sec.\n",
      "Iteration 8410 || Loss: 4.7616 || 10iter: 6.1371 sec.\n",
      "Iteration 8420 || Loss: 5.1088 || 10iter: 5.5594 sec.\n",
      "Iteration 8430 || Loss: 5.4393 || 10iter: 4.8485 sec.\n",
      "Iteration 8440 || Loss: 5.1597 || 10iter: 3.7352 sec.\n",
      "-------------\n",
      "epoch 128 || Epoch_TRAIN_Loss:325.0485 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.3810 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 129/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8450 || Loss: 4.6176 || 10iter: 4.4044 sec.\n",
      "Iteration 8460 || Loss: 5.3746 || 10iter: 3.5587 sec.\n",
      "Iteration 8470 || Loss: 4.3954 || 10iter: 3.4591 sec.\n",
      "Iteration 8480 || Loss: 4.5788 || 10iter: 4.6207 sec.\n",
      "Iteration 8490 || Loss: 5.0249 || 10iter: 3.5967 sec.\n",
      "Iteration 8500 || Loss: 4.3568 || 10iter: 2.9964 sec.\n",
      "Iteration 8510 || Loss: 5.0629 || 10iter: 2.7939 sec.\n",
      "-------------\n",
      "epoch 129 || Epoch_TRAIN_Loss:320.2180 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.4902 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 130/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8520 || Loss: 5.1128 || 10iter: 7.1871 sec.\n",
      "Iteration 8530 || Loss: 4.8602 || 10iter: 4.9726 sec.\n",
      "Iteration 8540 || Loss: 4.8701 || 10iter: 5.2550 sec.\n",
      "Iteration 8550 || Loss: 4.5935 || 10iter: 5.1303 sec.\n",
      "Iteration 8560 || Loss: 4.1438 || 10iter: 5.0995 sec.\n",
      "Iteration 8570 || Loss: 5.0767 || 10iter: 3.7465 sec.\n",
      "Iteration 8580 || Loss: 6.4428 || 10iter: 2.9809 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 130 || Epoch_TRAIN_Loss:322.0315 ||Epoch_VAL_Loss:165.6329\n",
      "timer:  45.9806 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 131/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8590 || Loss: 4.7535 || 10iter: 9.1180 sec.\n",
      "Iteration 8600 || Loss: 4.7375 || 10iter: 4.5665 sec.\n",
      "Iteration 8610 || Loss: 4.8071 || 10iter: 5.0183 sec.\n",
      "Iteration 8620 || Loss: 4.7187 || 10iter: 4.5043 sec.\n",
      "Iteration 8630 || Loss: 5.3719 || 10iter: 3.5344 sec.\n",
      "Iteration 8640 || Loss: 4.3278 || 10iter: 2.9591 sec.\n",
      "-------------\n",
      "epoch 131 || Epoch_TRAIN_Loss:316.2151 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5757 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 132/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8650 || Loss: 5.3291 || 10iter: 6.2008 sec.\n",
      "Iteration 8660 || Loss: 4.4462 || 10iter: 6.0072 sec.\n",
      "Iteration 8670 || Loss: 5.1066 || 10iter: 4.9139 sec.\n",
      "Iteration 8680 || Loss: 5.0150 || 10iter: 5.2086 sec.\n",
      "Iteration 8690 || Loss: 4.9889 || 10iter: 5.2538 sec.\n",
      "Iteration 8700 || Loss: 4.9963 || 10iter: 3.8134 sec.\n",
      "Iteration 8710 || Loss: 4.7086 || 10iter: 3.1124 sec.\n",
      "-------------\n",
      "epoch 132 || Epoch_TRAIN_Loss:319.1656 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.3602 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 133/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8720 || Loss: 4.4948 || 10iter: 7.1342 sec.\n",
      "Iteration 8730 || Loss: 5.0700 || 10iter: 6.5944 sec.\n",
      "Iteration 8740 || Loss: 4.9359 || 10iter: 5.5022 sec.\n",
      "Iteration 8750 || Loss: 4.9211 || 10iter: 5.7292 sec.\n",
      "Iteration 8760 || Loss: 4.7952 || 10iter: 4.6505 sec.\n",
      "Iteration 8770 || Loss: 5.6508 || 10iter: 4.1810 sec.\n",
      "-------------\n",
      "epoch 133 || Epoch_TRAIN_Loss:323.5851 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.6188 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 134/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8780 || Loss: 4.7489 || 10iter: 2.5533 sec.\n",
      "Iteration 8790 || Loss: 4.7044 || 10iter: 4.2765 sec.\n",
      "Iteration 8800 || Loss: 5.0454 || 10iter: 4.1957 sec.\n",
      "Iteration 8810 || Loss: 4.7788 || 10iter: 7.0775 sec.\n",
      "Iteration 8820 || Loss: 4.6720 || 10iter: 4.6391 sec.\n",
      "Iteration 8830 || Loss: 5.6000 || 10iter: 5.3279 sec.\n",
      "Iteration 8840 || Loss: 5.2499 || 10iter: 3.4427 sec.\n",
      "-------------\n",
      "epoch 134 || Epoch_TRAIN_Loss:320.9266 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.1165 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 135/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8850 || Loss: 4.5736 || 10iter: 6.6766 sec.\n",
      "Iteration 8860 || Loss: 4.6574 || 10iter: 4.4855 sec.\n",
      "Iteration 8870 || Loss: 4.5451 || 10iter: 3.2055 sec.\n",
      "Iteration 8880 || Loss: 5.6156 || 10iter: 3.0599 sec.\n",
      "Iteration 8890 || Loss: 5.4693 || 10iter: 4.5659 sec.\n",
      "Iteration 8900 || Loss: 5.1324 || 10iter: 2.7383 sec.\n",
      "Iteration 8910 || Loss: 5.4507 || 10iter: 2.0083 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 135 || Epoch_TRAIN_Loss:317.2390 ||Epoch_VAL_Loss:165.4389\n",
      "timer:  38.6268 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 136/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8920 || Loss: 4.4013 || 10iter: 9.0975 sec.\n",
      "Iteration 8930 || Loss: 4.6817 || 10iter: 5.1008 sec.\n",
      "Iteration 8940 || Loss: 4.9103 || 10iter: 5.1780 sec.\n",
      "Iteration 8950 || Loss: 4.9245 || 10iter: 3.9841 sec.\n",
      "Iteration 8960 || Loss: 5.1711 || 10iter: 3.5213 sec.\n",
      "Iteration 8970 || Loss: 5.0182 || 10iter: 3.2280 sec.\n",
      "-------------\n",
      "epoch 136 || Epoch_TRAIN_Loss:320.5935 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2182 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 137/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8980 || Loss: 4.6691 || 10iter: 6.0251 sec.\n",
      "Iteration 8990 || Loss: 4.3999 || 10iter: 5.0017 sec.\n",
      "Iteration 9000 || Loss: 4.3296 || 10iter: 4.8868 sec.\n",
      "Iteration 9010 || Loss: 4.8665 || 10iter: 5.1236 sec.\n",
      "Iteration 9020 || Loss: 4.9121 || 10iter: 4.9803 sec.\n",
      "Iteration 9030 || Loss: 6.3326 || 10iter: 3.6951 sec.\n",
      "Iteration 9040 || Loss: 4.7884 || 10iter: 3.1850 sec.\n",
      "-------------\n",
      "epoch 137 || Epoch_TRAIN_Loss:321.7056 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.6045 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 138/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9050 || Loss: 4.7643 || 10iter: 8.6611 sec.\n",
      "Iteration 9060 || Loss: 4.6867 || 10iter: 5.0825 sec.\n",
      "Iteration 9070 || Loss: 4.9312 || 10iter: 5.4723 sec.\n",
      "Iteration 9080 || Loss: 5.8581 || 10iter: 5.7301 sec.\n",
      "Iteration 9090 || Loss: 4.3045 || 10iter: 5.2012 sec.\n",
      "Iteration 9100 || Loss: 4.7390 || 10iter: 3.7760 sec.\n",
      "-------------\n",
      "epoch 138 || Epoch_TRAIN_Loss:319.7416 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.6780 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 139/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9110 || Loss: 5.7041 || 10iter: 3.0430 sec.\n",
      "Iteration 9120 || Loss: 4.4398 || 10iter: 4.2035 sec.\n",
      "Iteration 9130 || Loss: 4.6831 || 10iter: 5.2088 sec.\n",
      "Iteration 9140 || Loss: 4.4712 || 10iter: 6.0717 sec.\n",
      "Iteration 9150 || Loss: 4.9473 || 10iter: 4.7997 sec.\n",
      "Iteration 9160 || Loss: 4.7247 || 10iter: 5.5275 sec.\n",
      "Iteration 9170 || Loss: 4.9181 || 10iter: 3.5351 sec.\n",
      "-------------\n",
      "epoch 139 || Epoch_TRAIN_Loss:315.7422 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8028 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 140/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9180 || Loss: 4.7356 || 10iter: 6.5409 sec.\n",
      "Iteration 9190 || Loss: 4.3471 || 10iter: 3.6502 sec.\n",
      "Iteration 9200 || Loss: 5.8381 || 10iter: 3.2680 sec.\n",
      "Iteration 9210 || Loss: 4.2589 || 10iter: 5.8037 sec.\n",
      "Iteration 9220 || Loss: 4.9831 || 10iter: 5.4460 sec.\n",
      "Iteration 9230 || Loss: 4.6601 || 10iter: 4.2429 sec.\n",
      "Iteration 9240 || Loss: 5.3073 || 10iter: 3.3680 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 140 || Epoch_TRAIN_Loss:318.7905 ||Epoch_VAL_Loss:165.6053\n",
      "timer:  43.2398 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 141/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9250 || Loss: 4.5349 || 10iter: 6.3282 sec.\n",
      "Iteration 9260 || Loss: 4.6136 || 10iter: 3.5151 sec.\n",
      "Iteration 9270 || Loss: 4.4334 || 10iter: 4.2306 sec.\n",
      "Iteration 9280 || Loss: 5.1071 || 10iter: 3.2004 sec.\n",
      "Iteration 9290 || Loss: 4.5927 || 10iter: 2.8215 sec.\n",
      "Iteration 9300 || Loss: 4.6832 || 10iter: 3.7068 sec.\n",
      "-------------\n",
      "epoch 141 || Epoch_TRAIN_Loss:319.8851 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  26.9166 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 142/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9310 || Loss: 4.4031 || 10iter: 5.8337 sec.\n",
      "Iteration 9320 || Loss: 5.9132 || 10iter: 5.1623 sec.\n",
      "Iteration 9330 || Loss: 5.0969 || 10iter: 5.1670 sec.\n",
      "Iteration 9340 || Loss: 5.0758 || 10iter: 5.2858 sec.\n",
      "Iteration 9350 || Loss: 5.0510 || 10iter: 4.5313 sec.\n",
      "Iteration 9360 || Loss: 5.9297 || 10iter: 3.3532 sec.\n",
      "Iteration 9370 || Loss: 4.3802 || 10iter: 3.0278 sec.\n",
      "-------------\n",
      "epoch 142 || Epoch_TRAIN_Loss:322.8831 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.0982 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 143/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9380 || Loss: 5.0819 || 10iter: 9.4845 sec.\n",
      "Iteration 9390 || Loss: 4.4473 || 10iter: 5.4849 sec.\n",
      "Iteration 9400 || Loss: 4.8772 || 10iter: 5.6052 sec.\n",
      "Iteration 9410 || Loss: 4.4989 || 10iter: 5.0764 sec.\n",
      "Iteration 9420 || Loss: 5.3145 || 10iter: 5.5488 sec.\n",
      "Iteration 9430 || Loss: 4.5052 || 10iter: 3.9404 sec.\n",
      "-------------\n",
      "epoch 143 || Epoch_TRAIN_Loss:319.8814 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.8322 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 144/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9440 || Loss: 4.3581 || 10iter: 2.2609 sec.\n",
      "Iteration 9450 || Loss: 4.2990 || 10iter: 6.1491 sec.\n",
      "Iteration 9460 || Loss: 4.7964 || 10iter: 6.0588 sec.\n",
      "Iteration 9470 || Loss: 4.2954 || 10iter: 5.4767 sec.\n",
      "Iteration 9480 || Loss: 4.4810 || 10iter: 4.9951 sec.\n",
      "Iteration 9490 || Loss: 5.7659 || 10iter: 4.7891 sec.\n",
      "Iteration 9500 || Loss: 5.1276 || 10iter: 3.3348 sec.\n",
      "-------------\n",
      "epoch 144 || Epoch_TRAIN_Loss:320.4467 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.5771 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 145/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9510 || Loss: 4.6919 || 10iter: 5.6067 sec.\n",
      "Iteration 9520 || Loss: 5.9755 || 10iter: 3.5960 sec.\n",
      "Iteration 9530 || Loss: 8.2865 || 10iter: 5.3110 sec.\n",
      "Iteration 9540 || Loss: 4.8583 || 10iter: 5.9674 sec.\n",
      "Iteration 9550 || Loss: 4.5766 || 10iter: 4.9652 sec.\n",
      "Iteration 9560 || Loss: 4.7931 || 10iter: 4.2462 sec.\n",
      "Iteration 9570 || Loss: 4.0062 || 10iter: 3.1297 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 145 || Epoch_TRAIN_Loss:327.4488 ||Epoch_VAL_Loss:164.6513\n",
      "timer:  43.5690 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 146/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9580 || Loss: 4.5971 || 10iter: 6.4001 sec.\n",
      "Iteration 9590 || Loss: 4.9993 || 10iter: 6.5721 sec.\n",
      "Iteration 9600 || Loss: 5.2672 || 10iter: 5.3286 sec.\n",
      "Iteration 9610 || Loss: 5.8114 || 10iter: 4.8602 sec.\n",
      "Iteration 9620 || Loss: 4.1949 || 10iter: 4.8850 sec.\n",
      "Iteration 9630 || Loss: 5.1824 || 10iter: 3.4773 sec.\n",
      "-------------\n",
      "epoch 146 || Epoch_TRAIN_Loss:327.6493 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5899 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 147/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9640 || Loss: 4.4753 || 10iter: 5.3127 sec.\n",
      "Iteration 9650 || Loss: 4.4144 || 10iter: 3.8705 sec.\n",
      "Iteration 9660 || Loss: 4.5932 || 10iter: 3.2885 sec.\n",
      "Iteration 9670 || Loss: 4.5420 || 10iter: 4.6326 sec.\n",
      "Iteration 9680 || Loss: 5.1500 || 10iter: 3.6284 sec.\n",
      "Iteration 9690 || Loss: 4.6372 || 10iter: 2.6470 sec.\n",
      "Iteration 9700 || Loss: 5.4495 || 10iter: 2.5877 sec.\n",
      "-------------\n",
      "epoch 147 || Epoch_TRAIN_Loss:324.0311 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  26.8559 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 148/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9710 || Loss: 4.8528 || 10iter: 9.3668 sec.\n",
      "Iteration 9720 || Loss: 4.5512 || 10iter: 4.9362 sec.\n",
      "Iteration 9730 || Loss: 4.7952 || 10iter: 5.3215 sec.\n",
      "Iteration 9740 || Loss: 4.5369 || 10iter: 5.3572 sec.\n",
      "Iteration 9750 || Loss: 5.1063 || 10iter: 5.3740 sec.\n",
      "Iteration 9760 || Loss: 5.2791 || 10iter: 3.6216 sec.\n",
      "-------------\n",
      "epoch 148 || Epoch_TRAIN_Loss:324.5457 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.6513 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 149/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9770 || Loss: 4.5774 || 10iter: 2.8627 sec.\n",
      "Iteration 9780 || Loss: 5.4817 || 10iter: 6.5680 sec.\n",
      "Iteration 9790 || Loss: 5.2185 || 10iter: 5.6087 sec.\n",
      "Iteration 9800 || Loss: 4.9231 || 10iter: 5.4145 sec.\n",
      "Iteration 9810 || Loss: 4.7131 || 10iter: 4.8151 sec.\n",
      "Iteration 9820 || Loss: 5.5229 || 10iter: 5.1385 sec.\n",
      "Iteration 9830 || Loss: 4.3903 || 10iter: 3.4372 sec.\n",
      "-------------\n",
      "epoch 149 || Epoch_TRAIN_Loss:324.4582 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.3447 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 150/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9840 || Loss: 4.2973 || 10iter: 4.9512 sec.\n",
      "Iteration 9850 || Loss: 4.4723 || 10iter: 3.3871 sec.\n",
      "Iteration 9860 || Loss: 5.2605 || 10iter: 5.4159 sec.\n",
      "Iteration 9870 || Loss: 4.8823 || 10iter: 6.0342 sec.\n",
      "Iteration 9880 || Loss: 4.6012 || 10iter: 4.9554 sec.\n",
      "Iteration 9890 || Loss: 5.6163 || 10iter: 4.1049 sec.\n",
      "Iteration 9900 || Loss: 7.2228 || 10iter: 3.2075 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 150 || Epoch_TRAIN_Loss:325.5823 ||Epoch_VAL_Loss:164.1998\n",
      "timer:  42.3976 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 151/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9910 || Loss: 4.9383 || 10iter: 6.2534 sec.\n",
      "Iteration 9920 || Loss: 5.0587 || 10iter: 6.4381 sec.\n",
      "Iteration 9930 || Loss: 4.6162 || 10iter: 5.4385 sec.\n",
      "Iteration 9940 || Loss: 4.6025 || 10iter: 5.3900 sec.\n",
      "Iteration 9950 || Loss: 4.2766 || 10iter: 5.1212 sec.\n",
      "Iteration 9960 || Loss: 5.5761 || 10iter: 3.3883 sec.\n",
      "-------------\n",
      "epoch 151 || Epoch_TRAIN_Loss:324.0794 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.2220 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 152/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9970 || Loss: 4.4975 || 10iter: 5.1110 sec.\n",
      "Iteration 9980 || Loss: 4.8009 || 10iter: 3.5176 sec.\n",
      "Iteration 9990 || Loss: 4.9380 || 10iter: 3.2655 sec.\n",
      "Iteration 10000 || Loss: 4.3285 || 10iter: 6.6860 sec.\n",
      "Iteration 10010 || Loss: 4.7475 || 10iter: 5.8819 sec.\n",
      "Iteration 10020 || Loss: 5.3540 || 10iter: 3.9922 sec.\n",
      "Iteration 10030 || Loss: 4.8505 || 10iter: 3.3873 sec.\n",
      "-------------\n",
      "epoch 152 || Epoch_TRAIN_Loss:321.0266 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.6226 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 153/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10040 || Loss: 4.9303 || 10iter: 7.8829 sec.\n",
      "Iteration 10050 || Loss: 4.7501 || 10iter: 4.6546 sec.\n",
      "Iteration 10060 || Loss: 4.7320 || 10iter: 3.4539 sec.\n",
      "Iteration 10070 || Loss: 4.8545 || 10iter: 3.4329 sec.\n",
      "Iteration 10080 || Loss: 4.2549 || 10iter: 4.1560 sec.\n",
      "Iteration 10090 || Loss: 4.8244 || 10iter: 2.7885 sec.\n",
      "-------------\n",
      "epoch 153 || Epoch_TRAIN_Loss:321.9288 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  28.2593 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 154/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10100 || Loss: 4.5844 || 10iter: 2.4941 sec.\n",
      "Iteration 10110 || Loss: 5.4918 || 10iter: 6.5707 sec.\n",
      "Iteration 10120 || Loss: 4.6322 || 10iter: 5.8584 sec.\n",
      "Iteration 10130 || Loss: 4.9842 || 10iter: 5.2826 sec.\n",
      "Iteration 10140 || Loss: 4.8692 || 10iter: 4.6104 sec.\n",
      "Iteration 10150 || Loss: 4.8186 || 10iter: 4.4648 sec.\n",
      "Iteration 10160 || Loss: 5.2041 || 10iter: 3.6222 sec.\n",
      "-------------\n",
      "epoch 154 || Epoch_TRAIN_Loss:312.6768 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.2233 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 155/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10170 || Loss: 4.9143 || 10iter: 6.0786 sec.\n",
      "Iteration 10180 || Loss: 4.5597 || 10iter: 3.3015 sec.\n",
      "Iteration 10190 || Loss: 4.8802 || 10iter: 4.3970 sec.\n",
      "Iteration 10200 || Loss: 4.3257 || 10iter: 6.6025 sec.\n",
      "Iteration 10210 || Loss: 4.8845 || 10iter: 4.6166 sec.\n",
      "Iteration 10220 || Loss: 4.4519 || 10iter: 4.0570 sec.\n",
      "Iteration 10230 || Loss: 4.2154 || 10iter: 3.2864 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 155 || Epoch_TRAIN_Loss:319.6816 ||Epoch_VAL_Loss:164.7809\n",
      "timer:  43.2514 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 156/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10240 || Loss: 4.2513 || 10iter: 6.2836 sec.\n",
      "Iteration 10250 || Loss: 4.5592 || 10iter: 6.1996 sec.\n",
      "Iteration 10260 || Loss: 5.2996 || 10iter: 5.7247 sec.\n",
      "Iteration 10270 || Loss: 5.0345 || 10iter: 4.7075 sec.\n",
      "Iteration 10280 || Loss: 4.8028 || 10iter: 4.8683 sec.\n",
      "Iteration 10290 || Loss: 4.8025 || 10iter: 3.5457 sec.\n",
      "-------------\n",
      "epoch 156 || Epoch_TRAIN_Loss:320.3700 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.3585 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 157/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10300 || Loss: 4.7695 || 10iter: 5.5673 sec.\n",
      "Iteration 10310 || Loss: 4.4966 || 10iter: 3.7578 sec.\n",
      "Iteration 10320 || Loss: 4.5775 || 10iter: 3.1575 sec.\n",
      "Iteration 10330 || Loss: 4.7602 || 10iter: 4.4345 sec.\n",
      "Iteration 10340 || Loss: 4.9522 || 10iter: 6.7863 sec.\n",
      "Iteration 10350 || Loss: 5.1020 || 10iter: 4.7758 sec.\n",
      "Iteration 10360 || Loss: 4.8591 || 10iter: 3.5631 sec.\n",
      "-------------\n",
      "epoch 157 || Epoch_TRAIN_Loss:322.3384 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8601 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 158/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10370 || Loss: 5.1630 || 10iter: 8.2294 sec.\n",
      "Iteration 10380 || Loss: 4.5192 || 10iter: 5.2202 sec.\n",
      "Iteration 10390 || Loss: 4.9526 || 10iter: 4.0134 sec.\n",
      "Iteration 10400 || Loss: 4.8422 || 10iter: 3.4062 sec.\n",
      "Iteration 10410 || Loss: 5.0001 || 10iter: 4.3992 sec.\n",
      "Iteration 10420 || Loss: 4.6126 || 10iter: 5.2667 sec.\n",
      "-------------\n",
      "epoch 158 || Epoch_TRAIN_Loss:320.6376 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4742 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 159/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10430 || Loss: 4.7094 || 10iter: 3.9243 sec.\n",
      "Iteration 10440 || Loss: 4.6106 || 10iter: 6.4520 sec.\n",
      "Iteration 10450 || Loss: 4.2287 || 10iter: 5.2465 sec.\n",
      "Iteration 10460 || Loss: 4.5192 || 10iter: 5.3142 sec.\n",
      "Iteration 10470 || Loss: 4.9623 || 10iter: 4.3914 sec.\n",
      "Iteration 10480 || Loss: 4.5557 || 10iter: 3.4589 sec.\n",
      "Iteration 10490 || Loss: 5.8528 || 10iter: 2.8357 sec.\n",
      "-------------\n",
      "epoch 159 || Epoch_TRAIN_Loss:322.1452 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8828 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 160/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10500 || Loss: 4.2535 || 10iter: 5.6569 sec.\n",
      "Iteration 10510 || Loss: 4.9758 || 10iter: 2.8165 sec.\n",
      "Iteration 10520 || Loss: 4.6893 || 10iter: 4.0229 sec.\n",
      "Iteration 10530 || Loss: 5.0000 || 10iter: 6.9167 sec.\n",
      "Iteration 10540 || Loss: 4.9262 || 10iter: 4.6364 sec.\n",
      "Iteration 10550 || Loss: 5.6563 || 10iter: 4.5045 sec.\n",
      "Iteration 10560 || Loss: 4.5835 || 10iter: 3.0931 sec.\n",
      "-------------\n",
      "val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "epoch 160 || Epoch_TRAIN_Loss:316.5791 ||Epoch_VAL_Loss:164.7865\n",
      "timer:  42.1644 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 161/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10570 || Loss: 6.0005 || 10iter: 6.1736 sec.\n",
      "Iteration 10580 || Loss: 4.8639 || 10iter: 6.2610 sec.\n",
      "Iteration 10590 || Loss: 5.4910 || 10iter: 5.7215 sec.\n",
      "Iteration 10600 || Loss: 4.8533 || 10iter: 4.7865 sec.\n",
      "Iteration 10610 || Loss: 5.5204 || 10iter: 4.9801 sec.\n",
      "Iteration 10620 || Loss: 4.4629 || 10iter: 3.3868 sec.\n",
      "-------------\n",
      "epoch 161 || Epoch_TRAIN_Loss:322.1030 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4837 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 162/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10630 || Loss: 4.8515 || 10iter: 5.4857 sec.\n",
      "Iteration 10640 || Loss: 5.5003 || 10iter: 4.2352 sec.\n",
      "Iteration 10650 || Loss: 4.8769 || 10iter: 3.6301 sec.\n",
      "Iteration 10660 || Loss: 4.8427 || 10iter: 4.8025 sec.\n",
      "Iteration 10670 || Loss: 4.7288 || 10iter: 6.7819 sec.\n",
      "Iteration 10680 || Loss: 4.7539 || 10iter: 4.4086 sec.\n",
      "Iteration 10690 || Loss: 4.4997 || 10iter: 3.4952 sec.\n",
      "-------------\n",
      "epoch 162 || Epoch_TRAIN_Loss:319.8835 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.6040 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 163/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10700 || Loss: 4.5961 || 10iter: 7.6819 sec.\n",
      "Iteration 10710 || Loss: 4.4725 || 10iter: 5.1291 sec.\n",
      "Iteration 10720 || Loss: 5.3414 || 10iter: 4.4180 sec.\n",
      "Iteration 10730 || Loss: 4.7253 || 10iter: 3.4400 sec.\n",
      "Iteration 10740 || Loss: 4.8618 || 10iter: 4.2768 sec.\n",
      "Iteration 10750 || Loss: 4.9257 || 10iter: 5.0254 sec.\n",
      "-------------\n",
      "epoch 163 || Epoch_TRAIN_Loss:315.6545 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.6016 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 164/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10760 || Loss: 4.4060 || 10iter: 4.3595 sec.\n",
      "Iteration 10770 || Loss: 4.9735 || 10iter: 6.1635 sec.\n",
      "Iteration 10780 || Loss: 5.2043 || 10iter: 4.9819 sec.\n",
      "Iteration 10790 || Loss: 4.6668 || 10iter: 5.2157 sec.\n",
      "Iteration 10800 || Loss: 6.6687 || 10iter: 4.5902 sec.\n",
      "Iteration 10810 || Loss: 4.8326 || 10iter: 3.2986 sec.\n",
      "Iteration 10820 || Loss: 5.5054 || 10iter: 3.1066 sec.\n",
      "-------------\n",
      "epoch 164 || Epoch_TRAIN_Loss:322.9096 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.3593 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 165/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10830 || Loss: 4.7855 || 10iter: 8.0301 sec.\n",
      "Iteration 10840 || Loss: 4.4392 || 10iter: 4.9814 sec.\n",
      "Iteration 10850 || Loss: 5.2931 || 10iter: 5.3288 sec.\n",
      "Iteration 10860 || Loss: 4.8811 || 10iter: 5.5194 sec.\n",
      "Iteration 10870 || Loss: 4.3159 || 10iter: 5.1335 sec.\n",
      "Iteration 10880 || Loss: 5.2900 || 10iter: 3.9177 sec.\n",
      "Iteration 10890 || Loss: 4.6199 || 10iter: 2.9114 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 165 || Epoch_TRAIN_Loss:322.1963 ||Epoch_VAL_Loss:164.3749\n",
      "timer:  43.8417 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 166/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10900 || Loss: 5.1745 || 10iter: 6.5505 sec.\n",
      "Iteration 10910 || Loss: 4.6702 || 10iter: 6.5699 sec.\n",
      "Iteration 10920 || Loss: 5.2571 || 10iter: 4.8666 sec.\n",
      "Iteration 10930 || Loss: 4.5866 || 10iter: 5.1883 sec.\n",
      "Iteration 10940 || Loss: 4.9652 || 10iter: 4.9875 sec.\n",
      "Iteration 10950 || Loss: 5.0807 || 10iter: 3.7432 sec.\n",
      "-------------\n",
      "epoch 166 || Epoch_TRAIN_Loss:326.9635 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.1548 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 167/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10960 || Loss: 5.0945 || 10iter: 4.9219 sec.\n",
      "Iteration 10970 || Loss: 4.9455 || 10iter: 3.8580 sec.\n",
      "Iteration 10980 || Loss: 4.7222 || 10iter: 3.5284 sec.\n",
      "Iteration 10990 || Loss: 5.2982 || 10iter: 6.5755 sec.\n",
      "Iteration 11000 || Loss: 4.7265 || 10iter: 4.8480 sec.\n",
      "Iteration 11010 || Loss: 4.5220 || 10iter: 4.6993 sec.\n",
      "Iteration 11020 || Loss: 4.2474 || 10iter: 3.3044 sec.\n",
      "-------------\n",
      "epoch 167 || Epoch_TRAIN_Loss:318.6253 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5161 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 168/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11030 || Loss: 4.6433 || 10iter: 7.5844 sec.\n",
      "Iteration 11040 || Loss: 4.7979 || 10iter: 5.0256 sec.\n",
      "Iteration 11050 || Loss: 4.2565 || 10iter: 3.5312 sec.\n",
      "Iteration 11060 || Loss: 4.8154 || 10iter: 3.4145 sec.\n",
      "Iteration 11070 || Loss: 4.6317 || 10iter: 5.4632 sec.\n",
      "Iteration 11080 || Loss: 4.8717 || 10iter: 4.9917 sec.\n",
      "-------------\n",
      "epoch 168 || Epoch_TRAIN_Loss:317.9991 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.9272 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 169/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11090 || Loss: 4.7787 || 10iter: 4.0971 sec.\n",
      "Iteration 11100 || Loss: 5.6024 || 10iter: 6.2655 sec.\n",
      "Iteration 11110 || Loss: 4.2072 || 10iter: 5.8893 sec.\n",
      "Iteration 11120 || Loss: 5.4636 || 10iter: 5.1621 sec.\n",
      "Iteration 11130 || Loss: 5.0930 || 10iter: 3.3974 sec.\n",
      "Iteration 11140 || Loss: 4.2545 || 10iter: 3.2874 sec.\n",
      "Iteration 11150 || Loss: 5.2199 || 10iter: 4.4335 sec.\n",
      "-------------\n",
      "epoch 169 || Epoch_TRAIN_Loss:321.3250 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.3547 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 170/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11160 || Loss: 4.9798 || 10iter: 6.4574 sec.\n",
      "Iteration 11170 || Loss: 4.9790 || 10iter: 4.9495 sec.\n",
      "Iteration 11180 || Loss: 5.2060 || 10iter: 5.4546 sec.\n",
      "Iteration 11190 || Loss: 4.6308 || 10iter: 4.6112 sec.\n",
      "Iteration 11200 || Loss: 5.0558 || 10iter: 5.1952 sec.\n",
      "Iteration 11210 || Loss: 5.5681 || 10iter: 3.3422 sec.\n",
      "Iteration 11220 || Loss: 4.2632 || 10iter: 2.9998 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 170 || Epoch_TRAIN_Loss:317.4738 ||Epoch_VAL_Loss:163.9759\n",
      "timer:  45.0151 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 171/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11230 || Loss: 4.8346 || 10iter: 9.2542 sec.\n",
      "Iteration 11240 || Loss: 4.3374 || 10iter: 4.8627 sec.\n",
      "Iteration 11250 || Loss: 4.8229 || 10iter: 4.7397 sec.\n",
      "Iteration 11260 || Loss: 4.8603 || 10iter: 4.4410 sec.\n",
      "Iteration 11270 || Loss: 5.0548 || 10iter: 3.3915 sec.\n",
      "Iteration 11280 || Loss: 5.0998 || 10iter: 3.1046 sec.\n",
      "-------------\n",
      "epoch 171 || Epoch_TRAIN_Loss:322.2895 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.6626 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 172/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11290 || Loss: 4.5877 || 10iter: 4.3358 sec.\n",
      "Iteration 11300 || Loss: 4.2252 || 10iter: 3.2113 sec.\n",
      "Iteration 11310 || Loss: 4.8375 || 10iter: 3.7697 sec.\n",
      "Iteration 11320 || Loss: 4.8038 || 10iter: 7.0777 sec.\n",
      "Iteration 11330 || Loss: 5.2249 || 10iter: 5.1285 sec.\n",
      "Iteration 11340 || Loss: 4.8997 || 10iter: 4.2222 sec.\n",
      "Iteration 11350 || Loss: 4.9549 || 10iter: 3.4435 sec.\n",
      "-------------\n",
      "epoch 172 || Epoch_TRAIN_Loss:323.1393 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.9077 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 173/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11360 || Loss: 4.5868 || 10iter: 7.8056 sec.\n",
      "Iteration 11370 || Loss: 4.2545 || 10iter: 4.3536 sec.\n",
      "Iteration 11380 || Loss: 4.3331 || 10iter: 3.5196 sec.\n",
      "Iteration 11390 || Loss: 6.1340 || 10iter: 3.9063 sec.\n",
      "Iteration 11400 || Loss: 4.5569 || 10iter: 7.0255 sec.\n",
      "Iteration 11410 || Loss: 4.4582 || 10iter: 3.7184 sec.\n",
      "-------------\n",
      "epoch 173 || Epoch_TRAIN_Loss:316.2855 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2478 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 174/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11420 || Loss: 4.1012 || 10iter: 4.5060 sec.\n",
      "Iteration 11430 || Loss: 4.5018 || 10iter: 5.5844 sec.\n",
      "Iteration 11440 || Loss: 5.2552 || 10iter: 5.3139 sec.\n",
      "Iteration 11450 || Loss: 4.6155 || 10iter: 4.9823 sec.\n",
      "Iteration 11460 || Loss: 4.8745 || 10iter: 3.4197 sec.\n",
      "Iteration 11470 || Loss: 4.8875 || 10iter: 3.1331 sec.\n",
      "Iteration 11480 || Loss: 4.7762 || 10iter: 4.1424 sec.\n",
      "-------------\n",
      "epoch 174 || Epoch_TRAIN_Loss:321.6531 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8046 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 175/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11490 || Loss: 4.1337 || 10iter: 6.6861 sec.\n",
      "Iteration 11500 || Loss: 4.3610 || 10iter: 4.9597 sec.\n",
      "Iteration 11510 || Loss: 5.4886 || 10iter: 4.7528 sec.\n",
      "Iteration 11520 || Loss: 5.5101 || 10iter: 5.2591 sec.\n",
      "Iteration 11530 || Loss: 5.3613 || 10iter: 4.7009 sec.\n",
      "Iteration 11540 || Loss: 4.1142 || 10iter: 3.4005 sec.\n",
      "Iteration 11550 || Loss: 5.0983 || 10iter: 2.8950 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 175 || Epoch_TRAIN_Loss:321.0307 ||Epoch_VAL_Loss:164.0349\n",
      "timer:  44.8865 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 176/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11560 || Loss: 4.7058 || 10iter: 9.1420 sec.\n",
      "Iteration 11570 || Loss: 4.6724 || 10iter: 5.3245 sec.\n",
      "Iteration 11580 || Loss: 4.7470 || 10iter: 5.0385 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11590 || Loss: 4.8813 || 10iter: 3.8428 sec.\n",
      "Iteration 11600 || Loss: 4.3704 || 10iter: 3.4456 sec.\n",
      "Iteration 11610 || Loss: 5.2291 || 10iter: 4.4330 sec.\n",
      "-------------\n",
      "epoch 176 || Epoch_TRAIN_Loss:320.1908 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.7727 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 177/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11620 || Loss: 4.6144 || 10iter: 5.5711 sec.\n",
      "Iteration 11630 || Loss: 4.2258 || 10iter: 5.0864 sec.\n",
      "Iteration 11640 || Loss: 4.1578 || 10iter: 5.2454 sec.\n",
      "Iteration 11650 || Loss: 4.5457 || 10iter: 5.6194 sec.\n",
      "Iteration 11660 || Loss: 5.1554 || 10iter: 4.7318 sec.\n",
      "Iteration 11670 || Loss: 4.6454 || 10iter: 3.3378 sec.\n",
      "Iteration 11680 || Loss: 4.5446 || 10iter: 3.0090 sec.\n",
      "-------------\n",
      "epoch 177 || Epoch_TRAIN_Loss:319.8769 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5152 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 178/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11690 || Loss: 4.8656 || 10iter: 6.5267 sec.\n",
      "Iteration 11700 || Loss: 4.4989 || 10iter: 3.0973 sec.\n",
      "Iteration 11710 || Loss: 5.0630 || 10iter: 3.7827 sec.\n",
      "Iteration 11720 || Loss: 4.6052 || 10iter: 6.5607 sec.\n",
      "Iteration 11730 || Loss: 5.2581 || 10iter: 5.1036 sec.\n",
      "Iteration 11740 || Loss: 4.3691 || 10iter: 3.9725 sec.\n",
      "-------------\n",
      "epoch 178 || Epoch_TRAIN_Loss:320.3887 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.7051 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 179/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11750 || Loss: 4.3875 || 10iter: 4.2934 sec.\n",
      "Iteration 11760 || Loss: 6.5367 || 10iter: 5.9023 sec.\n",
      "Iteration 11770 || Loss: 5.9908 || 10iter: 4.1251 sec.\n",
      "Iteration 11780 || Loss: 5.3507 || 10iter: 4.0421 sec.\n",
      "Iteration 11790 || Loss: 5.0895 || 10iter: 3.9190 sec.\n",
      "Iteration 11800 || Loss: 4.8303 || 10iter: 6.2860 sec.\n",
      "Iteration 11810 || Loss: 5.6822 || 10iter: 3.4239 sec.\n",
      "-------------\n",
      "epoch 179 || Epoch_TRAIN_Loss:322.5828 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4565 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 180/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11820 || Loss: 4.5374 || 10iter: 7.0982 sec.\n",
      "Iteration 11830 || Loss: 5.1530 || 10iter: 4.7820 sec.\n",
      "Iteration 11840 || Loss: 5.1056 || 10iter: 5.3868 sec.\n",
      "Iteration 11850 || Loss: 4.9879 || 10iter: 4.6608 sec.\n",
      "Iteration 11860 || Loss: 4.6788 || 10iter: 3.5545 sec.\n",
      "Iteration 11870 || Loss: 5.2970 || 10iter: 2.9681 sec.\n",
      "Iteration 11880 || Loss: 7.8996 || 10iter: 4.3453 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 180 || Epoch_TRAIN_Loss:329.4411 ||Epoch_VAL_Loss:164.3319\n",
      "timer:  43.4830 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 181/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11890 || Loss: 4.4425 || 10iter: 8.6221 sec.\n",
      "Iteration 11900 || Loss: 4.8081 || 10iter: 4.9487 sec.\n",
      "Iteration 11910 || Loss: 4.4278 || 10iter: 4.4974 sec.\n",
      "Iteration 11920 || Loss: 4.5781 || 10iter: 3.3783 sec.\n",
      "Iteration 11930 || Loss: 4.3662 || 10iter: 4.0448 sec.\n",
      "Iteration 11940 || Loss: 4.9762 || 10iter: 4.9382 sec.\n",
      "-------------\n",
      "epoch 181 || Epoch_TRAIN_Loss:313.9606 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5004 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 182/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11950 || Loss: 4.6464 || 10iter: 5.7546 sec.\n",
      "Iteration 11960 || Loss: 4.8819 || 10iter: 5.4891 sec.\n",
      "Iteration 11970 || Loss: 4.7091 || 10iter: 4.9169 sec.\n",
      "Iteration 11980 || Loss: 4.9881 || 10iter: 5.0057 sec.\n",
      "Iteration 11990 || Loss: 4.6542 || 10iter: 4.4486 sec.\n",
      "Iteration 12000 || Loss: 4.6419 || 10iter: 3.4103 sec.\n",
      "Iteration 12010 || Loss: 4.7467 || 10iter: 3.0824 sec.\n",
      "-------------\n",
      "epoch 182 || Epoch_TRAIN_Loss:318.1594 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.0925 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 183/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12020 || Loss: 4.5202 || 10iter: 9.0469 sec.\n",
      "Iteration 12030 || Loss: 4.6527 || 10iter: 4.9844 sec.\n",
      "Iteration 12040 || Loss: 4.4429 || 10iter: 5.0010 sec.\n",
      "Iteration 12050 || Loss: 5.3842 || 10iter: 5.0511 sec.\n",
      "Iteration 12060 || Loss: 4.8930 || 10iter: 5.3170 sec.\n",
      "Iteration 12070 || Loss: 4.7401 || 10iter: 3.4802 sec.\n",
      "-------------\n",
      "epoch 183 || Epoch_TRAIN_Loss:319.4284 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.3939 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 184/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12080 || Loss: 6.1694 || 10iter: 3.2251 sec.\n",
      "Iteration 12090 || Loss: 4.4838 || 10iter: 4.5575 sec.\n",
      "Iteration 12100 || Loss: 4.4416 || 10iter: 3.4539 sec.\n",
      "Iteration 12110 || Loss: 5.8293 || 10iter: 3.0870 sec.\n",
      "Iteration 12120 || Loss: 4.9433 || 10iter: 3.0884 sec.\n",
      "Iteration 12130 || Loss: 4.4256 || 10iter: 5.7891 sec.\n",
      "Iteration 12140 || Loss: 4.2060 || 10iter: 4.4633 sec.\n",
      "-------------\n",
      "epoch 184 || Epoch_TRAIN_Loss:314.6878 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  29.2139 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 185/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12150 || Loss: 4.8163 || 10iter: 6.6744 sec.\n",
      "Iteration 12160 || Loss: 4.3496 || 10iter: 4.9639 sec.\n",
      "Iteration 12170 || Loss: 5.5763 || 10iter: 5.0871 sec.\n",
      "Iteration 12180 || Loss: 5.5244 || 10iter: 5.0495 sec.\n",
      "Iteration 12190 || Loss: 4.7683 || 10iter: 3.6434 sec.\n",
      "Iteration 12200 || Loss: 4.8451 || 10iter: 3.2487 sec.\n",
      "Iteration 12210 || Loss: 4.4074 || 10iter: 3.1310 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 185 || Epoch_TRAIN_Loss:318.3069 ||Epoch_VAL_Loss:164.2763\n",
      "timer:  44.2076 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 186/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12220 || Loss: 5.0841 || 10iter: 8.7850 sec.\n",
      "Iteration 12230 || Loss: 4.6824 || 10iter: 4.6331 sec.\n",
      "Iteration 12240 || Loss: 5.5636 || 10iter: 4.7925 sec.\n",
      "Iteration 12250 || Loss: 4.5160 || 10iter: 3.6977 sec.\n",
      "Iteration 12260 || Loss: 4.8448 || 10iter: 3.2698 sec.\n",
      "Iteration 12270 || Loss: 4.4180 || 10iter: 4.8218 sec.\n",
      "-------------\n",
      "epoch 186 || Epoch_TRAIN_Loss:321.0364 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.4632 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 187/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12280 || Loss: 4.8562 || 10iter: 5.3395 sec.\n",
      "Iteration 12290 || Loss: 4.2600 || 10iter: 5.2884 sec.\n",
      "Iteration 12300 || Loss: 5.1697 || 10iter: 5.3121 sec.\n",
      "Iteration 12310 || Loss: 4.2798 || 10iter: 4.9203 sec.\n",
      "Iteration 12320 || Loss: 4.4314 || 10iter: 4.6935 sec.\n",
      "Iteration 12330 || Loss: 4.0912 || 10iter: 3.3795 sec.\n",
      "Iteration 12340 || Loss: 4.6366 || 10iter: 3.0535 sec.\n",
      "-------------\n",
      "epoch 187 || Epoch_TRAIN_Loss:317.6776 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5991 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 188/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12350 || Loss: 4.6978 || 10iter: 10.1472 sec.\n",
      "Iteration 12360 || Loss: 4.4689 || 10iter: 4.4639 sec.\n",
      "Iteration 12370 || Loss: 4.2322 || 10iter: 5.3647 sec.\n",
      "Iteration 12380 || Loss: 4.9861 || 10iter: 5.1485 sec.\n",
      "Iteration 12390 || Loss: 4.8401 || 10iter: 5.1532 sec.\n",
      "Iteration 12400 || Loss: 4.5520 || 10iter: 3.8557 sec.\n",
      "-------------\n",
      "epoch 188 || Epoch_TRAIN_Loss:325.3401 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.6559 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 189/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12410 || Loss: 4.4655 || 10iter: 2.4566 sec.\n",
      "Iteration 12420 || Loss: 5.4821 || 10iter: 4.6799 sec.\n",
      "Iteration 12430 || Loss: 4.3556 || 10iter: 6.1875 sec.\n",
      "Iteration 12440 || Loss: 4.7514 || 10iter: 5.4774 sec.\n",
      "Iteration 12450 || Loss: 4.5084 || 10iter: 5.5230 sec.\n",
      "Iteration 12460 || Loss: 4.5315 || 10iter: 5.1945 sec.\n",
      "Iteration 12470 || Loss: 4.2918 || 10iter: 3.3172 sec.\n",
      "-------------\n",
      "epoch 189 || Epoch_TRAIN_Loss:321.1983 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.2557 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 190/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12480 || Loss: 5.5266 || 10iter: 6.6119 sec.\n",
      "Iteration 12490 || Loss: 4.4790 || 10iter: 3.6741 sec.\n",
      "Iteration 12500 || Loss: 5.0301 || 10iter: 3.5078 sec.\n",
      "Iteration 12510 || Loss: 5.1623 || 10iter: 4.6427 sec.\n",
      "Iteration 12520 || Loss: 4.9378 || 10iter: 3.6217 sec.\n",
      "Iteration 12530 || Loss: 4.8823 || 10iter: 2.4668 sec.\n",
      "Iteration 12540 || Loss: 5.5329 || 10iter: 2.7417 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 190 || Epoch_TRAIN_Loss:320.1053 ||Epoch_VAL_Loss:164.1124\n",
      "timer:  39.7737 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 191/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12550 || Loss: 5.0215 || 10iter: 9.0770 sec.\n",
      "Iteration 12560 || Loss: 4.7091 || 10iter: 4.9698 sec.\n",
      "Iteration 12570 || Loss: 4.5300 || 10iter: 4.1106 sec.\n",
      "Iteration 12580 || Loss: 4.2214 || 10iter: 3.5839 sec.\n",
      "Iteration 12590 || Loss: 4.6617 || 10iter: 3.7816 sec.\n",
      "Iteration 12600 || Loss: 5.1180 || 10iter: 5.3758 sec.\n",
      "-------------\n",
      "epoch 191 || Epoch_TRAIN_Loss:317.7079 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.9724 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 192/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12610 || Loss: 4.3798 || 10iter: 5.5793 sec.\n",
      "Iteration 12620 || Loss: 4.4647 || 10iter: 5.1616 sec.\n",
      "Iteration 12630 || Loss: 5.5162 || 10iter: 4.8770 sec.\n",
      "Iteration 12640 || Loss: 4.3813 || 10iter: 5.3605 sec.\n",
      "Iteration 12650 || Loss: 4.8488 || 10iter: 4.3335 sec.\n",
      "Iteration 12660 || Loss: 5.0741 || 10iter: 3.3244 sec.\n",
      "Iteration 12670 || Loss: 4.6730 || 10iter: 2.9862 sec.\n",
      "-------------\n",
      "epoch 192 || Epoch_TRAIN_Loss:322.8654 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5527 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 193/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12680 || Loss: 4.7536 || 10iter: 9.5488 sec.\n",
      "Iteration 12690 || Loss: 4.2966 || 10iter: 4.7999 sec.\n",
      "Iteration 12700 || Loss: 4.9862 || 10iter: 5.3979 sec.\n",
      "Iteration 12710 || Loss: 4.8596 || 10iter: 5.0418 sec.\n",
      "Iteration 12720 || Loss: 4.4760 || 10iter: 4.6922 sec.\n",
      "Iteration 12730 || Loss: 4.8177 || 10iter: 3.9262 sec.\n",
      "-------------\n",
      "epoch 193 || Epoch_TRAIN_Loss:321.2833 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.9907 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 194/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12740 || Loss: 4.2829 || 10iter: 3.2842 sec.\n",
      "Iteration 12750 || Loss: 4.2355 || 10iter: 4.6626 sec.\n",
      "Iteration 12760 || Loss: 5.1263 || 10iter: 6.1212 sec.\n",
      "Iteration 12770 || Loss: 4.8566 || 10iter: 5.2686 sec.\n",
      "Iteration 12780 || Loss: 4.4918 || 10iter: 4.9642 sec.\n",
      "Iteration 12790 || Loss: 5.1540 || 10iter: 5.5084 sec.\n",
      "Iteration 12800 || Loss: 4.2074 || 10iter: 3.7164 sec.\n",
      "-------------\n",
      "epoch 194 || Epoch_TRAIN_Loss:318.0022 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.0610 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 195/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12810 || Loss: 5.0031 || 10iter: 5.8133 sec.\n",
      "Iteration 12820 || Loss: 4.6834 || 10iter: 3.4453 sec.\n",
      "Iteration 12830 || Loss: 5.0372 || 10iter: 4.3608 sec.\n",
      "Iteration 12840 || Loss: 5.1808 || 10iter: 6.8334 sec.\n",
      "Iteration 12850 || Loss: 4.8789 || 10iter: 4.5480 sec.\n",
      "Iteration 12860 || Loss: 4.3645 || 10iter: 4.2906 sec.\n",
      "Iteration 12870 || Loss: 4.6944 || 10iter: 3.2575 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 195 || Epoch_TRAIN_Loss:318.9184 ||Epoch_VAL_Loss:164.2436\n",
      "timer:  43.5915 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 196/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12880 || Loss: 4.6636 || 10iter: 5.9671 sec.\n",
      "Iteration 12890 || Loss: 4.9106 || 10iter: 4.3966 sec.\n",
      "Iteration 12900 || Loss: 5.2432 || 10iter: 4.0582 sec.\n",
      "Iteration 12910 || Loss: 4.7907 || 10iter: 2.9672 sec.\n",
      "Iteration 12920 || Loss: 4.9255 || 10iter: 4.2112 sec.\n",
      "Iteration 12930 || Loss: 4.5683 || 10iter: 4.5877 sec.\n",
      "-------------\n",
      "epoch 196 || Epoch_TRAIN_Loss:320.0986 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  28.3198 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 197/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12940 || Loss: 4.9062 || 10iter: 5.1461 sec.\n",
      "Iteration 12950 || Loss: 4.4219 || 10iter: 5.1303 sec.\n",
      "Iteration 12960 || Loss: 4.5938 || 10iter: 5.0178 sec.\n",
      "Iteration 12970 || Loss: 4.5494 || 10iter: 5.5382 sec.\n",
      "Iteration 12980 || Loss: 4.3067 || 10iter: 3.9902 sec.\n",
      "Iteration 12990 || Loss: 4.7225 || 10iter: 3.2946 sec.\n",
      "Iteration 13000 || Loss: 4.4081 || 10iter: 2.8603 sec.\n",
      "-------------\n",
      "epoch 197 || Epoch_TRAIN_Loss:316.1378 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.9708 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 198/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13010 || Loss: 4.9584 || 10iter: 9.9684 sec.\n",
      "Iteration 13020 || Loss: 4.4377 || 10iter: 4.9282 sec.\n",
      "Iteration 13030 || Loss: 4.9296 || 10iter: 5.5535 sec.\n",
      "Iteration 13040 || Loss: 6.5013 || 10iter: 4.9703 sec.\n",
      "Iteration 13050 || Loss: 6.5428 || 10iter: 5.0758 sec.\n",
      "Iteration 13060 || Loss: 4.3089 || 10iter: 3.5534 sec.\n",
      "-------------\n",
      "epoch 198 || Epoch_TRAIN_Loss:326.9002 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.6722 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 199/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13070 || Loss: 4.6537 || 10iter: 3.3505 sec.\n",
      "Iteration 13080 || Loss: 5.0745 || 10iter: 5.6488 sec.\n",
      "Iteration 13090 || Loss: 4.8085 || 10iter: 6.0159 sec.\n",
      "Iteration 13100 || Loss: 4.9949 || 10iter: 5.2179 sec.\n",
      "Iteration 13110 || Loss: 6.1393 || 10iter: 4.9899 sec.\n",
      "Iteration 13120 || Loss: 4.3161 || 10iter: 4.6492 sec.\n",
      "Iteration 13130 || Loss: 4.4704 || 10iter: 3.7331 sec.\n",
      "-------------\n",
      "epoch 199 || Epoch_TRAIN_Loss:321.3370 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.1608 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 200/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13140 || Loss: 4.5405 || 10iter: 5.4590 sec.\n",
      "Iteration 13150 || Loss: 5.4372 || 10iter: 3.3867 sec.\n",
      "Iteration 13160 || Loss: 5.0032 || 10iter: 4.0505 sec.\n",
      "Iteration 13170 || Loss: 5.4292 || 10iter: 6.7794 sec.\n",
      "Iteration 13180 || Loss: 4.5635 || 10iter: 4.9964 sec.\n",
      "Iteration 13190 || Loss: 5.3549 || 10iter: 3.7658 sec.\n",
      "Iteration 13200 || Loss: 3.8892 || 10iter: 3.1010 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 200 || Epoch_TRAIN_Loss:319.6411 ||Epoch_VAL_Loss:164.1555\n",
      "timer:  42.1786 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 201/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13210 || Loss: 5.2862 || 10iter: 6.2772 sec.\n",
      "Iteration 13220 || Loss: 4.4322 || 10iter: 5.9999 sec.\n",
      "Iteration 13230 || Loss: 4.9204 || 10iter: 5.7988 sec.\n",
      "Iteration 13240 || Loss: 8.3165 || 10iter: 4.8901 sec.\n",
      "Iteration 13250 || Loss: 5.6364 || 10iter: 5.0552 sec.\n",
      "Iteration 13260 || Loss: 4.8103 || 10iter: 3.4680 sec.\n",
      "-------------\n",
      "epoch 201 || Epoch_TRAIN_Loss:324.0087 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5056 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
