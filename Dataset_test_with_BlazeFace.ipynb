{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000009\n",
      "000017\n",
      "000021\n",
      "000023\n",
      "000030\n",
      "000032\n",
      "000035\n",
      "000041\n",
      "000048\n",
      "000050\n",
      "000051\n",
      "000066\n",
      "000073\n",
      "000081\n",
      "000083\n",
      "000089\n",
      "000101\n",
      "000104\n",
      "000110\n",
      "000113\n",
      "000125\n",
      "000129\n",
      "000131\n",
      "000133\n",
      "000138\n",
      "000146\n",
      "000150\n",
      "000159\n",
      "000162\n",
      "000163\n",
      "000164\n",
      "000165\n",
      "000169\n",
      "000170\n",
      "000171\n",
      "000173\n",
      "000174\n",
      "000177\n",
      "000190\n",
      "000192\n",
      "000193\n",
      "000194\n",
      "000200\n",
      "000210\n",
      "000218\n",
      "000220\n",
      "000222\n",
      "000229\n",
      "000232\n",
      "000245\n",
      "000251\n",
      "000257\n",
      "000259\n",
      "000269\n",
      "000275\n",
      "000276\n",
      "000278\n",
      "000282\n",
      "000285\n",
      "000288\n",
      "000298\n",
      "000302\n",
      "000305\n",
      "000308\n",
      "000320\n",
      "000321\n",
      "000322\n",
      "000323\n",
      "000328\n",
      "000331\n",
      "000337\n",
      "000338\n",
      "000352\n",
      "000359\n",
      "000367\n",
      "000372\n",
      "000374\n",
      "000382\n",
      "000394\n",
      "000406\n",
      "000407\n",
      "000411\n",
      "000419\n",
      "000428\n",
      "000433\n",
      "000435\n",
      "000438\n",
      "000443\n",
      "000446\n",
      "000448\n",
      "000463\n",
      "000468\n",
      "000470\n",
      "000476\n",
      "000477\n",
      "000480\n",
      "000482\n",
      "000483\n",
      "000498\n",
      "000499\n",
      "000500\n",
      "000515\n",
      "000516\n",
      "000518\n",
      "000520\n",
      "000523\n",
      "000524\n",
      "000525\n",
      "000526\n",
      "000530\n",
      "000531\n",
      "000535\n",
      "000541\n",
      "000545\n",
      "000554\n",
      "000555\n",
      "000579\n",
      "000583\n",
      "000589\n",
      "000591\n",
      "000597\n",
      "000612\n",
      "000613\n",
      "000625\n",
      "000626\n",
      "000628\n",
      "000633\n",
      "000648\n",
      "000654\n",
      "000677\n",
      "000684\n",
      "000688\n",
      "000690\n",
      "000694\n",
      "000695\n",
      "000702\n",
      "000709\n",
      "000717\n",
      "000726\n",
      "000731\n",
      "000733\n",
      "000739\n",
      "000742\n",
      "000750\n",
      "000752\n",
      "000753\n",
      "000755\n",
      "000760\n",
      "000770\n",
      "000777\n",
      "000782\n",
      "000786\n",
      "000793\n",
      "000797\n",
      "000799\n",
      "000802\n",
      "000805\n",
      "000806\n",
      "000810\n",
      "000812\n",
      "000814\n",
      "000816\n",
      "000828\n",
      "000829\n",
      "000834\n",
      "000842\n",
      "000843\n",
      "000847\n",
      "000848\n",
      "000851\n",
      "000854\n",
      "000855\n",
      "000859\n",
      "000860\n",
      "000862\n",
      "000865\n",
      "000874\n",
      "000878\n",
      "000879\n",
      "000885\n",
      "000892\n",
      "000895\n",
      "000898\n",
      "000902\n",
      "000903\n",
      "000904\n",
      "000906\n",
      "000911\n",
      "000915\n",
      "000918\n",
      "000920\n",
      "000926\n",
      "000937\n",
      "000943\n",
      "000948\n",
      "000949\n",
      "000966\n",
      "000967\n",
      "000971\n",
      "000982\n",
      "000987\n",
      "000991\n",
      "000996\n",
      "000999\n",
      "001001\n",
      "001011\n",
      "001014\n",
      "001017\n",
      "001024\n",
      "001028\n",
      "001036\n",
      "001042\n",
      "001050\n",
      "001057\n",
      "001060\n",
      "001061\n",
      "001064\n",
      "001066\n",
      "001071\n",
      "001072\n",
      "001079\n",
      "001084\n",
      "001091\n",
      "001092\n",
      "001097\n",
      "001101\n",
      "001109\n",
      "001113\n",
      "001125\n",
      "001129\n",
      "001137\n",
      "001140\n",
      "001145\n",
      "001147\n",
      "001151\n",
      "001152\n",
      "001164\n",
      "001168\n",
      "001170\n",
      "001171\n",
      "001175\n",
      "001185\n",
      "001186\n",
      "001201\n",
      "001206\n",
      "001212\n",
      "001221\n",
      "001224\n",
      "001229\n",
      "001234\n",
      "001236\n",
      "001240\n",
      "001241\n",
      "001248\n",
      "001254\n",
      "001259\n",
      "001265\n",
      "001266\n",
      "001269\n",
      "001272\n",
      "001279\n",
      "001281\n",
      "001284\n",
      "001287\n",
      "001292\n",
      "001298\n",
      "001304\n",
      "001309\n",
      "001310\n",
      "001311\n",
      "001315\n",
      "001325\n",
      "001327\n",
      "001330\n",
      "001333\n",
      "001337\n",
      "001346\n",
      "001350\n",
      "001352\n",
      "001362\n",
      "001378\n",
      "001388\n",
      "001390\n",
      "001393\n",
      "001405\n",
      "001406\n",
      "001408\n",
      "001409\n",
      "001414\n",
      "001420\n",
      "001421\n",
      "001426\n",
      "001427\n",
      "001430\n",
      "001434\n",
      "001445\n",
      "001450\n",
      "001451\n",
      "001455\n",
      "001460\n",
      "001463\n",
      "001472\n",
      "001475\n",
      "001479\n",
      "001480\n",
      "001484\n",
      "001485\n",
      "001493\n",
      "001498\n",
      "001499\n",
      "001501\n",
      "001504\n",
      "001509\n",
      "001510\n",
      "001514\n",
      "001521\n",
      "001523\n",
      "001524\n",
      "001526\n",
      "001531\n",
      "001532\n",
      "001536\n",
      "001537\n",
      "001544\n",
      "001548\n",
      "001554\n",
      "001557\n",
      "001561\n",
      "001563\n",
      "001571\n",
      "001577\n",
      "001579\n",
      "001580\n",
      "001586\n",
      "001593\n",
      "001594\n",
      "001603\n",
      "001608\n",
      "001611\n",
      "001617\n",
      "001622\n",
      "001627\n",
      "001628\n",
      "001630\n",
      "001640\n",
      "001649\n",
      "001651\n",
      "001654\n",
      "001678\n",
      "001682\n",
      "001686\n",
      "001689\n",
      "001690\n",
      "001691\n",
      "001711\n",
      "001717\n",
      "001721\n",
      "001723\n",
      "001725\n",
      "001726\n",
      "001727\n",
      "001729\n",
      "001730\n",
      "001749\n",
      "001754\n",
      "001756\n",
      "001758\n",
      "001759\n",
      "001766\n",
      "001772\n",
      "001775\n",
      "001784\n",
      "001785\n",
      "001789\n",
      "001793\n",
      "001795\n",
      "001797\n",
      "001799\n",
      "001800\n",
      "001801\n",
      "001806\n",
      "001807\n",
      "001809\n",
      "001810\n",
      "001828\n",
      "001830\n",
      "001832\n",
      "001833\n",
      "001836\n",
      "001837\n",
      "001841\n",
      "001843\n",
      "001849\n",
      "001854\n",
      "001861\n",
      "001864\n",
      "001881\n",
      "001887\n",
      "001894\n",
      "001899\n",
      "001902\n",
      "001903\n",
      "001904\n",
      "001918\n",
      "001920\n",
      "001927\n",
      "001930\n",
      "001932\n",
      "001938\n",
      "001941\n",
      "001944\n",
      "001945\n",
      "001950\n",
      "001958\n",
      "001963\n",
      "001977\n",
      "001978\n",
      "001980\n",
      "001989\n",
      "001999\n",
      "002000\n",
      "002012\n",
      "002022\n",
      "002024\n",
      "002030\n",
      "002051\n",
      "002055\n",
      "002061\n",
      "002069\n",
      "002082\n",
      "002090\n",
      "002091\n",
      "002096\n",
      "002104\n",
      "002108\n",
      "002114\n",
      "002116\n",
      "002117\n",
      "002120\n",
      "002142\n",
      "002145\n",
      "002156\n",
      "002163\n",
      "002169\n",
      "002170\n",
      "002171\n",
      "002172\n",
      "002174\n",
      "002176\n",
      "002180\n",
      "002183\n",
      "002186\n",
      "002187\n",
      "002190\n",
      "002193\n",
      "002194\n",
      "002213\n",
      "002214\n",
      "002219\n",
      "002220\n",
      "002221\n",
      "002224\n",
      "002226\n",
      "002233\n",
      "002238\n",
      "002241\n",
      "002244\n",
      "002256\n",
      "002261\n",
      "002265\n",
      "002272\n",
      "002273\n",
      "002277\n",
      "002281\n",
      "002285\n",
      "002288\n",
      "002290\n",
      "002293\n",
      "002302\n",
      "002306\n",
      "002307\n",
      "002311\n",
      "002324\n",
      "002329\n",
      "002332\n",
      "002337\n",
      "002342\n",
      "002361\n",
      "002362\n",
      "002367\n",
      "002372\n",
      "002378\n",
      "002382\n",
      "002384\n",
      "002385\n",
      "002387\n",
      "002401\n",
      "002403\n",
      "002405\n",
      "002410\n",
      "002413\n",
      "002419\n",
      "002425\n",
      "002433\n",
      "002443\n",
      "002444\n",
      "002448\n",
      "002450\n",
      "002458\n",
      "002459\n",
      "002460\n",
      "002472\n",
      "002476\n",
      "002477\n",
      "002481\n",
      "002483\n",
      "002491\n",
      "002497\n",
      "002501\n",
      "002504\n",
      "002514\n",
      "002518\n",
      "002520\n",
      "002524\n",
      "002529\n",
      "002533\n",
      "002539\n",
      "002540\n",
      "002542\n",
      "002545\n",
      "002554\n",
      "002555\n",
      "002558\n",
      "002561\n",
      "002563\n",
      "002564\n",
      "002565\n",
      "002566\n",
      "002572\n",
      "002584\n",
      "002585\n",
      "002589\n",
      "002595\n",
      "002611\n",
      "002613\n",
      "002625\n",
      "002632\n",
      "002633\n",
      "002636\n",
      "002637\n",
      "002641\n",
      "002643\n",
      "002649\n",
      "002657\n",
      "002658\n",
      "002662\n",
      "002668\n",
      "002669\n",
      "002675\n",
      "002678\n",
      "002689\n",
      "002691\n",
      "002693\n",
      "002696\n",
      "002710\n",
      "002713\n",
      "002714\n",
      "002715\n",
      "002723\n",
      "002727\n",
      "002730\n",
      "002735\n",
      "002741\n",
      "002744\n",
      "002751\n",
      "002755\n",
      "002759\n",
      "002767\n",
      "002776\n",
      "002786\n",
      "002795\n",
      "002798\n",
      "002803\n",
      "002804\n",
      "002807\n",
      "002815\n",
      "002826\n",
      "002827\n",
      "002834\n",
      "002838\n",
      "002841\n",
      "002842\n",
      "002855\n",
      "002858\n",
      "002859\n",
      "002864\n",
      "002867\n",
      "002870\n",
      "002879\n",
      "002881\n",
      "002884\n",
      "002886\n",
      "002889\n",
      "002899\n",
      "002901\n",
      "002912\n",
      "002913\n",
      "002914\n",
      "002915\n",
      "002916\n",
      "002931\n",
      "002932\n",
      "002933\n",
      "002935\n",
      "002939\n",
      "002940\n",
      "002954\n",
      "002958\n",
      "002960\n",
      "002962\n",
      "002965\n",
      "002975\n",
      "002976\n",
      "002987\n",
      "002988\n",
      "002990\n",
      "002992\n",
      "003002\n",
      "003003\n",
      "003017\n",
      "003023\n",
      "003024\n",
      "003027\n",
      "003031\n",
      "003034\n",
      "003039\n",
      "003042\n",
      "003044\n",
      "003057\n",
      "003058\n",
      "003063\n",
      "003064\n",
      "003065\n",
      "003077\n",
      "003078\n",
      "003086\n",
      "003088\n",
      "003089\n",
      "003100\n",
      "003102\n",
      "003106\n",
      "003107\n",
      "003118\n",
      "003121\n",
      "003122\n",
      "003124\n",
      "003129\n",
      "003134\n",
      "003138\n",
      "003140\n",
      "003145\n",
      "003147\n",
      "003150\n",
      "003159\n",
      "003162\n",
      "003170\n",
      "003175\n",
      "003181\n",
      "003185\n",
      "003188\n",
      "003189\n",
      "003205\n",
      "003210\n",
      "003213\n",
      "003218\n",
      "003228\n",
      "003236\n",
      "003247\n",
      "003253\n",
      "003259\n",
      "003260\n",
      "003261\n",
      "003270\n",
      "003280\n",
      "003282\n",
      "003284\n",
      "003290\n",
      "003292\n",
      "003296\n",
      "003301\n",
      "003303\n",
      "003311\n",
      "003313\n",
      "003320\n",
      "003327\n",
      "003331\n",
      "003335\n",
      "003344\n",
      "003350\n",
      "003355\n",
      "003360\n",
      "003362\n",
      "003367\n",
      "003369\n",
      "003373\n",
      "003376\n",
      "003380\n",
      "003382\n",
      "003386\n",
      "003390\n",
      "003391\n",
      "003395\n",
      "003396\n",
      "003397\n",
      "003403\n",
      "003406\n",
      "003407\n",
      "003412\n",
      "003416\n",
      "003417\n",
      "003422\n",
      "003424\n",
      "003429\n",
      "003433\n",
      "003435\n",
      "003436\n",
      "003450\n",
      "003453\n",
      "003455\n",
      "003462\n",
      "003465\n",
      "003466\n",
      "003468\n",
      "003469\n",
      "003470\n",
      "003484\n",
      "003487\n",
      "003492\n",
      "003496\n",
      "003510\n",
      "003518\n",
      "003521\n",
      "003528\n",
      "003529\n",
      "003530\n",
      "003536\n",
      "003539\n",
      "003548\n",
      "003550\n",
      "003556\n",
      "003566\n",
      "003567\n",
      "003575\n",
      "003576\n",
      "003577\n",
      "003580\n",
      "003585\n",
      "003589\n",
      "003603\n",
      "003611\n",
      "003618\n",
      "003621\n",
      "003625\n",
      "003628\n",
      "003638\n",
      "003644\n",
      "003645\n",
      "003648\n",
      "003651\n",
      "003654\n",
      "003655\n",
      "003662\n",
      "003663\n",
      "003664\n",
      "003669\n",
      "003678\n",
      "003685\n",
      "003688\n",
      "003690\n",
      "003691\n",
      "003694\n",
      "003696\n",
      "003699\n",
      "003700\n",
      "003703\n",
      "003706\n",
      "003708\n",
      "003711\n",
      "003717\n",
      "003732\n",
      "003735\n",
      "003740\n",
      "003749\n",
      "003751\n",
      "003759\n",
      "003760\n",
      "003779\n",
      "003780\n",
      "003781\n",
      "003797\n",
      "003811\n",
      "003817\n",
      "003820\n",
      "003824\n",
      "003826\n",
      "003827\n",
      "003838\n",
      "003847\n",
      "003849\n",
      "003855\n",
      "003856\n",
      "003859\n",
      "003860\n",
      "003863\n",
      "003865\n",
      "003868\n",
      "003869\n",
      "003877\n",
      "003879\n",
      "003889\n",
      "003891\n",
      "003899\n",
      "003913\n",
      "003915\n",
      "003923\n",
      "003932\n",
      "003939\n",
      "003945\n",
      "003946\n",
      "003948\n",
      "003953\n",
      "003960\n",
      "003961\n",
      "003963\n",
      "003966\n",
      "003970\n",
      "003973\n",
      "003974\n",
      "003986\n",
      "003992\n",
      "003994\n",
      "003996\n",
      "004008\n",
      "004010\n",
      "004012\n",
      "004014\n",
      "004016\n",
      "004017\n",
      "004019\n",
      "004020\n",
      "004028\n",
      "004031\n",
      "004033\n",
      "004039\n",
      "004046\n",
      "004047\n",
      "004051\n",
      "004058\n",
      "004060\n",
      "004067\n",
      "004073\n",
      "004075\n",
      "004077\n",
      "004085\n",
      "004089\n",
      "004091\n",
      "004092\n",
      "004110\n",
      "004111\n",
      "004117\n",
      "004121\n",
      "004122\n",
      "004133\n",
      "004136\n",
      "004137\n",
      "004138\n",
      "004141\n",
      "004142\n",
      "004145\n",
      "004158\n",
      "004168\n",
      "004169\n",
      "004170\n",
      "004191\n",
      "004195\n",
      "004201\n",
      "004205\n",
      "004209\n",
      "004223\n",
      "004224\n",
      "004229\n",
      "004230\n",
      "004231\n",
      "004232\n",
      "004237\n",
      "004244\n",
      "004246\n",
      "004257\n",
      "004258\n",
      "004259\n",
      "004265\n",
      "004269\n",
      "004270\n",
      "004273\n",
      "004275\n",
      "004279\n",
      "004281\n",
      "004287\n",
      "004291\n",
      "004292\n",
      "004296\n",
      "004298\n",
      "004304\n",
      "004321\n",
      "004322\n",
      "004323\n",
      "004327\n",
      "004329\n",
      "004339\n",
      "004346\n",
      "004356\n",
      "004359\n",
      "004360\n",
      "004361\n",
      "004370\n",
      "004372\n",
      "004380\n",
      "004384\n",
      "004387\n",
      "004389\n",
      "004392\n",
      "004397\n",
      "004404\n",
      "004424\n",
      "004433\n",
      "004437\n",
      "004438\n",
      "004457\n",
      "004459\n",
      "004463\n",
      "004464\n",
      "004466\n",
      "004470\n",
      "004471\n",
      "004479\n",
      "004481\n",
      "004487\n",
      "004494\n",
      "004495\n",
      "004496\n",
      "004499\n",
      "004508\n",
      "004509\n",
      "004512\n",
      "004517\n",
      "004518\n",
      "004520\n",
      "004528\n",
      "004530\n",
      "004535\n",
      "004537\n",
      "004542\n",
      "004553\n",
      "004562\n",
      "004563\n",
      "004571\n",
      "004574\n",
      "004588\n",
      "004595\n",
      "004601\n",
      "004604\n",
      "004605\n",
      "004607\n",
      "004609\n",
      "004611\n",
      "004622\n",
      "004630\n",
      "004631\n",
      "004647\n",
      "004648\n",
      "004653\n",
      "004656\n",
      "004660\n",
      "004662\n",
      "004672\n",
      "004673\n",
      "004676\n",
      "004679\n",
      "004685\n",
      "004686\n",
      "004687\n",
      "004692\n",
      "004693\n",
      "004694\n",
      "004699\n",
      "004706\n",
      "004707\n",
      "004715\n",
      "004718\n",
      "004722\n",
      "004723\n",
      "004727\n",
      "004732\n",
      "004742\n",
      "004750\n",
      "004753\n",
      "004760\n",
      "004761\n",
      "004770\n",
      "004779\n",
      "004782\n",
      "004788\n",
      "004789\n",
      "004790\n",
      "004796\n",
      "004797\n",
      "004801\n",
      "004812\n",
      "004815\n",
      "004826\n",
      "004828\n",
      "004830\n",
      "004831\n",
      "004832\n",
      "004834\n",
      "004840\n",
      "004846\n",
      "004852\n",
      "004857\n",
      "004859\n",
      "004868\n",
      "004872\n",
      "004879\n",
      "004882\n",
      "004886\n",
      "004897\n",
      "004898\n",
      "004902\n",
      "004913\n",
      "004916\n",
      "004928\n",
      "004931\n",
      "004935\n",
      "004939\n",
      "004943\n",
      "004946\n",
      "004950\n",
      "004953\n",
      "004954\n",
      "004968\n",
      "004976\n",
      "004977\n",
      "004985\n",
      "004987\n",
      "004990\n",
      "004994\n",
      "004995\n",
      "005014\n",
      "005024\n",
      "005027\n",
      "005029\n",
      "005036\n",
      "005037\n",
      "005039\n",
      "005042\n",
      "005045\n",
      "005052\n",
      "005054\n",
      "005057\n",
      "005061\n",
      "005063\n",
      "005064\n",
      "005081\n",
      "005086\n",
      "005090\n",
      "005093\n",
      "005097\n",
      "005102\n",
      "005107\n",
      "005111\n",
      "005114\n",
      "005116\n",
      "005122\n",
      "005124\n",
      "005129\n",
      "005134\n",
      "005144\n",
      "005146\n",
      "005156\n",
      "005159\n",
      "005160\n",
      "005161\n",
      "005173\n",
      "005176\n",
      "005189\n",
      "005190\n",
      "005195\n",
      "005208\n",
      "005219\n",
      "005220\n",
      "005222\n",
      "005229\n",
      "005230\n",
      "005231\n",
      "005236\n",
      "005239\n",
      "005242\n",
      "005245\n",
      "005248\n",
      "005257\n",
      "005260\n",
      "005268\n",
      "005269\n",
      "005273\n",
      "005274\n",
      "005278\n",
      "005281\n",
      "005283\n",
      "005297\n",
      "005298\n",
      "005303\n",
      "005304\n",
      "005306\n",
      "005307\n",
      "005310\n",
      "005311\n",
      "005315\n",
      "005326\n",
      "005327\n",
      "005331\n",
      "005336\n",
      "005343\n",
      "005344\n",
      "005346\n",
      "005348\n",
      "005351\n",
      "005368\n",
      "005371\n",
      "005374\n",
      "005383\n",
      "005384\n",
      "005385\n",
      "005396\n",
      "005398\n",
      "005405\n",
      "005410\n",
      "005413\n",
      "005414\n",
      "005417\n",
      "005419\n",
      "005421\n",
      "005424\n",
      "005429\n",
      "005430\n",
      "005439\n",
      "005441\n",
      "005448\n",
      "005450\n",
      "005451\n",
      "005457\n",
      "005461\n",
      "005467\n",
      "005471\n",
      "005475\n",
      "005478\n",
      "005485\n",
      "005487\n",
      "005496\n",
      "005499\n",
      "005507\n",
      "005508\n",
      "005509\n",
      "005511\n",
      "005522\n",
      "005526\n",
      "005527\n",
      "005536\n",
      "005541\n",
      "005542\n",
      "005547\n",
      "005550\n",
      "005552\n",
      "005554\n",
      "005563\n",
      "005573\n",
      "005574\n",
      "005582\n",
      "005583\n",
      "005585\n",
      "005593\n",
      "005599\n",
      "005600\n",
      "005606\n",
      "005608\n",
      "005611\n",
      "005613\n",
      "005615\n",
      "005618\n",
      "005631\n",
      "005636\n",
      "005639\n",
      "005644\n",
      "005645\n",
      "005648\n",
      "005652\n",
      "005653\n",
      "005657\n",
      "005658\n",
      "005660\n",
      "005662\n",
      "005664\n",
      "005669\n",
      "005672\n",
      "005680\n",
      "005682\n",
      "005693\n",
      "005695\n",
      "005700\n",
      "005702\n",
      "005704\n",
      "005714\n",
      "005716\n",
      "005719\n",
      "005728\n",
      "005729\n",
      "005731\n",
      "005732\n",
      "005738\n",
      "005740\n",
      "005741\n",
      "005747\n",
      "005757\n",
      "005761\n",
      "005764\n",
      "005765\n",
      "005768\n",
      "005773\n",
      "005780\n",
      "005784\n",
      "005786\n",
      "005788\n",
      "005790\n",
      "005796\n",
      "005799\n",
      "005811\n",
      "005814\n",
      "005818\n",
      "005824\n",
      "005826\n",
      "005829\n",
      "005836\n",
      "005840\n",
      "005845\n",
      "005850\n",
      "005852\n",
      "005856\n",
      "005859\n",
      "005863\n",
      "005873\n",
      "005881\n",
      "005884\n",
      "005885\n",
      "005889\n",
      "005903\n",
      "005905\n",
      "005906\n",
      "005908\n",
      "005910\n",
      "005919\n",
      "005923\n",
      "005928\n",
      "005930\n",
      "005938\n",
      "005940\n",
      "005948\n",
      "005951\n",
      "005956\n",
      "005961\n",
      "005971\n",
      "005975\n",
      "005979\n",
      "005984\n",
      "005985\n",
      "005988\n",
      "005989\n",
      "005990\n",
      "005991\n",
      "005992\n",
      "006004\n",
      "006005\n",
      "006011\n",
      "006028\n",
      "006029\n",
      "006033\n",
      "006038\n",
      "006041\n",
      "006043\n",
      "006071\n",
      "006073\n",
      "006095\n",
      "006103\n",
      "006104\n",
      "006117\n",
      "006128\n",
      "006130\n",
      "006133\n",
      "006140\n",
      "006148\n",
      "006150\n",
      "006151\n",
      "006156\n",
      "006158\n",
      "006161\n",
      "006163\n",
      "006171\n",
      "006172\n",
      "006175\n",
      "006176\n",
      "006177\n",
      "006179\n",
      "006180\n",
      "006185\n",
      "006187\n",
      "006198\n",
      "006209\n",
      "006210\n",
      "006221\n",
      "006225\n",
      "006229\n",
      "006233\n",
      "006235\n",
      "006236\n",
      "006238\n",
      "006240\n",
      "006243\n",
      "006259\n",
      "006272\n",
      "006276\n",
      "006282\n",
      "006284\n",
      "006285\n",
      "006286\n",
      "006289\n",
      "006296\n",
      "006300\n",
      "006320\n",
      "006339\n",
      "006341\n",
      "006344\n",
      "006346\n",
      "006351\n",
      "006352\n",
      "006355\n",
      "006369\n",
      "006381\n",
      "006387\n",
      "006391\n",
      "006392\n",
      "006395\n",
      "006398\n",
      "006409\n",
      "006427\n",
      "006428\n",
      "006433\n",
      "006440\n",
      "006442\n",
      "006444\n",
      "006445\n",
      "006450\n",
      "006455\n",
      "006459\n",
      "006463\n",
      "006465\n",
      "006475\n",
      "006476\n",
      "006482\n",
      "006483\n",
      "006486\n",
      "006488\n",
      "006492\n",
      "006497\n",
      "006503\n",
      "006506\n",
      "006519\n",
      "006529\n",
      "006532\n",
      "006536\n",
      "006542\n",
      "006550\n",
      "006551\n",
      "006560\n",
      "006572\n",
      "006576\n",
      "006578\n",
      "006583\n",
      "006584\n",
      "006585\n",
      "006587\n",
      "006588\n",
      "006599\n",
      "006602\n",
      "006609\n",
      "006610\n",
      "006611\n",
      "006612\n",
      "006617\n",
      "006618\n",
      "006621\n",
      "006625\n",
      "006626\n",
      "006631\n",
      "006637\n",
      "006645\n",
      "006647\n",
      "006648\n",
      "006658\n",
      "006661\n",
      "006664\n",
      "006666\n",
      "006667\n",
      "006668\n",
      "006670\n",
      "006677\n",
      "006682\n",
      "006690\n",
      "006695\n",
      "006696\n",
      "006697\n",
      "006698\n",
      "006699\n",
      "006703\n",
      "006707\n",
      "006709\n",
      "006714\n",
      "006718\n",
      "006722\n",
      "006727\n",
      "006731\n",
      "006735\n",
      "006738\n",
      "006739\n",
      "006748\n",
      "006753\n",
      "006762\n",
      "006766\n",
      "006768\n",
      "006772\n",
      "006777\n",
      "006782\n",
      "006783\n",
      "006784\n",
      "006786\n",
      "006789\n",
      "006797\n",
      "006800\n",
      "006802\n",
      "006803\n",
      "006808\n",
      "006810\n",
      "006821\n",
      "006824\n",
      "006833\n",
      "006835\n",
      "006836\n",
      "006838\n",
      "006840\n",
      "006841\n",
      "006847\n",
      "006850\n",
      "006855\n",
      "006858\n",
      "006862\n",
      "006864\n",
      "006866\n",
      "006868\n",
      "006874\n",
      "006876\n",
      "006880\n",
      "006887\n",
      "006896\n",
      "006903\n",
      "006908\n",
      "006910\n",
      "006911\n",
      "006912\n",
      "006919\n",
      "006924\n",
      "006932\n",
      "006934\n",
      "006939\n",
      "006943\n",
      "006945\n",
      "006948\n",
      "006950\n",
      "006956\n",
      "006958\n",
      "006959\n",
      "006963\n",
      "006965\n",
      "006968\n",
      "006981\n",
      "006983\n",
      "006987\n",
      "006988\n",
      "006989\n",
      "006990\n",
      "007004\n",
      "007007\n",
      "007016\n",
      "007020\n",
      "007023\n",
      "007031\n",
      "007035\n",
      "007036\n",
      "007040\n",
      "007045\n",
      "007046\n",
      "007048\n",
      "007049\n",
      "007052\n",
      "007054\n",
      "007059\n",
      "007073\n",
      "007079\n",
      "007080\n",
      "007093\n",
      "007108\n",
      "007109\n",
      "007114\n",
      "007117\n",
      "007121\n",
      "007122\n",
      "007128\n",
      "007129\n",
      "007138\n",
      "007139\n",
      "007140\n",
      "007147\n",
      "007148\n",
      "007150\n",
      "007153\n",
      "007154\n",
      "007159\n",
      "007163\n",
      "007167\n",
      "007172\n",
      "007182\n",
      "007184\n",
      "007187\n",
      "007189\n",
      "007200\n",
      "007208\n",
      "007213\n",
      "007214\n",
      "007216\n",
      "007227\n",
      "007234\n",
      "007236\n",
      "007243\n",
      "007244\n",
      "007256\n",
      "007260\n",
      "007261\n",
      "007266\n",
      "007270\n",
      "007271\n",
      "007276\n",
      "007284\n",
      "007292\n",
      "007296\n",
      "007298\n",
      "007305\n",
      "007308\n",
      "007314\n",
      "007318\n",
      "007325\n",
      "007330\n",
      "007336\n",
      "007346\n",
      "007350\n",
      "007359\n",
      "007369\n",
      "007375\n",
      "007381\n",
      "007388\n",
      "007398\n",
      "007408\n",
      "007410\n",
      "007413\n",
      "007414\n",
      "007436\n",
      "007438\n",
      "007445\n",
      "007446\n",
      "007448\n",
      "007449\n",
      "007457\n",
      "007466\n",
      "007475\n",
      "007480\n",
      "007482\n",
      "007483\n",
      "007484\n",
      "007493\n",
      "007497\n",
      "007498\n",
      "007511\n",
      "007526\n",
      "007535\n",
      "007540\n",
      "007543\n",
      "007547\n",
      "007558\n",
      "007559\n",
      "007563\n",
      "007566\n",
      "007568\n",
      "007570\n",
      "007571\n",
      "007572\n",
      "007576\n",
      "007578\n",
      "007579\n",
      "007586\n",
      "007590\n",
      "007592\n",
      "007594\n",
      "007605\n",
      "007606\n",
      "007611\n",
      "007618\n",
      "007619\n",
      "007629\n",
      "007631\n",
      "007639\n",
      "007647\n",
      "007649\n",
      "007653\n",
      "007656\n",
      "007657\n",
      "007668\n",
      "007675\n",
      "007683\n",
      "007687\n",
      "007694\n",
      "007697\n",
      "007699\n",
      "007709\n",
      "007712\n",
      "007718\n",
      "007723\n",
      "007727\n",
      "007735\n",
      "007745\n",
      "007746\n",
      "007748\n",
      "007751\n",
      "007760\n",
      "007763\n",
      "007768\n",
      "007777\n",
      "007786\n",
      "007793\n",
      "007795\n",
      "007799\n",
      "007809\n",
      "007810\n",
      "007813\n",
      "007814\n",
      "007815\n",
      "007821\n",
      "007836\n",
      "007841\n",
      "007856\n",
      "007872\n",
      "007877\n",
      "007878\n",
      "007884\n",
      "007889\n",
      "007897\n",
      "007901\n",
      "007902\n",
      "007910\n",
      "007911\n",
      "007915\n",
      "007919\n",
      "007928\n",
      "007931\n",
      "007932\n",
      "007940\n",
      "007943\n",
      "007946\n",
      "007953\n",
      "007963\n",
      "007964\n",
      "007970\n",
      "007971\n",
      "007979\n",
      "007984\n",
      "007987\n",
      "007996\n",
      "007997\n",
      "007998\n",
      "008002\n",
      "008008\n",
      "008019\n",
      "008023\n",
      "008024\n",
      "008026\n",
      "008033\n",
      "008040\n",
      "008042\n",
      "008043\n",
      "008048\n",
      "008053\n",
      "008069\n",
      "008072\n",
      "008079\n",
      "008082\n",
      "008083\n",
      "008086\n",
      "008091\n",
      "008096\n",
      "008103\n",
      "008105\n",
      "008108\n",
      "008122\n",
      "008125\n",
      "008127\n",
      "008130\n",
      "008132\n",
      "008137\n",
      "008139\n",
      "008142\n",
      "008160\n",
      "008163\n",
      "008164\n",
      "008171\n",
      "008175\n",
      "008177\n",
      "008180\n",
      "008188\n",
      "008190\n",
      "008199\n",
      "008204\n",
      "008209\n",
      "008211\n",
      "008216\n",
      "008218\n",
      "008220\n",
      "008224\n",
      "008225\n",
      "008232\n",
      "008241\n",
      "008248\n",
      "008253\n",
      "008261\n",
      "008263\n",
      "008272\n",
      "008279\n",
      "008281\n",
      "008284\n",
      "008285\n",
      "008293\n",
      "008294\n",
      "008301\n",
      "008302\n",
      "008307\n",
      "008312\n",
      "008313\n",
      "008315\n",
      "008327\n",
      "008332\n",
      "008341\n",
      "008360\n",
      "008368\n",
      "008374\n",
      "008376\n",
      "008381\n",
      "008385\n",
      "008387\n",
      "008390\n",
      "008391\n",
      "008397\n",
      "008398\n",
      "008410\n",
      "008413\n",
      "008430\n",
      "008433\n",
      "008437\n",
      "008444\n",
      "008445\n",
      "008449\n",
      "008452\n",
      "008453\n",
      "008461\n",
      "008467\n",
      "008470\n",
      "008475\n",
      "008482\n",
      "008485\n",
      "008503\n",
      "008509\n",
      "008513\n",
      "008514\n",
      "008526\n",
      "008529\n",
      "008541\n",
      "008550\n",
      "008559\n",
      "008562\n",
      "008573\n",
      "008576\n",
      "008582\n",
      "008585\n",
      "008587\n",
      "008592\n",
      "008596\n",
      "008602\n",
      "008604\n",
      "008612\n",
      "008617\n",
      "008618\n",
      "008628\n",
      "008638\n",
      "008644\n",
      "008647\n",
      "008663\n",
      "008665\n",
      "008676\n",
      "008690\n",
      "008692\n",
      "008698\n",
      "008699\n",
      "008701\n",
      "008702\n",
      "008706\n",
      "008718\n",
      "008728\n",
      "008731\n",
      "008738\n",
      "008739\n",
      "008742\n",
      "008744\n",
      "008748\n",
      "008752\n",
      "008766\n",
      "008768\n",
      "008769\n",
      "008771\n",
      "008775\n",
      "008794\n",
      "008801\n",
      "008805\n",
      "008810\n",
      "008813\n",
      "008817\n",
      "008833\n",
      "008836\n",
      "008838\n",
      "008862\n",
      "008865\n",
      "008867\n",
      "008872\n",
      "008876\n",
      "008879\n",
      "008884\n",
      "008886\n",
      "008891\n",
      "008892\n",
      "008900\n",
      "008911\n",
      "008920\n",
      "008921\n",
      "008931\n",
      "008932\n",
      "008943\n",
      "008944\n",
      "008953\n",
      "008960\n",
      "008961\n",
      "008965\n",
      "008975\n",
      "008979\n",
      "008983\n",
      "009000\n",
      "009005\n",
      "009020\n",
      "009022\n",
      "009024\n",
      "009034\n",
      "009036\n",
      "009045\n",
      "009049\n",
      "009064\n",
      "009068\n",
      "009073\n",
      "009078\n",
      "009079\n",
      "009087\n",
      "009089\n",
      "009091\n",
      "009114\n",
      "009123\n",
      "009128\n",
      "009136\n",
      "009141\n",
      "009153\n",
      "009160\n",
      "009161\n",
      "009174\n",
      "009180\n",
      "009184\n",
      "009193\n",
      "009197\n",
      "009202\n",
      "009208\n",
      "009209\n",
      "009214\n",
      "009224\n",
      "009230\n",
      "009236\n",
      "009238\n",
      "009239\n",
      "009242\n",
      "009251\n",
      "009259\n",
      "009273\n",
      "009283\n",
      "009286\n",
      "009287\n",
      "009288\n",
      "009290\n",
      "009291\n",
      "009295\n",
      "009296\n",
      "009299\n",
      "009303\n",
      "009306\n",
      "009308\n",
      "009312\n",
      "009316\n",
      "009318\n",
      "009327\n",
      "009330\n",
      "009331\n",
      "009336\n",
      "009342\n",
      "009343\n",
      "009349\n",
      "009358\n",
      "009362\n",
      "009368\n",
      "009374\n",
      "009375\n",
      "009377\n",
      "009382\n",
      "009393\n",
      "009394\n",
      "009407\n",
      "009410\n",
      "009411\n",
      "009412\n",
      "009413\n",
      "009414\n",
      "009418\n",
      "009420\n",
      "009421\n",
      "009424\n",
      "009433\n",
      "009434\n",
      "009437\n",
      "009440\n",
      "009448\n",
      "009456\n",
      "009460\n",
      "009468\n",
      "009469\n",
      "009477\n",
      "009479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009481\n",
      "009490\n",
      "009494\n",
      "009496\n",
      "009497\n",
      "009499\n",
      "009500\n",
      "009504\n",
      "009512\n",
      "009517\n",
      "009518\n",
      "009520\n",
      "009524\n",
      "009526\n",
      "009531\n",
      "009532\n",
      "009537\n",
      "009541\n",
      "009542\n",
      "009546\n",
      "009551\n",
      "009557\n",
      "009558\n",
      "009565\n",
      "009567\n",
      "009568\n",
      "009573\n",
      "009577\n",
      "009579\n",
      "009585\n",
      "009591\n",
      "009596\n",
      "009600\n",
      "009603\n",
      "009609\n",
      "009611\n",
      "009613\n",
      "009614\n",
      "009617\n",
      "009618\n",
      "009638\n",
      "009641\n",
      "009647\n",
      "009649\n",
      "009654\n",
      "009655\n",
      "009656\n",
      "009659\n",
      "009666\n",
      "009668\n",
      "009671\n",
      "009676\n",
      "009684\n",
      "009687\n",
      "009691\n",
      "009693\n",
      "009698\n",
      "009703\n",
      "009711\n",
      "009712\n",
      "009713\n",
      "009717\n",
      "009718\n",
      "009721\n",
      "009726\n",
      "009732\n",
      "009734\n",
      "009738\n",
      "009747\n",
      "009754\n",
      "009755\n",
      "009756\n",
      "009762\n",
      "009767\n",
      "009773\n",
      "009776\n",
      "009780\n",
      "009781\n",
      "009789\n",
      "009792\n",
      "009796\n",
      "009800\n",
      "009809\n",
      "009813\n",
      "009822\n",
      "009828\n",
      "009841\n",
      "009845\n",
      "009848\n",
      "009851\n",
      "009859\n",
      "009867\n",
      "009868\n",
      "009869\n",
      "009874\n",
      "009877\n",
      "009878\n",
      "009879\n",
      "009880\n",
      "009881\n",
      "009882\n",
      "009900\n",
      "009902\n",
      "009917\n",
      "009918\n",
      "009926\n",
      "009935\n",
      "009942\n",
      "009944\n",
      "009946\n",
      "009947\n",
      "009949\n",
      "009950\n",
      "009954\n",
      "009958\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 128  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n",
      "32\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3773, 0.2824, 0.7485, 1.0000, 0.0000],\n",
       "        [0.8037, 0.3389, 0.9356, 0.5000, 0.0000],\n",
       "        [0.8896, 0.3368, 0.9540, 0.4979, 0.0000],\n",
       "        [0.3650, 0.3410, 0.4479, 0.4874, 0.0000],\n",
       "        [0.3006, 0.3410, 0.3405, 0.3954, 0.0000],\n",
       "        [0.0767, 0.3410, 0.1166, 0.4079, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazeFace(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (4): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (5): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (7): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (8): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (9): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BlazeFaceExtra(\n",
      "  (features): Sequential(\n",
      "    (0): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.blazeface import SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 128,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [4, 8],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 128],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=2, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-3\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface128_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 18.8874 || 10iter: 5.3144 sec.\n",
      "Iteration 20 || Loss: 21.3995 || 10iter: 2.2963 sec.\n",
      "Iteration 30 || Loss: 16.8589 || 10iter: 2.3712 sec.\n",
      "Iteration 40 || Loss: 19.2261 || 10iter: 2.3401 sec.\n",
      "Iteration 50 || Loss: 13.8212 || 10iter: 2.3640 sec.\n",
      "Iteration 60 || Loss: 11.6308 || 10iter: 1.5229 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:1270.1455 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4049 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 70 || Loss: 17.1358 || 10iter: 2.9972 sec.\n",
      "Iteration 80 || Loss: 12.7206 || 10iter: 3.0023 sec.\n",
      "Iteration 90 || Loss: 15.6719 || 10iter: 2.3185 sec.\n",
      "Iteration 100 || Loss: 12.8525 || 10iter: 2.2683 sec.\n",
      "Iteration 110 || Loss: 11.6462 || 10iter: 2.2905 sec.\n",
      "Iteration 120 || Loss: 9.6436 || 10iter: 2.1855 sec.\n",
      "Iteration 130 || Loss: 10.9450 || 10iter: 1.4105 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:853.2513 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9428 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 140 || Loss: 10.8893 || 10iter: 4.5854 sec.\n",
      "Iteration 150 || Loss: 9.9130 || 10iter: 2.5745 sec.\n",
      "Iteration 160 || Loss: 10.8102 || 10iter: 2.2485 sec.\n",
      "Iteration 170 || Loss: 12.8943 || 10iter: 2.2360 sec.\n",
      "Iteration 180 || Loss: 10.2262 || 10iter: 2.2329 sec.\n",
      "Iteration 190 || Loss: 9.5626 || 10iter: 1.5255 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:740.8950 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5377 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 200 || Loss: 9.8857 || 10iter: 2.1899 sec.\n",
      "Iteration 210 || Loss: 9.3505 || 10iter: 3.2584 sec.\n",
      "Iteration 220 || Loss: 8.6282 || 10iter: 2.4845 sec.\n",
      "Iteration 230 || Loss: 9.9044 || 10iter: 2.1962 sec.\n",
      "Iteration 240 || Loss: 10.1706 || 10iter: 2.3791 sec.\n",
      "Iteration 250 || Loss: 8.7279 || 10iter: 1.9958 sec.\n",
      "Iteration 260 || Loss: 9.7935 || 10iter: 1.3563 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:603.3565 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4543 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 270 || Loss: 7.8867 || 10iter: 3.9036 sec.\n",
      "Iteration 280 || Loss: 9.3923 || 10iter: 2.5500 sec.\n",
      "Iteration 290 || Loss: 9.2539 || 10iter: 2.5677 sec.\n",
      "Iteration 300 || Loss: 11.8253 || 10iter: 2.2810 sec.\n",
      "Iteration 310 || Loss: 8.8687 || 10iter: 2.2331 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b796d5f98bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1f39654e9bb6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# 損失の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BlazeFace_Person_pytorch/utils/ssd_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;31m# conf_t_label：各DBoxに一番近い正解のBBoxのラベルを格納させる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# loc_t:各DBoxに一番近い正解のBBoxの位置情報を格納させる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0mconf_t_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mloc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
