{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000009\n",
      "000017\n",
      "000021\n",
      "000023\n",
      "000030\n",
      "000032\n",
      "000035\n",
      "000041\n",
      "000048\n",
      "000050\n",
      "000051\n",
      "000066\n",
      "000073\n",
      "000081\n",
      "000083\n",
      "000089\n",
      "000101\n",
      "000104\n",
      "000110\n",
      "000113\n",
      "000125\n",
      "000129\n",
      "000131\n",
      "000133\n",
      "000138\n",
      "000146\n",
      "000150\n",
      "000159\n",
      "000162\n",
      "000163\n",
      "000164\n",
      "000165\n",
      "000169\n",
      "000170\n",
      "000171\n",
      "000173\n",
      "000174\n",
      "000177\n",
      "000190\n",
      "000192\n",
      "000193\n",
      "000194\n",
      "000200\n",
      "000210\n",
      "000218\n",
      "000220\n",
      "000222\n",
      "000229\n",
      "000232\n",
      "000245\n",
      "000251\n",
      "000257\n",
      "000259\n",
      "000269\n",
      "000275\n",
      "000276\n",
      "000278\n",
      "000282\n",
      "000285\n",
      "000288\n",
      "000298\n",
      "000302\n",
      "000305\n",
      "000308\n",
      "000320\n",
      "000321\n",
      "000322\n",
      "000323\n",
      "000328\n",
      "000331\n",
      "000337\n",
      "000338\n",
      "000352\n",
      "000359\n",
      "000367\n",
      "000372\n",
      "000374\n",
      "000382\n",
      "000394\n",
      "000406\n",
      "000407\n",
      "000411\n",
      "000419\n",
      "000428\n",
      "000433\n",
      "000435\n",
      "000438\n",
      "000443\n",
      "000446\n",
      "000448\n",
      "000463\n",
      "000468\n",
      "000470\n",
      "000476\n",
      "000477\n",
      "000480\n",
      "000482\n",
      "000483\n",
      "000498\n",
      "000499\n",
      "000500\n",
      "000515\n",
      "000516\n",
      "000518\n",
      "000520\n",
      "000523\n",
      "000524\n",
      "000525\n",
      "000526\n",
      "000530\n",
      "000531\n",
      "000535\n",
      "000541\n",
      "000545\n",
      "000554\n",
      "000555\n",
      "000579\n",
      "000583\n",
      "000589\n",
      "000591\n",
      "000597\n",
      "000612\n",
      "000613\n",
      "000625\n",
      "000626\n",
      "000628\n",
      "000633\n",
      "000648\n",
      "000654\n",
      "000677\n",
      "000684\n",
      "000688\n",
      "000690\n",
      "000694\n",
      "000695\n",
      "000702\n",
      "000709\n",
      "000717\n",
      "000726\n",
      "000731\n",
      "000733\n",
      "000739\n",
      "000742\n",
      "000750\n",
      "000752\n",
      "000753\n",
      "000755\n",
      "000760\n",
      "000770\n",
      "000777\n",
      "000782\n",
      "000786\n",
      "000793\n",
      "000797\n",
      "000799\n",
      "000802\n",
      "000805\n",
      "000806\n",
      "000810\n",
      "000812\n",
      "000814\n",
      "000816\n",
      "000828\n",
      "000829\n",
      "000834\n",
      "000842\n",
      "000843\n",
      "000847\n",
      "000848\n",
      "000851\n",
      "000854\n",
      "000855\n",
      "000859\n",
      "000860\n",
      "000862\n",
      "000865\n",
      "000874\n",
      "000878\n",
      "000879\n",
      "000885\n",
      "000892\n",
      "000895\n",
      "000898\n",
      "000902\n",
      "000903\n",
      "000904\n",
      "000906\n",
      "000911\n",
      "000915\n",
      "000918\n",
      "000920\n",
      "000926\n",
      "000937\n",
      "000943\n",
      "000948\n",
      "000949\n",
      "000966\n",
      "000967\n",
      "000971\n",
      "000982\n",
      "000987\n",
      "000991\n",
      "000996\n",
      "000999\n",
      "001001\n",
      "001011\n",
      "001014\n",
      "001017\n",
      "001024\n",
      "001028\n",
      "001036\n",
      "001042\n",
      "001050\n",
      "001057\n",
      "001060\n",
      "001061\n",
      "001064\n",
      "001066\n",
      "001071\n",
      "001072\n",
      "001079\n",
      "001084\n",
      "001091\n",
      "001092\n",
      "001097\n",
      "001101\n",
      "001109\n",
      "001113\n",
      "001125\n",
      "001129\n",
      "001137\n",
      "001140\n",
      "001145\n",
      "001147\n",
      "001151\n",
      "001152\n",
      "001164\n",
      "001168\n",
      "001170\n",
      "001171\n",
      "001175\n",
      "001185\n",
      "001186\n",
      "001201\n",
      "001206\n",
      "001212\n",
      "001221\n",
      "001224\n",
      "001229\n",
      "001234\n",
      "001236\n",
      "001240\n",
      "001241\n",
      "001248\n",
      "001254\n",
      "001259\n",
      "001265\n",
      "001266\n",
      "001269\n",
      "001272\n",
      "001279\n",
      "001281\n",
      "001284\n",
      "001287\n",
      "001292\n",
      "001298\n",
      "001304\n",
      "001309\n",
      "001310\n",
      "001311\n",
      "001315\n",
      "001325\n",
      "001327\n",
      "001330\n",
      "001333\n",
      "001337\n",
      "001346\n",
      "001350\n",
      "001352\n",
      "001362\n",
      "001378\n",
      "001388\n",
      "001390\n",
      "001393\n",
      "001405\n",
      "001406\n",
      "001408\n",
      "001409\n",
      "001414\n",
      "001420\n",
      "001421\n",
      "001426\n",
      "001427\n",
      "001430\n",
      "001434\n",
      "001445\n",
      "001450\n",
      "001451\n",
      "001455\n",
      "001460\n",
      "001463\n",
      "001472\n",
      "001475\n",
      "001479\n",
      "001480\n",
      "001484\n",
      "001485\n",
      "001493\n",
      "001498\n",
      "001499\n",
      "001501\n",
      "001504\n",
      "001509\n",
      "001510\n",
      "001514\n",
      "001521\n",
      "001523\n",
      "001524\n",
      "001526\n",
      "001531\n",
      "001532\n",
      "001536\n",
      "001537\n",
      "001544\n",
      "001548\n",
      "001554\n",
      "001557\n",
      "001561\n",
      "001563\n",
      "001571\n",
      "001577\n",
      "001579\n",
      "001580\n",
      "001586\n",
      "001593\n",
      "001594\n",
      "001603\n",
      "001608\n",
      "001611\n",
      "001617\n",
      "001622\n",
      "001627\n",
      "001628\n",
      "001630\n",
      "001640\n",
      "001649\n",
      "001651\n",
      "001654\n",
      "001678\n",
      "001682\n",
      "001686\n",
      "001689\n",
      "001690\n",
      "001691\n",
      "001711\n",
      "001717\n",
      "001721\n",
      "001723\n",
      "001725\n",
      "001726\n",
      "001727\n",
      "001729\n",
      "001730\n",
      "001749\n",
      "001754\n",
      "001756\n",
      "001758\n",
      "001759\n",
      "001766\n",
      "001772\n",
      "001775\n",
      "001784\n",
      "001785\n",
      "001789\n",
      "001793\n",
      "001795\n",
      "001797\n",
      "001799\n",
      "001800\n",
      "001801\n",
      "001806\n",
      "001807\n",
      "001809\n",
      "001810\n",
      "001828\n",
      "001830\n",
      "001832\n",
      "001833\n",
      "001836\n",
      "001837\n",
      "001841\n",
      "001843\n",
      "001849\n",
      "001854\n",
      "001861\n",
      "001864\n",
      "001881\n",
      "001887\n",
      "001894\n",
      "001899\n",
      "001902\n",
      "001903\n",
      "001904\n",
      "001918\n",
      "001920\n",
      "001927\n",
      "001930\n",
      "001932\n",
      "001938\n",
      "001941\n",
      "001944\n",
      "001945\n",
      "001950\n",
      "001958\n",
      "001963\n",
      "001977\n",
      "001978\n",
      "001980\n",
      "001989\n",
      "001999\n",
      "002000\n",
      "002012\n",
      "002022\n",
      "002024\n",
      "002030\n",
      "002051\n",
      "002055\n",
      "002061\n",
      "002069\n",
      "002082\n",
      "002090\n",
      "002091\n",
      "002096\n",
      "002104\n",
      "002108\n",
      "002114\n",
      "002116\n",
      "002117\n",
      "002120\n",
      "002142\n",
      "002145\n",
      "002156\n",
      "002163\n",
      "002169\n",
      "002170\n",
      "002171\n",
      "002172\n",
      "002174\n",
      "002176\n",
      "002180\n",
      "002183\n",
      "002186\n",
      "002187\n",
      "002190\n",
      "002193\n",
      "002194\n",
      "002213\n",
      "002214\n",
      "002219\n",
      "002220\n",
      "002221\n",
      "002224\n",
      "002226\n",
      "002233\n",
      "002238\n",
      "002241\n",
      "002244\n",
      "002256\n",
      "002261\n",
      "002265\n",
      "002272\n",
      "002273\n",
      "002277\n",
      "002281\n",
      "002285\n",
      "002288\n",
      "002290\n",
      "002293\n",
      "002302\n",
      "002306\n",
      "002307\n",
      "002311\n",
      "002324\n",
      "002329\n",
      "002332\n",
      "002337\n",
      "002342\n",
      "002361\n",
      "002362\n",
      "002367\n",
      "002372\n",
      "002378\n",
      "002382\n",
      "002384\n",
      "002385\n",
      "002387\n",
      "002401\n",
      "002403\n",
      "002405\n",
      "002410\n",
      "002413\n",
      "002419\n",
      "002425\n",
      "002433\n",
      "002443\n",
      "002444\n",
      "002448\n",
      "002450\n",
      "002458\n",
      "002459\n",
      "002460\n",
      "002472\n",
      "002476\n",
      "002477\n",
      "002481\n",
      "002483\n",
      "002491\n",
      "002497\n",
      "002501\n",
      "002504\n",
      "002514\n",
      "002518\n",
      "002520\n",
      "002524\n",
      "002529\n",
      "002533\n",
      "002539\n",
      "002540\n",
      "002542\n",
      "002545\n",
      "002554\n",
      "002555\n",
      "002558\n",
      "002561\n",
      "002563\n",
      "002564\n",
      "002565\n",
      "002566\n",
      "002572\n",
      "002584\n",
      "002585\n",
      "002589\n",
      "002595\n",
      "002611\n",
      "002613\n",
      "002625\n",
      "002632\n",
      "002633\n",
      "002636\n",
      "002637\n",
      "002641\n",
      "002643\n",
      "002649\n",
      "002657\n",
      "002658\n",
      "002662\n",
      "002668\n",
      "002669\n",
      "002675\n",
      "002678\n",
      "002689\n",
      "002691\n",
      "002693\n",
      "002696\n",
      "002710\n",
      "002713\n",
      "002714\n",
      "002715\n",
      "002723\n",
      "002727\n",
      "002730\n",
      "002735\n",
      "002741\n",
      "002744\n",
      "002751\n",
      "002755\n",
      "002759\n",
      "002767\n",
      "002776\n",
      "002786\n",
      "002795\n",
      "002798\n",
      "002803\n",
      "002804\n",
      "002807\n",
      "002815\n",
      "002826\n",
      "002827\n",
      "002834\n",
      "002838\n",
      "002841\n",
      "002842\n",
      "002855\n",
      "002858\n",
      "002859\n",
      "002864\n",
      "002867\n",
      "002870\n",
      "002879\n",
      "002881\n",
      "002884\n",
      "002886\n",
      "002889\n",
      "002899\n",
      "002901\n",
      "002912\n",
      "002913\n",
      "002914\n",
      "002915\n",
      "002916\n",
      "002931\n",
      "002932\n",
      "002933\n",
      "002935\n",
      "002939\n",
      "002940\n",
      "002954\n",
      "002958\n",
      "002960\n",
      "002962\n",
      "002965\n",
      "002975\n",
      "002976\n",
      "002987\n",
      "002988\n",
      "002990\n",
      "002992\n",
      "003002\n",
      "003003\n",
      "003017\n",
      "003023\n",
      "003024\n",
      "003027\n",
      "003031\n",
      "003034\n",
      "003039\n",
      "003042\n",
      "003044\n",
      "003057\n",
      "003058\n",
      "003063\n",
      "003064\n",
      "003065\n",
      "003077\n",
      "003078\n",
      "003086\n",
      "003088\n",
      "003089\n",
      "003100\n",
      "003102\n",
      "003106\n",
      "003107\n",
      "003118\n",
      "003121\n",
      "003122\n",
      "003124\n",
      "003129\n",
      "003134\n",
      "003138\n",
      "003140\n",
      "003145\n",
      "003147\n",
      "003150\n",
      "003159\n",
      "003162\n",
      "003170\n",
      "003175\n",
      "003181\n",
      "003185\n",
      "003188\n",
      "003189\n",
      "003205\n",
      "003210\n",
      "003213\n",
      "003218\n",
      "003228\n",
      "003236\n",
      "003247\n",
      "003253\n",
      "003259\n",
      "003260\n",
      "003261\n",
      "003270\n",
      "003280\n",
      "003282\n",
      "003284\n",
      "003290\n",
      "003292\n",
      "003296\n",
      "003301\n",
      "003303\n",
      "003311\n",
      "003313\n",
      "003320\n",
      "003327\n",
      "003331\n",
      "003335\n",
      "003344\n",
      "003350\n",
      "003355\n",
      "003360\n",
      "003362\n",
      "003367\n",
      "003369\n",
      "003373\n",
      "003376\n",
      "003380\n",
      "003382\n",
      "003386\n",
      "003390\n",
      "003391\n",
      "003395\n",
      "003396\n",
      "003397\n",
      "003403\n",
      "003406\n",
      "003407\n",
      "003412\n",
      "003416\n",
      "003417\n",
      "003422\n",
      "003424\n",
      "003429\n",
      "003433\n",
      "003435\n",
      "003436\n",
      "003450\n",
      "003453\n",
      "003455\n",
      "003462\n",
      "003465\n",
      "003466\n",
      "003468\n",
      "003469\n",
      "003470\n",
      "003484\n",
      "003487\n",
      "003492\n",
      "003496\n",
      "003510\n",
      "003518\n",
      "003521\n",
      "003528\n",
      "003529\n",
      "003530\n",
      "003536\n",
      "003539\n",
      "003548\n",
      "003550\n",
      "003556\n",
      "003566\n",
      "003567\n",
      "003575\n",
      "003576\n",
      "003577\n",
      "003580\n",
      "003585\n",
      "003589\n",
      "003603\n",
      "003611\n",
      "003618\n",
      "003621\n",
      "003625\n",
      "003628\n",
      "003638\n",
      "003644\n",
      "003645\n",
      "003648\n",
      "003651\n",
      "003654\n",
      "003655\n",
      "003662\n",
      "003663\n",
      "003664\n",
      "003669\n",
      "003678\n",
      "003685\n",
      "003688\n",
      "003690\n",
      "003691\n",
      "003694\n",
      "003696\n",
      "003699\n",
      "003700\n",
      "003703\n",
      "003706\n",
      "003708\n",
      "003711\n",
      "003717\n",
      "003732\n",
      "003735\n",
      "003740\n",
      "003749\n",
      "003751\n",
      "003759\n",
      "003760\n",
      "003779\n",
      "003780\n",
      "003781\n",
      "003797\n",
      "003811\n",
      "003817\n",
      "003820\n",
      "003824\n",
      "003826\n",
      "003827\n",
      "003838\n",
      "003847\n",
      "003849\n",
      "003855\n",
      "003856\n",
      "003859\n",
      "003860\n",
      "003863\n",
      "003865\n",
      "003868\n",
      "003869\n",
      "003877\n",
      "003879\n",
      "003889\n",
      "003891\n",
      "003899\n",
      "003913\n",
      "003915\n",
      "003923\n",
      "003932\n",
      "003939\n",
      "003945\n",
      "003946\n",
      "003948\n",
      "003953\n",
      "003960\n",
      "003961\n",
      "003963\n",
      "003966\n",
      "003970\n",
      "003973\n",
      "003974\n",
      "003986\n",
      "003992\n",
      "003994\n",
      "003996\n",
      "004008\n",
      "004010\n",
      "004012\n",
      "004014\n",
      "004016\n",
      "004017\n",
      "004019\n",
      "004020\n",
      "004028\n",
      "004031\n",
      "004033\n",
      "004039\n",
      "004046\n",
      "004047\n",
      "004051\n",
      "004058\n",
      "004060\n",
      "004067\n",
      "004073\n",
      "004075\n",
      "004077\n",
      "004085\n",
      "004089\n",
      "004091\n",
      "004092\n",
      "004110\n",
      "004111\n",
      "004117\n",
      "004121\n",
      "004122\n",
      "004133\n",
      "004136\n",
      "004137\n",
      "004138\n",
      "004141\n",
      "004142\n",
      "004145\n",
      "004158\n",
      "004168\n",
      "004169\n",
      "004170\n",
      "004191\n",
      "004195\n",
      "004201\n",
      "004205\n",
      "004209\n",
      "004223\n",
      "004224\n",
      "004229\n",
      "004230\n",
      "004231\n",
      "004232\n",
      "004237\n",
      "004244\n",
      "004246\n",
      "004257\n",
      "004258\n",
      "004259\n",
      "004265\n",
      "004269\n",
      "004270\n",
      "004273\n",
      "004275\n",
      "004279\n",
      "004281\n",
      "004287\n",
      "004291\n",
      "004292\n",
      "004296\n",
      "004298\n",
      "004304\n",
      "004321\n",
      "004322\n",
      "004323\n",
      "004327\n",
      "004329\n",
      "004339\n",
      "004346\n",
      "004356\n",
      "004359\n",
      "004360\n",
      "004361\n",
      "004370\n",
      "004372\n",
      "004380\n",
      "004384\n",
      "004387\n",
      "004389\n",
      "004392\n",
      "004397\n",
      "004404\n",
      "004424\n",
      "004433\n",
      "004437\n",
      "004438\n",
      "004457\n",
      "004459\n",
      "004463\n",
      "004464\n",
      "004466\n",
      "004470\n",
      "004471\n",
      "004479\n",
      "004481\n",
      "004487\n",
      "004494\n",
      "004495\n",
      "004496\n",
      "004499\n",
      "004508\n",
      "004509\n",
      "004512\n",
      "004517\n",
      "004518\n",
      "004520\n",
      "004528\n",
      "004530\n",
      "004535\n",
      "004537\n",
      "004542\n",
      "004553\n",
      "004562\n",
      "004563\n",
      "004571\n",
      "004574\n",
      "004588\n",
      "004595\n",
      "004601\n",
      "004604\n",
      "004605\n",
      "004607\n",
      "004609\n",
      "004611\n",
      "004622\n",
      "004630\n",
      "004631\n",
      "004647\n",
      "004648\n",
      "004653\n",
      "004656\n",
      "004660\n",
      "004662\n",
      "004672\n",
      "004673\n",
      "004676\n",
      "004679\n",
      "004685\n",
      "004686\n",
      "004687\n",
      "004692\n",
      "004693\n",
      "004694\n",
      "004699\n",
      "004706\n",
      "004707\n",
      "004715\n",
      "004718\n",
      "004722\n",
      "004723\n",
      "004727\n",
      "004732\n",
      "004742\n",
      "004750\n",
      "004753\n",
      "004760\n",
      "004761\n",
      "004770\n",
      "004779\n",
      "004782\n",
      "004788\n",
      "004789\n",
      "004790\n",
      "004796\n",
      "004797\n",
      "004801\n",
      "004812\n",
      "004815\n",
      "004826\n",
      "004828\n",
      "004830\n",
      "004831\n",
      "004832\n",
      "004834\n",
      "004840\n",
      "004846\n",
      "004852\n",
      "004857\n",
      "004859\n",
      "004868\n",
      "004872\n",
      "004879\n",
      "004882\n",
      "004886\n",
      "004897\n",
      "004898\n",
      "004902\n",
      "004913\n",
      "004916\n",
      "004928\n",
      "004931\n",
      "004935\n",
      "004939\n",
      "004943\n",
      "004946\n",
      "004950\n",
      "004953\n",
      "004954\n",
      "004968\n",
      "004976\n",
      "004977\n",
      "004985\n",
      "004987\n",
      "004990\n",
      "004994\n",
      "004995\n",
      "005014\n",
      "005024\n",
      "005027\n",
      "005029\n",
      "005036\n",
      "005037\n",
      "005039\n",
      "005042\n",
      "005045\n",
      "005052\n",
      "005054\n",
      "005057\n",
      "005061\n",
      "005063\n",
      "005064\n",
      "005081\n",
      "005086\n",
      "005090\n",
      "005093\n",
      "005097\n",
      "005102\n",
      "005107\n",
      "005111\n",
      "005114\n",
      "005116\n",
      "005122\n",
      "005124\n",
      "005129\n",
      "005134\n",
      "005144\n",
      "005146\n",
      "005156\n",
      "005159\n",
      "005160\n",
      "005161\n",
      "005173\n",
      "005176\n",
      "005189\n",
      "005190\n",
      "005195\n",
      "005208\n",
      "005219\n",
      "005220\n",
      "005222\n",
      "005229\n",
      "005230\n",
      "005231\n",
      "005236\n",
      "005239\n",
      "005242\n",
      "005245\n",
      "005248\n",
      "005257\n",
      "005260\n",
      "005268\n",
      "005269\n",
      "005273\n",
      "005274\n",
      "005278\n",
      "005281\n",
      "005283\n",
      "005297\n",
      "005298\n",
      "005303\n",
      "005304\n",
      "005306\n",
      "005307\n",
      "005310\n",
      "005311\n",
      "005315\n",
      "005326\n",
      "005327\n",
      "005331\n",
      "005336\n",
      "005343\n",
      "005344\n",
      "005346\n",
      "005348\n",
      "005351\n",
      "005368\n",
      "005371\n",
      "005374\n",
      "005383\n",
      "005384\n",
      "005385\n",
      "005396\n",
      "005398\n",
      "005405\n",
      "005410\n",
      "005413\n",
      "005414\n",
      "005417\n",
      "005419\n",
      "005421\n",
      "005424\n",
      "005429\n",
      "005430\n",
      "005439\n",
      "005441\n",
      "005448\n",
      "005450\n",
      "005451\n",
      "005457\n",
      "005461\n",
      "005467\n",
      "005471\n",
      "005475\n",
      "005478\n",
      "005485\n",
      "005487\n",
      "005496\n",
      "005499\n",
      "005507\n",
      "005508\n",
      "005509\n",
      "005511\n",
      "005522\n",
      "005526\n",
      "005527\n",
      "005536\n",
      "005541\n",
      "005542\n",
      "005547\n",
      "005550\n",
      "005552\n",
      "005554\n",
      "005563\n",
      "005573\n",
      "005574\n",
      "005582\n",
      "005583\n",
      "005585\n",
      "005593\n",
      "005599\n",
      "005600\n",
      "005606\n",
      "005608\n",
      "005611\n",
      "005613\n",
      "005615\n",
      "005618\n",
      "005631\n",
      "005636\n",
      "005639\n",
      "005644\n",
      "005645\n",
      "005648\n",
      "005652\n",
      "005653\n",
      "005657\n",
      "005658\n",
      "005660\n",
      "005662\n",
      "005664\n",
      "005669\n",
      "005672\n",
      "005680\n",
      "005682\n",
      "005693\n",
      "005695\n",
      "005700\n",
      "005702\n",
      "005704\n",
      "005714\n",
      "005716\n",
      "005719\n",
      "005728\n",
      "005729\n",
      "005731\n",
      "005732\n",
      "005738\n",
      "005740\n",
      "005741\n",
      "005747\n",
      "005757\n",
      "005761\n",
      "005764\n",
      "005765\n",
      "005768\n",
      "005773\n",
      "005780\n",
      "005784\n",
      "005786\n",
      "005788\n",
      "005790\n",
      "005796\n",
      "005799\n",
      "005811\n",
      "005814\n",
      "005818\n",
      "005824\n",
      "005826\n",
      "005829\n",
      "005836\n",
      "005840\n",
      "005845\n",
      "005850\n",
      "005852\n",
      "005856\n",
      "005859\n",
      "005863\n",
      "005873\n",
      "005881\n",
      "005884\n",
      "005885\n",
      "005889\n",
      "005903\n",
      "005905\n",
      "005906\n",
      "005908\n",
      "005910\n",
      "005919\n",
      "005923\n",
      "005928\n",
      "005930\n",
      "005938\n",
      "005940\n",
      "005948\n",
      "005951\n",
      "005956\n",
      "005961\n",
      "005971\n",
      "005975\n",
      "005979\n",
      "005984\n",
      "005985\n",
      "005988\n",
      "005989\n",
      "005990\n",
      "005991\n",
      "005992\n",
      "006004\n",
      "006005\n",
      "006011\n",
      "006028\n",
      "006029\n",
      "006033\n",
      "006038\n",
      "006041\n",
      "006043\n",
      "006071\n",
      "006073\n",
      "006095\n",
      "006103\n",
      "006104\n",
      "006117\n",
      "006128\n",
      "006130\n",
      "006133\n",
      "006140\n",
      "006148\n",
      "006150\n",
      "006151\n",
      "006156\n",
      "006158\n",
      "006161\n",
      "006163\n",
      "006171\n",
      "006172\n",
      "006175\n",
      "006176\n",
      "006177\n",
      "006179\n",
      "006180\n",
      "006185\n",
      "006187\n",
      "006198\n",
      "006209\n",
      "006210\n",
      "006221\n",
      "006225\n",
      "006229\n",
      "006233\n",
      "006235\n",
      "006236\n",
      "006238\n",
      "006240\n",
      "006243\n",
      "006259\n",
      "006272\n",
      "006276\n",
      "006282\n",
      "006284\n",
      "006285\n",
      "006286\n",
      "006289\n",
      "006296\n",
      "006300\n",
      "006320\n",
      "006339\n",
      "006341\n",
      "006344\n",
      "006346\n",
      "006351\n",
      "006352\n",
      "006355\n",
      "006369\n",
      "006381\n",
      "006387\n",
      "006391\n",
      "006392\n",
      "006395\n",
      "006398\n",
      "006409\n",
      "006427\n",
      "006428\n",
      "006433\n",
      "006440\n",
      "006442\n",
      "006444\n",
      "006445\n",
      "006450\n",
      "006455\n",
      "006459\n",
      "006463\n",
      "006465\n",
      "006475\n",
      "006476\n",
      "006482\n",
      "006483\n",
      "006486\n",
      "006488\n",
      "006492\n",
      "006497\n",
      "006503\n",
      "006506\n",
      "006519\n",
      "006529\n",
      "006532\n",
      "006536\n",
      "006542\n",
      "006550\n",
      "006551\n",
      "006560\n",
      "006572\n",
      "006576\n",
      "006578\n",
      "006583\n",
      "006584\n",
      "006585\n",
      "006587\n",
      "006588\n",
      "006599\n",
      "006602\n",
      "006609\n",
      "006610\n",
      "006611\n",
      "006612\n",
      "006617\n",
      "006618\n",
      "006621\n",
      "006625\n",
      "006626\n",
      "006631\n",
      "006637\n",
      "006645\n",
      "006647\n",
      "006648\n",
      "006658\n",
      "006661\n",
      "006664\n",
      "006666\n",
      "006667\n",
      "006668\n",
      "006670\n",
      "006677\n",
      "006682\n",
      "006690\n",
      "006695\n",
      "006696\n",
      "006697\n",
      "006698\n",
      "006699\n",
      "006703\n",
      "006707\n",
      "006709\n",
      "006714\n",
      "006718\n",
      "006722\n",
      "006727\n",
      "006731\n",
      "006735\n",
      "006738\n",
      "006739\n",
      "006748\n",
      "006753\n",
      "006762\n",
      "006766\n",
      "006768\n",
      "006772\n",
      "006777\n",
      "006782\n",
      "006783\n",
      "006784\n",
      "006786\n",
      "006789\n",
      "006797\n",
      "006800\n",
      "006802\n",
      "006803\n",
      "006808\n",
      "006810\n",
      "006821\n",
      "006824\n",
      "006833\n",
      "006835\n",
      "006836\n",
      "006838\n",
      "006840\n",
      "006841\n",
      "006847\n",
      "006850\n",
      "006855\n",
      "006858\n",
      "006862\n",
      "006864\n",
      "006866\n",
      "006868\n",
      "006874\n",
      "006876\n",
      "006880\n",
      "006887\n",
      "006896\n",
      "006903\n",
      "006908\n",
      "006910\n",
      "006911\n",
      "006912\n",
      "006919\n",
      "006924\n",
      "006932\n",
      "006934\n",
      "006939\n",
      "006943\n",
      "006945\n",
      "006948\n",
      "006950\n",
      "006956\n",
      "006958\n",
      "006959\n",
      "006963\n",
      "006965\n",
      "006968\n",
      "006981\n",
      "006983\n",
      "006987\n",
      "006988\n",
      "006989\n",
      "006990\n",
      "007004\n",
      "007007\n",
      "007016\n",
      "007020\n",
      "007023\n",
      "007031\n",
      "007035\n",
      "007036\n",
      "007040\n",
      "007045\n",
      "007046\n",
      "007048\n",
      "007049\n",
      "007052\n",
      "007054\n",
      "007059\n",
      "007073\n",
      "007079\n",
      "007080\n",
      "007093\n",
      "007108\n",
      "007109\n",
      "007114\n",
      "007117\n",
      "007121\n",
      "007122\n",
      "007128\n",
      "007129\n",
      "007138\n",
      "007139\n",
      "007140\n",
      "007147\n",
      "007148\n",
      "007150\n",
      "007153\n",
      "007154\n",
      "007159\n",
      "007163\n",
      "007167\n",
      "007172\n",
      "007182\n",
      "007184\n",
      "007187\n",
      "007189\n",
      "007200\n",
      "007208\n",
      "007213\n",
      "007214\n",
      "007216\n",
      "007227\n",
      "007234\n",
      "007236\n",
      "007243\n",
      "007244\n",
      "007256\n",
      "007260\n",
      "007261\n",
      "007266\n",
      "007270\n",
      "007271\n",
      "007276\n",
      "007284\n",
      "007292\n",
      "007296\n",
      "007298\n",
      "007305\n",
      "007308\n",
      "007314\n",
      "007318\n",
      "007325\n",
      "007330\n",
      "007336\n",
      "007346\n",
      "007350\n",
      "007359\n",
      "007369\n",
      "007375\n",
      "007381\n",
      "007388\n",
      "007398\n",
      "007408\n",
      "007410\n",
      "007413\n",
      "007414\n",
      "007436\n",
      "007438\n",
      "007445\n",
      "007446\n",
      "007448\n",
      "007449\n",
      "007457\n",
      "007466\n",
      "007475\n",
      "007480\n",
      "007482\n",
      "007483\n",
      "007484\n",
      "007493\n",
      "007497\n",
      "007498\n",
      "007511\n",
      "007526\n",
      "007535\n",
      "007540\n",
      "007543\n",
      "007547\n",
      "007558\n",
      "007559\n",
      "007563\n",
      "007566\n",
      "007568\n",
      "007570\n",
      "007571\n",
      "007572\n",
      "007576\n",
      "007578\n",
      "007579\n",
      "007586\n",
      "007590\n",
      "007592\n",
      "007594\n",
      "007605\n",
      "007606\n",
      "007611\n",
      "007618\n",
      "007619\n",
      "007629\n",
      "007631\n",
      "007639\n",
      "007647\n",
      "007649\n",
      "007653\n",
      "007656\n",
      "007657\n",
      "007668\n",
      "007675\n",
      "007683\n",
      "007687\n",
      "007694\n",
      "007697\n",
      "007699\n",
      "007709\n",
      "007712\n",
      "007718\n",
      "007723\n",
      "007727\n",
      "007735\n",
      "007745\n",
      "007746\n",
      "007748\n",
      "007751\n",
      "007760\n",
      "007763\n",
      "007768\n",
      "007777\n",
      "007786\n",
      "007793\n",
      "007795\n",
      "007799\n",
      "007809\n",
      "007810\n",
      "007813\n",
      "007814\n",
      "007815\n",
      "007821\n",
      "007836\n",
      "007841\n",
      "007856\n",
      "007872\n",
      "007877\n",
      "007878\n",
      "007884\n",
      "007889\n",
      "007897\n",
      "007901\n",
      "007902\n",
      "007910\n",
      "007911\n",
      "007915\n",
      "007919\n",
      "007928\n",
      "007931\n",
      "007932\n",
      "007940\n",
      "007943\n",
      "007946\n",
      "007953\n",
      "007963\n",
      "007964\n",
      "007970\n",
      "007971\n",
      "007979\n",
      "007984\n",
      "007987\n",
      "007996\n",
      "007997\n",
      "007998\n",
      "008002\n",
      "008008\n",
      "008019\n",
      "008023\n",
      "008024\n",
      "008026\n",
      "008033\n",
      "008040\n",
      "008042\n",
      "008043\n",
      "008048\n",
      "008053\n",
      "008069\n",
      "008072\n",
      "008079\n",
      "008082\n",
      "008083\n",
      "008086\n",
      "008091\n",
      "008096\n",
      "008103\n",
      "008105\n",
      "008108\n",
      "008122\n",
      "008125\n",
      "008127\n",
      "008130\n",
      "008132\n",
      "008137\n",
      "008139\n",
      "008142\n",
      "008160\n",
      "008163\n",
      "008164\n",
      "008171\n",
      "008175\n",
      "008177\n",
      "008180\n",
      "008188\n",
      "008190\n",
      "008199\n",
      "008204\n",
      "008209\n",
      "008211\n",
      "008216\n",
      "008218\n",
      "008220\n",
      "008224\n",
      "008225\n",
      "008232\n",
      "008241\n",
      "008248\n",
      "008253\n",
      "008261\n",
      "008263\n",
      "008272\n",
      "008279\n",
      "008281\n",
      "008284\n",
      "008285\n",
      "008293\n",
      "008294\n",
      "008301\n",
      "008302\n",
      "008307\n",
      "008312\n",
      "008313\n",
      "008315\n",
      "008327\n",
      "008332\n",
      "008341\n",
      "008360\n",
      "008368\n",
      "008374\n",
      "008376\n",
      "008381\n",
      "008385\n",
      "008387\n",
      "008390\n",
      "008391\n",
      "008397\n",
      "008398\n",
      "008410\n",
      "008413\n",
      "008430\n",
      "008433\n",
      "008437\n",
      "008444\n",
      "008445\n",
      "008449\n",
      "008452\n",
      "008453\n",
      "008461\n",
      "008467\n",
      "008470\n",
      "008475\n",
      "008482\n",
      "008485\n",
      "008503\n",
      "008509\n",
      "008513\n",
      "008514\n",
      "008526\n",
      "008529\n",
      "008541\n",
      "008550\n",
      "008559\n",
      "008562\n",
      "008573\n",
      "008576\n",
      "008582\n",
      "008585\n",
      "008587\n",
      "008592\n",
      "008596\n",
      "008602\n",
      "008604\n",
      "008612\n",
      "008617\n",
      "008618\n",
      "008628\n",
      "008638\n",
      "008644\n",
      "008647\n",
      "008663\n",
      "008665\n",
      "008676\n",
      "008690\n",
      "008692\n",
      "008698\n",
      "008699\n",
      "008701\n",
      "008702\n",
      "008706\n",
      "008718\n",
      "008728\n",
      "008731\n",
      "008738\n",
      "008739\n",
      "008742\n",
      "008744\n",
      "008748\n",
      "008752\n",
      "008766\n",
      "008768\n",
      "008769\n",
      "008771\n",
      "008775\n",
      "008794\n",
      "008801\n",
      "008805\n",
      "008810\n",
      "008813\n",
      "008817\n",
      "008833\n",
      "008836\n",
      "008838\n",
      "008862\n",
      "008865\n",
      "008867\n",
      "008872\n",
      "008876\n",
      "008879\n",
      "008884\n",
      "008886\n",
      "008891\n",
      "008892\n",
      "008900\n",
      "008911\n",
      "008920\n",
      "008921\n",
      "008931\n",
      "008932\n",
      "008943\n",
      "008944\n",
      "008953\n",
      "008960\n",
      "008961\n",
      "008965\n",
      "008975\n",
      "008979\n",
      "008983\n",
      "009000\n",
      "009005\n",
      "009020\n",
      "009022\n",
      "009024\n",
      "009034\n",
      "009036\n",
      "009045\n",
      "009049\n",
      "009064\n",
      "009068\n",
      "009073\n",
      "009078\n",
      "009079\n",
      "009087\n",
      "009089\n",
      "009091\n",
      "009114\n",
      "009123\n",
      "009128\n",
      "009136\n",
      "009141\n",
      "009153\n",
      "009160\n",
      "009161\n",
      "009174\n",
      "009180\n",
      "009184\n",
      "009193\n",
      "009197\n",
      "009202\n",
      "009208\n",
      "009209\n",
      "009214\n",
      "009224\n",
      "009230\n",
      "009236\n",
      "009238\n",
      "009239\n",
      "009242\n",
      "009251\n",
      "009259\n",
      "009273\n",
      "009283\n",
      "009286\n",
      "009287\n",
      "009288\n",
      "009290\n",
      "009291\n",
      "009295\n",
      "009296\n",
      "009299\n",
      "009303\n",
      "009306\n",
      "009308\n",
      "009312\n",
      "009316\n",
      "009318\n",
      "009327\n",
      "009330\n",
      "009331\n",
      "009336\n",
      "009342\n",
      "009343\n",
      "009349\n",
      "009358\n",
      "009362\n",
      "009368\n",
      "009374\n",
      "009375\n",
      "009377\n",
      "009382\n",
      "009393\n",
      "009394\n",
      "009407\n",
      "009410\n",
      "009411\n",
      "009412\n",
      "009413\n",
      "009414\n",
      "009418\n",
      "009420\n",
      "009421\n",
      "009424\n",
      "009433\n",
      "009434\n",
      "009437\n",
      "009440\n",
      "009448\n",
      "009456\n",
      "009460\n",
      "009468\n",
      "009469\n",
      "009477\n",
      "009479\n",
      "009481\n",
      "009490\n",
      "009494\n",
      "009496\n",
      "009497\n",
      "009499\n",
      "009500\n",
      "009504\n",
      "009512\n",
      "009517\n",
      "009518\n",
      "009520\n",
      "009524\n",
      "009526\n",
      "009531\n",
      "009532\n",
      "009537\n",
      "009541\n",
      "009542\n",
      "009546\n",
      "009551\n",
      "009557\n",
      "009558\n",
      "009565\n",
      "009567\n",
      "009568\n",
      "009573\n",
      "009577\n",
      "009579\n",
      "009585\n",
      "009591\n",
      "009596\n",
      "009600\n",
      "009603\n",
      "009609\n",
      "009611\n",
      "009613\n",
      "009614\n",
      "009617\n",
      "009618\n",
      "009638\n",
      "009641\n",
      "009647\n",
      "009649\n",
      "009654\n",
      "009655\n",
      "009656\n",
      "009659\n",
      "009666\n",
      "009668\n",
      "009671\n",
      "009676\n",
      "009684\n",
      "009687\n",
      "009691\n",
      "009693\n",
      "009698\n",
      "009703\n",
      "009711\n",
      "009712\n",
      "009713\n",
      "009717\n",
      "009718\n",
      "009721\n",
      "009726\n",
      "009732\n",
      "009734\n",
      "009738\n",
      "009747\n",
      "009754\n",
      "009755\n",
      "009756\n",
      "009762\n",
      "009767\n",
      "009773\n",
      "009776\n",
      "009780\n",
      "009781\n",
      "009789\n",
      "009792\n",
      "009796\n",
      "009800\n",
      "009809\n",
      "009813\n",
      "009822\n",
      "009828\n",
      "009841\n",
      "009845\n",
      "009848\n",
      "009851\n",
      "009859\n",
      "009867\n",
      "009868\n",
      "009869\n",
      "009874\n",
      "009877\n",
      "009878\n",
      "009879\n",
      "009880\n",
      "009881\n",
      "009882\n",
      "009900\n",
      "009902\n",
      "009917\n",
      "009918\n",
      "009926\n",
      "009935\n",
      "009942\n",
      "009944\n",
      "009946\n",
      "009947\n",
      "009949\n",
      "009950\n",
      "009954\n",
      "009958\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 300  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 300, 300])\n",
      "32\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0365, 0.1676, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace)\n",
      "  (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "  (32): ReLU(inplace)\n",
      "  (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (34): ReLU(inplace)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "from utils.ssd import SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using vgg weights\n",
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # class including background\n",
    "    'input_size': 300,  # input size\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # bbox aspects\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],  # feature map size of each stages\n",
    "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]], # anchor settings\n",
    "}\n",
    "\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "print(\"using vgg weights\")\n",
    "vgg_weights = torch.load(\"./weights/vgg16_reducedfc.pth\")\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# 初期値を適応\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (vgg): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (32): ReLU(inplace)\n",
      "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (34): ReLU(inplace)\n",
      "  )\n",
      "  (extras): ModuleList(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (L2Norm): L2Norm()\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 10 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は10回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/ssd300_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 8.9183 || 10iter: 49.2415 sec.\n",
      "Iteration 20 || Loss: 8.2571 || 10iter: 19.4378 sec.\n",
      "Iteration 30 || Loss: 18.7474 || 10iter: 19.4420 sec.\n",
      "Iteration 40 || Loss: 10.8588 || 10iter: 19.4117 sec.\n",
      "Iteration 50 || Loss: 9.7334 || 10iter: 19.3884 sec.\n",
      "Iteration 60 || Loss: 7.4762 || 10iter: 19.3043 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:656.4293 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  181.9912 sec.\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 70 || Loss: 6.5129 || 10iter: 9.4671 sec.\n",
      "Iteration 80 || Loss: 61.8643 || 10iter: 19.3187 sec.\n",
      "Iteration 90 || Loss: 10.1675 || 10iter: 19.2539 sec.\n",
      "Iteration 100 || Loss: 10.8150 || 10iter: 19.1732 sec.\n",
      "Iteration 110 || Loss: 8.5794 || 10iter: 19.2347 sec.\n",
      "Iteration 120 || Loss: 8.2563 || 10iter: 19.1311 sec.\n",
      "Iteration 130 || Loss: 13.7946 || 10iter: 19.0443 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:763.3820 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  136.7749 sec.\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 140 || Loss: 6.8033 || 10iter: 17.2705 sec.\n",
      "Iteration 150 || Loss: 8.1428 || 10iter: 19.1522 sec.\n",
      "Iteration 160 || Loss: 10.4509 || 10iter: 18.8280 sec.\n",
      "Iteration 170 || Loss: 7.1076 || 10iter: 18.8036 sec.\n",
      "Iteration 180 || Loss: 7.8436 || 10iter: 18.6935 sec.\n",
      "Iteration 190 || Loss: 7.3860 || 10iter: 18.6127 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:552.5822 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.6888 sec.\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 200 || Loss: 9.3731 || 10iter: 4.9496 sec.\n",
      "Iteration 210 || Loss: 8.7092 || 10iter: 18.5836 sec.\n",
      "Iteration 220 || Loss: 7.7370 || 10iter: 18.6027 sec.\n",
      "Iteration 230 || Loss: 7.2231 || 10iter: 18.7389 sec.\n",
      "Iteration 240 || Loss: 8.4859 || 10iter: 18.6806 sec.\n",
      "Iteration 250 || Loss: 7.2477 || 10iter: 18.6290 sec.\n",
      "Iteration 260 || Loss: 6.7015 || 10iter: 18.7090 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:559.2486 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.5440 sec.\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 270 || Loss: 10.6243 || 10iter: 13.0889 sec.\n",
      "Iteration 280 || Loss: 9.1721 || 10iter: 18.6254 sec.\n",
      "Iteration 290 || Loss: 6.4489 || 10iter: 18.7074 sec.\n",
      "Iteration 300 || Loss: 9.0970 || 10iter: 18.6198 sec.\n",
      "Iteration 310 || Loss: 11.5041 || 10iter: 18.6832 sec.\n",
      "Iteration 320 || Loss: 9.3215 || 10iter: 18.6114 sec.\n",
      "Iteration 330 || Loss: 13.3222 || 10iter: 18.2259 sec.\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:609.3863 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.5363 sec.\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 340 || Loss: 8.2499 || 10iter: 20.9121 sec.\n",
      "Iteration 350 || Loss: 8.8925 || 10iter: 18.5124 sec.\n",
      "Iteration 360 || Loss: 7.3500 || 10iter: 18.5980 sec.\n",
      "Iteration 370 || Loss: 7.5848 || 10iter: 18.6565 sec.\n",
      "Iteration 380 || Loss: 9.8803 || 10iter: 18.6623 sec.\n",
      "Iteration 390 || Loss: 9.2031 || 10iter: 18.6369 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:610.5802 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.4461 sec.\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 400 || Loss: 15.3007 || 10iter: 9.3393 sec.\n",
      "Iteration 410 || Loss: 10.3698 || 10iter: 18.7086 sec.\n",
      "Iteration 420 || Loss: 8.7773 || 10iter: 18.6039 sec.\n",
      "Iteration 430 || Loss: 13.9268 || 10iter: 18.6746 sec.\n",
      "Iteration 440 || Loss: 7.2505 || 10iter: 18.7797 sec.\n",
      "Iteration 450 || Loss: 10.1942 || 10iter: 18.7105 sec.\n",
      "Iteration 460 || Loss: 8.8163 || 10iter: 18.6215 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:634.6042 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9970 sec.\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 470 || Loss: 10.7318 || 10iter: 16.8933 sec.\n",
      "Iteration 480 || Loss: 9.6307 || 10iter: 18.5763 sec.\n",
      "Iteration 490 || Loss: 7.3362 || 10iter: 18.6360 sec.\n",
      "Iteration 500 || Loss: 12.5674 || 10iter: 18.5916 sec.\n",
      "Iteration 510 || Loss: 9.5555 || 10iter: 18.6048 sec.\n",
      "Iteration 520 || Loss: 9.1214 || 10iter: 18.6532 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:595.1201 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.3317 sec.\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 530 || Loss: 9.4026 || 10iter: 5.5458 sec.\n",
      "Iteration 540 || Loss: 10.9099 || 10iter: 18.6752 sec.\n",
      "Iteration 550 || Loss: 10.2273 || 10iter: 18.6507 sec.\n",
      "Iteration 560 || Loss: 8.7541 || 10iter: 18.6066 sec.\n",
      "Iteration 570 || Loss: 9.2651 || 10iter: 18.6644 sec.\n",
      "Iteration 580 || Loss: 6.7671 || 10iter: 18.6916 sec.\n",
      "Iteration 590 || Loss: 11.1973 || 10iter: 18.6588 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:647.4189 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0361 sec.\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 600 || Loss: 9.1085 || 10iter: 12.8736 sec.\n",
      "Iteration 610 || Loss: 6.8832 || 10iter: 18.5858 sec.\n",
      "Iteration 620 || Loss: 12.3353 || 10iter: 18.7368 sec.\n",
      "Iteration 630 || Loss: 10.3795 || 10iter: 18.6650 sec.\n",
      "Iteration 640 || Loss: 6.9616 || 10iter: 18.6991 sec.\n",
      "Iteration 650 || Loss: 6.5752 || 10iter: 18.7052 sec.\n",
      "Iteration 660 || Loss: 8.6272 || 10iter: 18.3401 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:627.5925 ||Epoch_VAL_Loss:308.4429\n",
      "timer:  160.5772 sec.\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 670 || Loss: 10.3794 || 10iter: 20.9479 sec.\n",
      "Iteration 680 || Loss: 7.4388 || 10iter: 18.6791 sec.\n",
      "Iteration 690 || Loss: 7.3869 || 10iter: 18.7981 sec.\n",
      "Iteration 700 || Loss: 7.1967 || 10iter: 18.7204 sec.\n",
      "Iteration 710 || Loss: 7.7391 || 10iter: 18.7974 sec.\n",
      "Iteration 720 || Loss: 9.7288 || 10iter: 18.7505 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:557.5615 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.1579 sec.\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 730 || Loss: 6.8381 || 10iter: 9.1292 sec.\n",
      "Iteration 740 || Loss: 8.9057 || 10iter: 18.7384 sec.\n",
      "Iteration 750 || Loss: 7.3402 || 10iter: 18.7530 sec.\n",
      "Iteration 760 || Loss: 7.3893 || 10iter: 18.7753 sec.\n",
      "Iteration 770 || Loss: 11.2461 || 10iter: 18.8549 sec.\n",
      "Iteration 780 || Loss: 9.7638 || 10iter: 18.6965 sec.\n",
      "Iteration 790 || Loss: 6.3278 || 10iter: 18.8003 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:557.2836 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.3004 sec.\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 800 || Loss: 7.1010 || 10iter: 17.2726 sec.\n",
      "Iteration 810 || Loss: 7.5542 || 10iter: 18.7543 sec.\n",
      "Iteration 820 || Loss: 5.8938 || 10iter: 18.7802 sec.\n",
      "Iteration 830 || Loss: 7.7157 || 10iter: 18.7568 sec.\n",
      "Iteration 840 || Loss: 7.6035 || 10iter: 18.7145 sec.\n",
      "Iteration 850 || Loss: 7.8958 || 10iter: 18.7120 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:490.7164 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.4224 sec.\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n",
      "Iteration 860 || Loss: 9.0186 || 10iter: 5.0526 sec.\n",
      "Iteration 870 || Loss: 5.5295 || 10iter: 18.9159 sec.\n",
      "Iteration 880 || Loss: 7.3859 || 10iter: 18.7173 sec.\n",
      "Iteration 890 || Loss: 9.8649 || 10iter: 18.7471 sec.\n",
      "Iteration 900 || Loss: 7.0183 || 10iter: 18.8311 sec.\n",
      "Iteration 910 || Loss: 6.0113 || 10iter: 18.8365 sec.\n",
      "Iteration 920 || Loss: 5.6592 || 10iter: 18.7716 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:472.4163 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.5506 sec.\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 930 || Loss: 8.3128 || 10iter: 12.9678 sec.\n",
      "Iteration 940 || Loss: 8.9450 || 10iter: 18.6595 sec.\n",
      "Iteration 950 || Loss: 8.4133 || 10iter: 18.7072 sec.\n",
      "Iteration 960 || Loss: 7.7518 || 10iter: 18.7566 sec.\n",
      "Iteration 970 || Loss: 5.2993 || 10iter: 18.6933 sec.\n",
      "Iteration 980 || Loss: 5.6633 || 10iter: 18.7526 sec.\n",
      "Iteration 990 || Loss: 8.2714 || 10iter: 18.4728 sec.\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:454.2109 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.1434 sec.\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1000 || Loss: 8.6485 || 10iter: 20.6671 sec.\n",
      "Iteration 1010 || Loss: 7.6440 || 10iter: 18.6851 sec.\n",
      "Iteration 1020 || Loss: 5.6770 || 10iter: 18.7777 sec.\n",
      "Iteration 1030 || Loss: 6.2016 || 10iter: 18.7542 sec.\n",
      "Iteration 1040 || Loss: 6.2375 || 10iter: 18.8177 sec.\n",
      "Iteration 1050 || Loss: 6.0573 || 10iter: 18.8923 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:437.5666 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0301 sec.\n",
      "-------------\n",
      "Epoch 17/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1060 || Loss: 6.5338 || 10iter: 9.4177 sec.\n",
      "Iteration 1070 || Loss: 5.3087 || 10iter: 18.8080 sec.\n",
      "Iteration 1080 || Loss: 5.7408 || 10iter: 18.7376 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1090 || Loss: 6.9219 || 10iter: 18.7180 sec.\n",
      "Iteration 1100 || Loss: 5.9252 || 10iter: 18.7662 sec.\n",
      "Iteration 1110 || Loss: 5.2466 || 10iter: 18.6699 sec.\n",
      "Iteration 1120 || Loss: 5.4438 || 10iter: 18.7254 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:410.7246 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.5529 sec.\n",
      "-------------\n",
      "Epoch 18/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1130 || Loss: 5.4337 || 10iter: 17.3313 sec.\n",
      "Iteration 1140 || Loss: 5.2270 || 10iter: 18.7843 sec.\n",
      "Iteration 1150 || Loss: 5.2085 || 10iter: 18.6881 sec.\n",
      "Iteration 1160 || Loss: 7.1727 || 10iter: 18.7562 sec.\n",
      "Iteration 1170 || Loss: 6.5629 || 10iter: 18.7732 sec.\n",
      "Iteration 1180 || Loss: 5.2826 || 10iter: 18.6969 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:390.0972 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.4199 sec.\n",
      "-------------\n",
      "Epoch 19/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1190 || Loss: 5.2840 || 10iter: 5.0411 sec.\n",
      "Iteration 1200 || Loss: 5.4446 || 10iter: 18.7377 sec.\n",
      "Iteration 1210 || Loss: 5.6456 || 10iter: 18.5934 sec.\n",
      "Iteration 1220 || Loss: 5.4023 || 10iter: 18.7498 sec.\n",
      "Iteration 1230 || Loss: 5.6137 || 10iter: 18.7485 sec.\n",
      "Iteration 1240 || Loss: 5.9669 || 10iter: 18.7182 sec.\n",
      "Iteration 1250 || Loss: 6.3426 || 10iter: 18.6982 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:396.1062 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.8776 sec.\n",
      "-------------\n",
      "Epoch 20/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1260 || Loss: 7.8181 || 10iter: 13.4837 sec.\n",
      "Iteration 1270 || Loss: 6.1026 || 10iter: 18.7323 sec.\n",
      "Iteration 1280 || Loss: 6.4666 || 10iter: 18.7750 sec.\n",
      "Iteration 1290 || Loss: 5.3792 || 10iter: 18.6218 sec.\n",
      "Iteration 1300 || Loss: 5.3487 || 10iter: 18.6572 sec.\n",
      "Iteration 1310 || Loss: 5.3766 || 10iter: 18.6334 sec.\n",
      "Iteration 1320 || Loss: 6.0850 || 10iter: 18.3326 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:402.6482 ||Epoch_VAL_Loss:170.0355\n",
      "timer:  158.0699 sec.\n",
      "-------------\n",
      "Epoch 21/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1330 || Loss: 5.5591 || 10iter: 20.9417 sec.\n",
      "Iteration 1340 || Loss: 5.2537 || 10iter: 18.6712 sec.\n",
      "Iteration 1350 || Loss: 5.0311 || 10iter: 18.7491 sec.\n",
      "Iteration 1360 || Loss: 6.3948 || 10iter: 18.7104 sec.\n",
      "Iteration 1370 || Loss: 7.4936 || 10iter: 18.7112 sec.\n",
      "Iteration 1380 || Loss: 8.4559 || 10iter: 18.7239 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:410.1420 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.8815 sec.\n",
      "-------------\n",
      "Epoch 22/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1390 || Loss: 5.6753 || 10iter: 9.4971 sec.\n",
      "Iteration 1400 || Loss: 5.8392 || 10iter: 18.6145 sec.\n",
      "Iteration 1410 || Loss: 5.8307 || 10iter: 18.6028 sec.\n",
      "Iteration 1420 || Loss: 5.4332 || 10iter: 18.6814 sec.\n",
      "Iteration 1430 || Loss: 5.6709 || 10iter: 18.7467 sec.\n",
      "Iteration 1440 || Loss: 5.4433 || 10iter: 18.7363 sec.\n",
      "Iteration 1450 || Loss: 5.1498 || 10iter: 18.7443 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:399.0858 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.3633 sec.\n",
      "-------------\n",
      "Epoch 23/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1460 || Loss: 6.3234 || 10iter: 17.1031 sec.\n",
      "Iteration 1470 || Loss: 6.5375 || 10iter: 18.6757 sec.\n",
      "Iteration 1480 || Loss: 6.2919 || 10iter: 18.7850 sec.\n",
      "Iteration 1490 || Loss: 5.5720 || 10iter: 18.7533 sec.\n",
      "Iteration 1500 || Loss: 6.1745 || 10iter: 18.8392 sec.\n",
      "Iteration 1510 || Loss: 6.2667 || 10iter: 18.7064 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:380.5289 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.3029 sec.\n",
      "-------------\n",
      "Epoch 24/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1520 || Loss: 7.0464 || 10iter: 5.5373 sec.\n",
      "Iteration 1530 || Loss: 5.9723 || 10iter: 18.5867 sec.\n",
      "Iteration 1540 || Loss: 6.9843 || 10iter: 18.6690 sec.\n",
      "Iteration 1550 || Loss: 6.2893 || 10iter: 18.6834 sec.\n",
      "Iteration 1560 || Loss: 6.2685 || 10iter: 18.6626 sec.\n",
      "Iteration 1570 || Loss: 5.1706 || 10iter: 18.7998 sec.\n",
      "Iteration 1580 || Loss: 5.1091 || 10iter: 18.6291 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:403.9231 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.2806 sec.\n",
      "-------------\n",
      "Epoch 25/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1590 || Loss: 5.5840 || 10iter: 12.8780 sec.\n",
      "Iteration 1600 || Loss: 5.1689 || 10iter: 18.7895 sec.\n",
      "Iteration 1610 || Loss: 5.0853 || 10iter: 18.7411 sec.\n",
      "Iteration 1620 || Loss: 5.7470 || 10iter: 18.6685 sec.\n",
      "Iteration 1630 || Loss: 6.0678 || 10iter: 18.7978 sec.\n",
      "Iteration 1640 || Loss: 5.4494 || 10iter: 18.7581 sec.\n",
      "Iteration 1650 || Loss: 5.3244 || 10iter: 18.3073 sec.\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:368.7063 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9218 sec.\n",
      "-------------\n",
      "Epoch 26/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1660 || Loss: 5.1308 || 10iter: 21.1962 sec.\n",
      "Iteration 1670 || Loss: 5.6880 || 10iter: 18.6748 sec.\n",
      "Iteration 1680 || Loss: 7.2602 || 10iter: 18.6860 sec.\n",
      "Iteration 1690 || Loss: 6.0442 || 10iter: 18.7466 sec.\n",
      "Iteration 1700 || Loss: 5.5830 || 10iter: 18.7336 sec.\n",
      "Iteration 1710 || Loss: 5.9243 || 10iter: 18.6864 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:394.2068 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.2316 sec.\n",
      "-------------\n",
      "Epoch 27/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1720 || Loss: 5.1914 || 10iter: 9.0615 sec.\n",
      "Iteration 1730 || Loss: 5.4800 || 10iter: 18.6951 sec.\n",
      "Iteration 1740 || Loss: 7.0915 || 10iter: 18.6927 sec.\n",
      "Iteration 1750 || Loss: 5.8243 || 10iter: 18.7612 sec.\n",
      "Iteration 1760 || Loss: 5.8071 || 10iter: 18.6653 sec.\n",
      "Iteration 1770 || Loss: 5.1068 || 10iter: 18.7726 sec.\n",
      "Iteration 1780 || Loss: 5.4620 || 10iter: 18.8320 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:392.9058 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0666 sec.\n",
      "-------------\n",
      "Epoch 28/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1790 || Loss: 5.2050 || 10iter: 17.1204 sec.\n",
      "Iteration 1800 || Loss: 5.5982 || 10iter: 18.7278 sec.\n",
      "Iteration 1810 || Loss: 5.3448 || 10iter: 18.7757 sec.\n",
      "Iteration 1820 || Loss: 5.1403 || 10iter: 18.6424 sec.\n",
      "Iteration 1830 || Loss: 5.1695 || 10iter: 18.7074 sec.\n",
      "Iteration 1840 || Loss: 5.5335 || 10iter: 18.7016 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:383.9505 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0524 sec.\n",
      "-------------\n",
      "Epoch 29/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1850 || Loss: 4.9813 || 10iter: 4.8944 sec.\n",
      "Iteration 1860 || Loss: 5.5047 || 10iter: 18.7426 sec.\n",
      "Iteration 1870 || Loss: 5.1790 || 10iter: 18.8048 sec.\n",
      "Iteration 1880 || Loss: 5.4418 || 10iter: 18.7624 sec.\n",
      "Iteration 1890 || Loss: 5.1818 || 10iter: 18.7174 sec.\n",
      "Iteration 1900 || Loss: 5.7988 || 10iter: 18.7320 sec.\n",
      "Iteration 1910 || Loss: 5.3373 || 10iter: 18.7653 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:374.3976 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9836 sec.\n",
      "-------------\n",
      "Epoch 30/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1920 || Loss: 5.3215 || 10iter: 13.9138 sec.\n",
      "Iteration 1930 || Loss: 5.8631 || 10iter: 18.7400 sec.\n",
      "Iteration 1940 || Loss: 15.1215 || 10iter: 18.8303 sec.\n",
      "Iteration 1950 || Loss: 8.4864 || 10iter: 18.6617 sec.\n",
      "Iteration 1960 || Loss: 6.3926 || 10iter: 18.7591 sec.\n",
      "Iteration 1970 || Loss: 6.6962 || 10iter: 18.6905 sec.\n",
      "Iteration 1980 || Loss: 5.9414 || 10iter: 18.3738 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:428.8935 ||Epoch_VAL_Loss:246.1065\n",
      "timer:  159.0007 sec.\n",
      "-------------\n",
      "Epoch 31/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1990 || Loss: 5.9856 || 10iter: 20.9771 sec.\n",
      "Iteration 2000 || Loss: 5.5380 || 10iter: 18.7521 sec.\n",
      "Iteration 2010 || Loss: 5.3016 || 10iter: 18.6741 sec.\n",
      "Iteration 2020 || Loss: 5.4833 || 10iter: 18.7326 sec.\n",
      "Iteration 2030 || Loss: 5.0067 || 10iter: 18.7380 sec.\n",
      "Iteration 2040 || Loss: 5.3482 || 10iter: 18.6439 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:371.8469 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9853 sec.\n",
      "-------------\n",
      "Epoch 32/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2050 || Loss: 5.2459 || 10iter: 8.7745 sec.\n",
      "Iteration 2060 || Loss: 7.1816 || 10iter: 18.7772 sec.\n",
      "Iteration 2070 || Loss: 8.0456 || 10iter: 18.7662 sec.\n",
      "Iteration 2080 || Loss: 5.6365 || 10iter: 18.7793 sec.\n",
      "Iteration 2090 || Loss: 5.1853 || 10iter: 18.8586 sec.\n",
      "Iteration 2100 || Loss: 5.4669 || 10iter: 18.7682 sec.\n",
      "Iteration 2110 || Loss: 5.2789 || 10iter: 18.7084 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:385.1125 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0047 sec.\n",
      "-------------\n",
      "Epoch 33/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2120 || Loss: 6.1139 || 10iter: 17.1166 sec.\n",
      "Iteration 2130 || Loss: 5.0752 || 10iter: 18.7412 sec.\n",
      "Iteration 2140 || Loss: 6.2258 || 10iter: 18.6538 sec.\n",
      "Iteration 2150 || Loss: 5.0282 || 10iter: 18.7716 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2160 || Loss: 6.9366 || 10iter: 18.7339 sec.\n",
      "Iteration 2170 || Loss: 7.1403 || 10iter: 18.6523 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:395.4583 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.1172 sec.\n",
      "-------------\n",
      "Epoch 34/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2180 || Loss: 6.1703 || 10iter: 5.6553 sec.\n",
      "Iteration 2190 || Loss: 5.5801 || 10iter: 18.6631 sec.\n",
      "Iteration 2200 || Loss: 6.1202 || 10iter: 18.5913 sec.\n",
      "Iteration 2210 || Loss: 6.7145 || 10iter: 18.7261 sec.\n",
      "Iteration 2220 || Loss: 5.8851 || 10iter: 18.7355 sec.\n",
      "Iteration 2230 || Loss: 6.5635 || 10iter: 18.6572 sec.\n",
      "Iteration 2240 || Loss: 5.7888 || 10iter: 18.7075 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:378.1537 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.4285 sec.\n",
      "-------------\n",
      "Epoch 35/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2250 || Loss: 5.6556 || 10iter: 13.0258 sec.\n",
      "Iteration 2260 || Loss: 7.3144 || 10iter: 18.6900 sec.\n",
      "Iteration 2270 || Loss: 5.0358 || 10iter: 18.6928 sec.\n",
      "Iteration 2280 || Loss: 5.6616 || 10iter: 18.7279 sec.\n",
      "Iteration 2290 || Loss: 5.4901 || 10iter: 18.6754 sec.\n",
      "Iteration 2300 || Loss: 6.0712 || 10iter: 18.7204 sec.\n",
      "Iteration 2310 || Loss: 5.4919 || 10iter: 18.2967 sec.\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:388.8490 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9310 sec.\n",
      "-------------\n",
      "Epoch 36/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2320 || Loss: 5.3115 || 10iter: 20.8199 sec.\n",
      "Iteration 2330 || Loss: 5.6015 || 10iter: 18.7567 sec.\n",
      "Iteration 2340 || Loss: 8.0526 || 10iter: 18.6149 sec.\n",
      "Iteration 2350 || Loss: 7.4235 || 10iter: 18.7627 sec.\n",
      "Iteration 2360 || Loss: 5.7531 || 10iter: 18.7524 sec.\n",
      "Iteration 2370 || Loss: 5.6488 || 10iter: 18.6779 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:390.2354 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7236 sec.\n",
      "-------------\n",
      "Epoch 37/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2380 || Loss: 5.5368 || 10iter: 9.5941 sec.\n",
      "Iteration 2390 || Loss: 5.0533 || 10iter: 18.7394 sec.\n",
      "Iteration 2400 || Loss: 6.9957 || 10iter: 18.7285 sec.\n",
      "Iteration 2410 || Loss: 6.2030 || 10iter: 18.6806 sec.\n",
      "Iteration 2420 || Loss: 5.7822 || 10iter: 18.7459 sec.\n",
      "Iteration 2430 || Loss: 6.6871 || 10iter: 18.8091 sec.\n",
      "Iteration 2440 || Loss: 6.2967 || 10iter: 18.7275 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:402.1357 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.7731 sec.\n",
      "-------------\n",
      "Epoch 38/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2450 || Loss: 6.0668 || 10iter: 17.4058 sec.\n",
      "Iteration 2460 || Loss: 5.4891 || 10iter: 18.6702 sec.\n",
      "Iteration 2470 || Loss: 5.0619 || 10iter: 18.7759 sec.\n",
      "Iteration 2480 || Loss: 4.9282 || 10iter: 18.7713 sec.\n",
      "Iteration 2490 || Loss: 10.0634 || 10iter: 18.7198 sec.\n",
      "Iteration 2500 || Loss: 6.6245 || 10iter: 18.8316 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:437.0470 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.6157 sec.\n",
      "-------------\n",
      "Epoch 39/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2510 || Loss: 5.4609 || 10iter: 4.7414 sec.\n",
      "Iteration 2520 || Loss: 5.2580 || 10iter: 18.7316 sec.\n",
      "Iteration 2530 || Loss: 5.0828 || 10iter: 18.6285 sec.\n",
      "Iteration 2540 || Loss: 5.4508 || 10iter: 18.8680 sec.\n",
      "Iteration 2550 || Loss: 5.7868 || 10iter: 18.7476 sec.\n",
      "Iteration 2560 || Loss: 5.3548 || 10iter: 18.7880 sec.\n",
      "Iteration 2570 || Loss: 5.4979 || 10iter: 18.7418 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:377.7580 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9350 sec.\n",
      "-------------\n",
      "Epoch 40/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2580 || Loss: 13.2998 || 10iter: 13.2277 sec.\n",
      "Iteration 2590 || Loss: 5.7808 || 10iter: 18.6587 sec.\n",
      "Iteration 2600 || Loss: 5.6547 || 10iter: 18.8209 sec.\n",
      "Iteration 2610 || Loss: 7.8431 || 10iter: 18.7482 sec.\n",
      "Iteration 2620 || Loss: 5.4438 || 10iter: 18.7279 sec.\n",
      "Iteration 2630 || Loss: 6.4552 || 10iter: 18.7801 sec.\n",
      "Iteration 2640 || Loss: 6.1926 || 10iter: 18.3289 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:418.4337 ||Epoch_VAL_Loss:173.6719\n",
      "timer:  158.2406 sec.\n",
      "-------------\n",
      "Epoch 41/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2650 || Loss: 4.9767 || 10iter: 21.2619 sec.\n",
      "Iteration 2660 || Loss: 6.1218 || 10iter: 18.7028 sec.\n",
      "Iteration 2670 || Loss: 8.8269 || 10iter: 18.7476 sec.\n",
      "Iteration 2680 || Loss: 5.6431 || 10iter: 18.7985 sec.\n",
      "Iteration 2690 || Loss: 5.0319 || 10iter: 18.7879 sec.\n",
      "Iteration 2700 || Loss: 6.5304 || 10iter: 18.7402 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:458.6646 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.4364 sec.\n",
      "-------------\n",
      "Epoch 42/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2710 || Loss: 5.8000 || 10iter: 9.5975 sec.\n",
      "Iteration 2720 || Loss: 9.1524 || 10iter: 18.6155 sec.\n",
      "Iteration 2730 || Loss: 5.4561 || 10iter: 18.7172 sec.\n",
      "Iteration 2740 || Loss: 9.5314 || 10iter: 18.7103 sec.\n",
      "Iteration 2750 || Loss: 6.5807 || 10iter: 18.7363 sec.\n",
      "Iteration 2760 || Loss: 6.3505 || 10iter: 18.7172 sec.\n",
      "Iteration 2770 || Loss: 5.6430 || 10iter: 18.7040 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:460.9734 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.5163 sec.\n",
      "-------------\n",
      "Epoch 43/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2780 || Loss: 6.9836 || 10iter: 16.7730 sec.\n",
      "Iteration 2790 || Loss: 5.7423 || 10iter: 18.7144 sec.\n",
      "Iteration 2800 || Loss: 5.3457 || 10iter: 18.7237 sec.\n",
      "Iteration 2810 || Loss: 7.0152 || 10iter: 18.7438 sec.\n",
      "Iteration 2820 || Loss: 5.5715 || 10iter: 18.6924 sec.\n",
      "Iteration 2830 || Loss: 5.8994 || 10iter: 18.7263 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:418.8472 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7315 sec.\n",
      "-------------\n",
      "Epoch 44/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2840 || Loss: 7.2442 || 10iter: 5.0962 sec.\n",
      "Iteration 2850 || Loss: 5.3020 || 10iter: 18.7817 sec.\n",
      "Iteration 2860 || Loss: 5.1826 || 10iter: 18.7021 sec.\n",
      "Iteration 2870 || Loss: 7.2189 || 10iter: 18.6812 sec.\n",
      "Iteration 2880 || Loss: 5.5030 || 10iter: 18.6915 sec.\n",
      "Iteration 2890 || Loss: 6.3612 || 10iter: 18.6393 sec.\n",
      "Iteration 2900 || Loss: 9.6203 || 10iter: 18.6897 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:430.1910 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0016 sec.\n",
      "-------------\n",
      "Epoch 45/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2910 || Loss: 6.4402 || 10iter: 13.3582 sec.\n",
      "Iteration 2920 || Loss: 7.0309 || 10iter: 18.7198 sec.\n",
      "Iteration 2930 || Loss: 5.7879 || 10iter: 18.7501 sec.\n",
      "Iteration 2940 || Loss: 6.9769 || 10iter: 18.6979 sec.\n",
      "Iteration 2950 || Loss: 5.5955 || 10iter: 18.7297 sec.\n",
      "Iteration 2960 || Loss: 6.5671 || 10iter: 18.7106 sec.\n",
      "Iteration 2970 || Loss: 6.2906 || 10iter: 18.3979 sec.\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:433.6356 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.2883 sec.\n",
      "-------------\n",
      "Epoch 46/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2980 || Loss: 5.2907 || 10iter: 20.6981 sec.\n",
      "Iteration 2990 || Loss: 7.3849 || 10iter: 18.6445 sec.\n",
      "Iteration 3000 || Loss: 8.3676 || 10iter: 18.8253 sec.\n",
      "Iteration 3010 || Loss: 5.6734 || 10iter: 18.6267 sec.\n",
      "Iteration 3020 || Loss: 6.4924 || 10iter: 18.6997 sec.\n",
      "Iteration 3030 || Loss: 5.3003 || 10iter: 18.7108 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:424.3922 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.5997 sec.\n",
      "-------------\n",
      "Epoch 47/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3040 || Loss: 5.1682 || 10iter: 9.0997 sec.\n",
      "Iteration 3050 || Loss: 5.2865 || 10iter: 18.6010 sec.\n",
      "Iteration 3060 || Loss: 6.7287 || 10iter: 18.7507 sec.\n",
      "Iteration 3070 || Loss: 5.6793 || 10iter: 18.7630 sec.\n",
      "Iteration 3080 || Loss: 6.5191 || 10iter: 18.7005 sec.\n",
      "Iteration 3090 || Loss: 5.4581 || 10iter: 18.7273 sec.\n",
      "Iteration 3100 || Loss: 7.6758 || 10iter: 18.7520 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:437.0197 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0625 sec.\n",
      "-------------\n",
      "Epoch 48/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3110 || Loss: 7.9625 || 10iter: 17.1447 sec.\n",
      "Iteration 3120 || Loss: 5.4285 || 10iter: 18.7296 sec.\n",
      "Iteration 3130 || Loss: 5.1436 || 10iter: 18.6469 sec.\n",
      "Iteration 3140 || Loss: 8.8290 || 10iter: 18.6340 sec.\n",
      "Iteration 3150 || Loss: 5.8351 || 10iter: 18.8424 sec.\n",
      "Iteration 3160 || Loss: 5.1658 || 10iter: 18.6920 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:437.4222 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0320 sec.\n",
      "-------------\n",
      "Epoch 49/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3170 || Loss: 6.9213 || 10iter: 5.0781 sec.\n",
      "Iteration 3180 || Loss: 5.5656 || 10iter: 18.6605 sec.\n",
      "Iteration 3190 || Loss: 5.4786 || 10iter: 18.7012 sec.\n",
      "Iteration 3200 || Loss: 7.4876 || 10iter: 18.8105 sec.\n",
      "Iteration 3210 || Loss: 6.1712 || 10iter: 18.7189 sec.\n",
      "Iteration 3220 || Loss: 7.5661 || 10iter: 18.7238 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3230 || Loss: 5.8698 || 10iter: 18.6964 sec.\n",
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:424.0163 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9771 sec.\n",
      "-------------\n",
      "Epoch 50/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3240 || Loss: 9.9410 || 10iter: 13.1421 sec.\n",
      "Iteration 3250 || Loss: 5.9626 || 10iter: 18.7161 sec.\n",
      "Iteration 3260 || Loss: 6.1380 || 10iter: 18.5863 sec.\n",
      "Iteration 3270 || Loss: 7.7567 || 10iter: 18.7522 sec.\n",
      "Iteration 3280 || Loss: 5.8150 || 10iter: 18.7049 sec.\n",
      "Iteration 3290 || Loss: 7.3401 || 10iter: 18.7806 sec.\n",
      "Iteration 3300 || Loss: 5.1786 || 10iter: 18.3304 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:424.7989 ||Epoch_VAL_Loss:176.5584\n",
      "timer:  158.2188 sec.\n",
      "-------------\n",
      "Epoch 51/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3310 || Loss: 6.6686 || 10iter: 21.0805 sec.\n",
      "Iteration 3320 || Loss: 5.6837 || 10iter: 18.6132 sec.\n",
      "Iteration 3330 || Loss: 7.7120 || 10iter: 18.7120 sec.\n",
      "Iteration 3340 || Loss: 5.3274 || 10iter: 18.6588 sec.\n",
      "Iteration 3350 || Loss: 5.6742 || 10iter: 18.6506 sec.\n",
      "Iteration 3360 || Loss: 6.2661 || 10iter: 18.6550 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:424.2155 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.8382 sec.\n",
      "-------------\n",
      "Epoch 52/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3370 || Loss: 5.3089 || 10iter: 9.4766 sec.\n",
      "Iteration 3380 || Loss: 6.6199 || 10iter: 18.7503 sec.\n",
      "Iteration 3390 || Loss: 5.2967 || 10iter: 18.6296 sec.\n",
      "Iteration 3400 || Loss: 6.5882 || 10iter: 18.6735 sec.\n",
      "Iteration 3410 || Loss: 6.5585 || 10iter: 18.8984 sec.\n",
      "Iteration 3420 || Loss: 5.9166 || 10iter: 18.6529 sec.\n",
      "Iteration 3430 || Loss: 5.6451 || 10iter: 18.7318 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:390.1404 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.4310 sec.\n",
      "-------------\n",
      "Epoch 53/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3440 || Loss: 5.6974 || 10iter: 17.1878 sec.\n",
      "Iteration 3450 || Loss: 5.4107 || 10iter: 18.7313 sec.\n",
      "Iteration 3460 || Loss: 6.0869 || 10iter: 18.6228 sec.\n",
      "Iteration 3470 || Loss: 6.5648 || 10iter: 18.7967 sec.\n",
      "Iteration 3480 || Loss: 6.1309 || 10iter: 18.8265 sec.\n",
      "Iteration 3490 || Loss: 5.7436 || 10iter: 18.6917 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:395.8092 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.2325 sec.\n",
      "-------------\n",
      "Epoch 54/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3500 || Loss: 5.4646 || 10iter: 5.1820 sec.\n",
      "Iteration 3510 || Loss: 6.0325 || 10iter: 18.5996 sec.\n",
      "Iteration 3520 || Loss: 5.3934 || 10iter: 18.7552 sec.\n",
      "Iteration 3530 || Loss: 5.3516 || 10iter: 18.7199 sec.\n",
      "Iteration 3540 || Loss: 5.6760 || 10iter: 18.7391 sec.\n",
      "Iteration 3550 || Loss: 5.0375 || 10iter: 18.6602 sec.\n",
      "Iteration 3560 || Loss: 5.5538 || 10iter: 18.7917 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:371.0531 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9461 sec.\n",
      "-------------\n",
      "Epoch 55/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3570 || Loss: 5.1476 || 10iter: 13.1561 sec.\n",
      "Iteration 3580 || Loss: 5.5657 || 10iter: 18.7687 sec.\n",
      "Iteration 3590 || Loss: 7.3875 || 10iter: 18.6200 sec.\n",
      "Iteration 3600 || Loss: 5.8910 || 10iter: 18.7490 sec.\n",
      "Iteration 3610 || Loss: 5.2023 || 10iter: 18.5928 sec.\n",
      "Iteration 3620 || Loss: 5.1608 || 10iter: 18.6932 sec.\n",
      "Iteration 3630 || Loss: 5.6358 || 10iter: 18.1883 sec.\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:372.6263 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.8232 sec.\n",
      "-------------\n",
      "Epoch 56/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3640 || Loss: 5.8497 || 10iter: 21.0902 sec.\n",
      "Iteration 3650 || Loss: 5.2620 || 10iter: 18.6397 sec.\n",
      "Iteration 3660 || Loss: 6.3258 || 10iter: 18.7543 sec.\n",
      "Iteration 3670 || Loss: 5.1824 || 10iter: 18.6799 sec.\n",
      "Iteration 3680 || Loss: 5.1536 || 10iter: 18.7508 sec.\n",
      "Iteration 3690 || Loss: 5.8398 || 10iter: 18.7853 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:365.4855 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0801 sec.\n",
      "-------------\n",
      "Epoch 57/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3700 || Loss: 5.2439 || 10iter: 9.0366 sec.\n",
      "Iteration 3710 || Loss: 6.0316 || 10iter: 18.6810 sec.\n",
      "Iteration 3720 || Loss: 5.3528 || 10iter: 18.6218 sec.\n",
      "Iteration 3730 || Loss: 5.2666 || 10iter: 18.7265 sec.\n",
      "Iteration 3740 || Loss: 5.2471 || 10iter: 18.8182 sec.\n",
      "Iteration 3750 || Loss: 5.8335 || 10iter: 18.6700 sec.\n",
      "Iteration 3760 || Loss: 5.2631 || 10iter: 18.7234 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:365.5746 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9510 sec.\n",
      "-------------\n",
      "Epoch 58/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3770 || Loss: 5.5288 || 10iter: 17.4152 sec.\n",
      "Iteration 3780 || Loss: 5.4724 || 10iter: 18.5526 sec.\n",
      "Iteration 3790 || Loss: 5.3916 || 10iter: 18.7012 sec.\n",
      "Iteration 3800 || Loss: 5.4934 || 10iter: 18.7612 sec.\n",
      "Iteration 3810 || Loss: 5.3661 || 10iter: 18.7214 sec.\n",
      "Iteration 3820 || Loss: 5.4325 || 10iter: 18.7619 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:359.7990 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.2096 sec.\n",
      "-------------\n",
      "Epoch 59/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3830 || Loss: 5.3226 || 10iter: 4.9703 sec.\n",
      "Iteration 3840 || Loss: 5.3786 || 10iter: 18.6662 sec.\n",
      "Iteration 3850 || Loss: 5.2081 || 10iter: 18.6909 sec.\n",
      "Iteration 3860 || Loss: 5.4845 || 10iter: 18.7177 sec.\n",
      "Iteration 3870 || Loss: 5.2736 || 10iter: 18.6283 sec.\n",
      "Iteration 3880 || Loss: 5.5136 || 10iter: 18.7897 sec.\n",
      "Iteration 3890 || Loss: 5.6469 || 10iter: 18.6041 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:355.4034 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7730 sec.\n",
      "-------------\n",
      "Epoch 60/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3900 || Loss: 5.2412 || 10iter: 13.2284 sec.\n",
      "Iteration 3910 || Loss: 5.4259 || 10iter: 18.8139 sec.\n",
      "Iteration 3920 || Loss: 6.1817 || 10iter: 18.6608 sec.\n",
      "Iteration 3930 || Loss: 5.2825 || 10iter: 18.6778 sec.\n",
      "Iteration 3940 || Loss: 5.1683 || 10iter: 18.7400 sec.\n",
      "Iteration 3950 || Loss: 5.1740 || 10iter: 18.6884 sec.\n",
      "Iteration 3960 || Loss: 5.2738 || 10iter: 18.3521 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:358.6169 ||Epoch_VAL_Loss:172.4901\n",
      "timer:  157.9223 sec.\n",
      "-------------\n",
      "Epoch 61/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3970 || Loss: 5.2112 || 10iter: 20.8878 sec.\n",
      "Iteration 3980 || Loss: 5.5018 || 10iter: 18.6284 sec.\n",
      "Iteration 3990 || Loss: 5.4429 || 10iter: 18.7902 sec.\n",
      "Iteration 4000 || Loss: 5.0890 || 10iter: 18.7020 sec.\n",
      "Iteration 4010 || Loss: 5.0697 || 10iter: 18.6198 sec.\n",
      "Iteration 4020 || Loss: 5.2850 || 10iter: 18.6473 sec.\n",
      "-------------\n",
      "epoch 61 || Epoch_TRAIN_Loss:348.6624 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.4956 sec.\n",
      "-------------\n",
      "Epoch 62/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4030 || Loss: 5.3046 || 10iter: 9.0504 sec.\n",
      "Iteration 4040 || Loss: 5.3625 || 10iter: 18.6133 sec.\n",
      "Iteration 4050 || Loss: 5.2422 || 10iter: 18.7409 sec.\n",
      "Iteration 4060 || Loss: 5.1567 || 10iter: 18.6637 sec.\n",
      "Iteration 4070 || Loss: 4.8822 || 10iter: 18.6441 sec.\n",
      "Iteration 4080 || Loss: 4.9569 || 10iter: 18.7303 sec.\n",
      "Iteration 4090 || Loss: 5.2052 || 10iter: 18.6407 sec.\n",
      "-------------\n",
      "epoch 62 || Epoch_TRAIN_Loss:348.5587 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7656 sec.\n",
      "-------------\n",
      "Epoch 63/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4100 || Loss: 5.8895 || 10iter: 16.7895 sec.\n",
      "Iteration 4110 || Loss: 4.9322 || 10iter: 18.7818 sec.\n",
      "Iteration 4120 || Loss: 5.0802 || 10iter: 18.6722 sec.\n",
      "Iteration 4130 || Loss: 5.5606 || 10iter: 18.6331 sec.\n",
      "Iteration 4140 || Loss: 5.2248 || 10iter: 18.6593 sec.\n",
      "Iteration 4150 || Loss: 5.2587 || 10iter: 18.6056 sec.\n",
      "-------------\n",
      "epoch 63 || Epoch_TRAIN_Loss:343.7561 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.4911 sec.\n",
      "-------------\n",
      "Epoch 64/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4160 || Loss: 4.9917 || 10iter: 5.3766 sec.\n",
      "Iteration 4170 || Loss: 5.4831 || 10iter: 18.6667 sec.\n",
      "Iteration 4180 || Loss: 5.2778 || 10iter: 18.5779 sec.\n",
      "Iteration 4190 || Loss: 4.8500 || 10iter: 18.6668 sec.\n",
      "Iteration 4200 || Loss: 5.0207 || 10iter: 18.6676 sec.\n",
      "Iteration 4210 || Loss: 5.0844 || 10iter: 18.6576 sec.\n",
      "Iteration 4220 || Loss: 4.9907 || 10iter: 18.6788 sec.\n",
      "-------------\n",
      "epoch 64 || Epoch_TRAIN_Loss:340.4270 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9423 sec.\n",
      "-------------\n",
      "Epoch 65/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4230 || Loss: 5.0607 || 10iter: 13.2472 sec.\n",
      "Iteration 4240 || Loss: 5.1209 || 10iter: 18.6210 sec.\n",
      "Iteration 4250 || Loss: 5.1409 || 10iter: 18.6737 sec.\n",
      "Iteration 4260 || Loss: 5.2053 || 10iter: 18.7175 sec.\n",
      "Iteration 4270 || Loss: 5.2307 || 10iter: 18.6942 sec.\n",
      "Iteration 4280 || Loss: 5.0716 || 10iter: 18.6818 sec.\n",
      "Iteration 4290 || Loss: 5.1283 || 10iter: 18.3631 sec.\n",
      "-------------\n",
      "epoch 65 || Epoch_TRAIN_Loss:338.6622 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9705 sec.\n",
      "-------------\n",
      "Epoch 66/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4300 || Loss: 5.0241 || 10iter: 21.2768 sec.\n",
      "Iteration 4310 || Loss: 5.1689 || 10iter: 18.5140 sec.\n",
      "Iteration 4320 || Loss: 4.9361 || 10iter: 18.7382 sec.\n",
      "Iteration 4330 || Loss: 5.2135 || 10iter: 18.6736 sec.\n",
      "Iteration 4340 || Loss: 5.0941 || 10iter: 18.5796 sec.\n",
      "Iteration 4350 || Loss: 5.1156 || 10iter: 18.6668 sec.\n",
      "-------------\n",
      "epoch 66 || Epoch_TRAIN_Loss:336.2827 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7980 sec.\n",
      "-------------\n",
      "Epoch 67/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4360 || Loss: 5.1832 || 10iter: 8.9679 sec.\n",
      "Iteration 4370 || Loss: 5.1663 || 10iter: 18.7033 sec.\n",
      "Iteration 4380 || Loss: 5.1473 || 10iter: 18.6529 sec.\n",
      "Iteration 4390 || Loss: 4.7913 || 10iter: 18.6021 sec.\n",
      "Iteration 4400 || Loss: 5.0719 || 10iter: 18.6819 sec.\n",
      "Iteration 4410 || Loss: 4.8873 || 10iter: 18.7559 sec.\n",
      "Iteration 4420 || Loss: 5.4415 || 10iter: 18.6291 sec.\n",
      "-------------\n",
      "epoch 67 || Epoch_TRAIN_Loss:335.3632 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.5303 sec.\n",
      "-------------\n",
      "Epoch 68/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4430 || Loss: 4.7083 || 10iter: 17.2158 sec.\n",
      "Iteration 4440 || Loss: 5.0132 || 10iter: 18.6307 sec.\n",
      "Iteration 4450 || Loss: 5.2872 || 10iter: 18.5169 sec.\n",
      "Iteration 4460 || Loss: 4.9768 || 10iter: 18.6260 sec.\n",
      "Iteration 4470 || Loss: 5.0761 || 10iter: 18.7143 sec.\n",
      "Iteration 4480 || Loss: 5.2817 || 10iter: 18.6802 sec.\n",
      "-------------\n",
      "epoch 68 || Epoch_TRAIN_Loss:334.0070 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7994 sec.\n",
      "-------------\n",
      "Epoch 69/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4490 || Loss: 4.9468 || 10iter: 5.5447 sec.\n",
      "Iteration 4500 || Loss: 5.1008 || 10iter: 18.6138 sec.\n",
      "Iteration 4510 || Loss: 5.1165 || 10iter: 18.5756 sec.\n",
      "Iteration 4520 || Loss: 4.9982 || 10iter: 18.6740 sec.\n",
      "Iteration 4530 || Loss: 4.9483 || 10iter: 18.6914 sec.\n",
      "Iteration 4540 || Loss: 4.8807 || 10iter: 18.6333 sec.\n",
      "Iteration 4550 || Loss: 4.9461 || 10iter: 18.6847 sec.\n",
      "-------------\n",
      "epoch 69 || Epoch_TRAIN_Loss:333.2761 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  133.0670 sec.\n",
      "-------------\n",
      "Epoch 70/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4560 || Loss: 5.1936 || 10iter: 13.1456 sec.\n",
      "Iteration 4570 || Loss: 4.9622 || 10iter: 18.5450 sec.\n",
      "Iteration 4580 || Loss: 4.8837 || 10iter: 18.5512 sec.\n",
      "Iteration 4590 || Loss: 4.9902 || 10iter: 18.6032 sec.\n",
      "Iteration 4600 || Loss: 4.9435 || 10iter: 18.6396 sec.\n",
      "Iteration 4610 || Loss: 5.0334 || 10iter: 18.6145 sec.\n",
      "Iteration 4620 || Loss: 4.9554 || 10iter: 18.3378 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 70 || Epoch_TRAIN_Loss:329.7684 ||Epoch_VAL_Loss:163.4648\n",
      "timer:  157.5130 sec.\n",
      "-------------\n",
      "Epoch 71/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4630 || Loss: 5.1528 || 10iter: 20.9175 sec.\n",
      "Iteration 4640 || Loss: 4.9940 || 10iter: 18.5028 sec.\n",
      "Iteration 4650 || Loss: 5.1590 || 10iter: 18.5798 sec.\n",
      "Iteration 4660 || Loss: 5.1784 || 10iter: 18.5965 sec.\n",
      "Iteration 4670 || Loss: 4.8817 || 10iter: 18.6280 sec.\n",
      "Iteration 4680 || Loss: 4.8111 || 10iter: 18.6237 sec.\n",
      "-------------\n",
      "epoch 71 || Epoch_TRAIN_Loss:331.7530 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.1742 sec.\n",
      "-------------\n",
      "Epoch 72/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4690 || Loss: 4.9994 || 10iter: 8.9426 sec.\n",
      "Iteration 4700 || Loss: 4.9734 || 10iter: 18.5563 sec.\n",
      "Iteration 4710 || Loss: 4.8842 || 10iter: 18.5789 sec.\n",
      "Iteration 4720 || Loss: 5.1430 || 10iter: 18.7949 sec.\n",
      "Iteration 4730 || Loss: 5.1212 || 10iter: 18.6662 sec.\n",
      "Iteration 4740 || Loss: 4.8506 || 10iter: 18.6889 sec.\n",
      "Iteration 4750 || Loss: 4.9654 || 10iter: 18.5933 sec.\n",
      "-------------\n",
      "epoch 72 || Epoch_TRAIN_Loss:328.0642 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.3234 sec.\n",
      "-------------\n",
      "Epoch 73/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4760 || Loss: 4.9712 || 10iter: 16.9464 sec.\n",
      "Iteration 4770 || Loss: 5.0711 || 10iter: 18.6647 sec.\n",
      "Iteration 4780 || Loss: 4.8167 || 10iter: 18.6055 sec.\n",
      "Iteration 4790 || Loss: 4.8599 || 10iter: 18.6337 sec.\n",
      "Iteration 4800 || Loss: 5.2202 || 10iter: 18.6496 sec.\n",
      "Iteration 4810 || Loss: 5.2313 || 10iter: 18.5793 sec.\n",
      "-------------\n",
      "epoch 73 || Epoch_TRAIN_Loss:329.3183 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.3939 sec.\n",
      "-------------\n",
      "Epoch 74/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4820 || Loss: 5.2280 || 10iter: 5.3551 sec.\n",
      "Iteration 4830 || Loss: 4.7761 || 10iter: 18.6245 sec.\n",
      "Iteration 4840 || Loss: 4.9035 || 10iter: 18.6424 sec.\n",
      "Iteration 4850 || Loss: 4.8946 || 10iter: 18.6923 sec.\n",
      "Iteration 4860 || Loss: 4.9578 || 10iter: 18.7170 sec.\n",
      "Iteration 4870 || Loss: 4.9354 || 10iter: 18.6609 sec.\n",
      "Iteration 4880 || Loss: 4.9719 || 10iter: 18.5549 sec.\n",
      "-------------\n",
      "epoch 74 || Epoch_TRAIN_Loss:327.6383 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7974 sec.\n",
      "-------------\n",
      "Epoch 75/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4890 || Loss: 4.9697 || 10iter: 13.1678 sec.\n",
      "Iteration 4900 || Loss: 4.9047 || 10iter: 18.6162 sec.\n",
      "Iteration 4910 || Loss: 4.6060 || 10iter: 18.6524 sec.\n",
      "Iteration 4920 || Loss: 5.1808 || 10iter: 18.6246 sec.\n",
      "Iteration 4930 || Loss: 4.8774 || 10iter: 18.5757 sec.\n",
      "Iteration 4940 || Loss: 5.2448 || 10iter: 18.5803 sec.\n",
      "Iteration 4950 || Loss: 4.5744 || 10iter: 18.2647 sec.\n",
      "-------------\n",
      "epoch 75 || Epoch_TRAIN_Loss:327.8357 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.4822 sec.\n",
      "-------------\n",
      "Epoch 76/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4960 || Loss: 5.1200 || 10iter: 21.0948 sec.\n",
      "Iteration 4970 || Loss: 5.1183 || 10iter: 18.5811 sec.\n",
      "Iteration 4980 || Loss: 4.8323 || 10iter: 18.5864 sec.\n",
      "Iteration 4990 || Loss: 4.8179 || 10iter: 18.7828 sec.\n",
      "Iteration 5000 || Loss: 5.0067 || 10iter: 18.7181 sec.\n",
      "Iteration 5010 || Loss: 4.9270 || 10iter: 18.7058 sec.\n",
      "-------------\n",
      "epoch 76 || Epoch_TRAIN_Loss:325.8633 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7100 sec.\n",
      "-------------\n",
      "Epoch 77/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5020 || Loss: 4.8564 || 10iter: 9.4343 sec.\n",
      "Iteration 5030 || Loss: 5.1176 || 10iter: 18.6578 sec.\n",
      "Iteration 5040 || Loss: 4.9007 || 10iter: 18.5828 sec.\n",
      "Iteration 5050 || Loss: 5.0811 || 10iter: 18.6339 sec.\n",
      "Iteration 5060 || Loss: 4.9431 || 10iter: 18.5911 sec.\n",
      "Iteration 5070 || Loss: 5.4774 || 10iter: 18.5811 sec.\n",
      "Iteration 5080 || Loss: 4.7696 || 10iter: 18.7232 sec.\n",
      "-------------\n",
      "epoch 77 || Epoch_TRAIN_Loss:326.8709 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.9492 sec.\n",
      "-------------\n",
      "Epoch 78/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5090 || Loss: 5.2655 || 10iter: 16.9730 sec.\n",
      "Iteration 5100 || Loss: 5.4728 || 10iter: 18.6973 sec.\n",
      "Iteration 5110 || Loss: 4.9564 || 10iter: 18.8012 sec.\n",
      "Iteration 5120 || Loss: 4.8084 || 10iter: 18.7965 sec.\n",
      "Iteration 5130 || Loss: 4.9414 || 10iter: 18.6250 sec.\n",
      "Iteration 5140 || Loss: 4.7108 || 10iter: 18.6461 sec.\n",
      "-------------\n",
      "epoch 78 || Epoch_TRAIN_Loss:326.8276 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.7787 sec.\n",
      "-------------\n",
      "Epoch 79/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5150 || Loss: 4.7601 || 10iter: 5.0823 sec.\n",
      "Iteration 5160 || Loss: 5.1120 || 10iter: 18.6153 sec.\n",
      "Iteration 5170 || Loss: 4.6772 || 10iter: 18.6274 sec.\n",
      "Iteration 5180 || Loss: 5.0101 || 10iter: 18.7607 sec.\n",
      "Iteration 5190 || Loss: 4.9280 || 10iter: 18.5908 sec.\n",
      "Iteration 5200 || Loss: 5.1546 || 10iter: 18.6767 sec.\n",
      "Iteration 5210 || Loss: 5.1406 || 10iter: 18.6059 sec.\n",
      "-------------\n",
      "epoch 79 || Epoch_TRAIN_Loss:327.6091 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  132.5912 sec.\n",
      "-------------\n",
      "Epoch 80/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5220 || Loss: 4.9103 || 10iter: 12.7941 sec.\n",
      "Iteration 5230 || Loss: 5.1394 || 10iter: 18.5817 sec.\n",
      "Iteration 5240 || Loss: 5.0042 || 10iter: 18.5900 sec.\n",
      "Iteration 5250 || Loss: 5.1825 || 10iter: 18.5692 sec.\n",
      "Iteration 5260 || Loss: 4.9479 || 10iter: 18.6572 sec.\n",
      "Iteration 5270 || Loss: 5.1133 || 10iter: 18.5576 sec.\n",
      "Iteration 5280 || Loss: 5.0177 || 10iter: 18.2645 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 80 || Epoch_TRAIN_Loss:326.6823 ||Epoch_VAL_Loss:162.7770\n",
      "timer:  156.9515 sec.\n",
      "-------------\n",
      "Epoch 81/200\n",
      "-------------\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
