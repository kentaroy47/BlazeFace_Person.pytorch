{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000009\n",
      "000017\n",
      "000021\n",
      "000023\n",
      "000030\n",
      "000032\n",
      "000035\n",
      "000041\n",
      "000048\n",
      "000050\n",
      "000051\n",
      "000066\n",
      "000073\n",
      "000081\n",
      "000083\n",
      "000089\n",
      "000101\n",
      "000104\n",
      "000110\n",
      "000113\n",
      "000125\n",
      "000129\n",
      "000131\n",
      "000133\n",
      "000138\n",
      "000146\n",
      "000150\n",
      "000159\n",
      "000162\n",
      "000163\n",
      "000164\n",
      "000165\n",
      "000169\n",
      "000170\n",
      "000171\n",
      "000173\n",
      "000174\n",
      "000177\n",
      "000190\n",
      "000192\n",
      "000193\n",
      "000194\n",
      "000200\n",
      "000210\n",
      "000218\n",
      "000220\n",
      "000222\n",
      "000229\n",
      "000232\n",
      "000245\n",
      "000251\n",
      "000257\n",
      "000259\n",
      "000269\n",
      "000275\n",
      "000276\n",
      "000278\n",
      "000282\n",
      "000285\n",
      "000288\n",
      "000298\n",
      "000302\n",
      "000305\n",
      "000308\n",
      "000320\n",
      "000321\n",
      "000322\n",
      "000323\n",
      "000328\n",
      "000331\n",
      "000337\n",
      "000338\n",
      "000352\n",
      "000359\n",
      "000367\n",
      "000372\n",
      "000374\n",
      "000382\n",
      "000394\n",
      "000406\n",
      "000407\n",
      "000411\n",
      "000419\n",
      "000428\n",
      "000433\n",
      "000435\n",
      "000438\n",
      "000443\n",
      "000446\n",
      "000448\n",
      "000463\n",
      "000468\n",
      "000470\n",
      "000476\n",
      "000477\n",
      "000480\n",
      "000482\n",
      "000483\n",
      "000498\n",
      "000499\n",
      "000500\n",
      "000515\n",
      "000516\n",
      "000518\n",
      "000520\n",
      "000523\n",
      "000524\n",
      "000525\n",
      "000526\n",
      "000530\n",
      "000531\n",
      "000535\n",
      "000541\n",
      "000545\n",
      "000554\n",
      "000555\n",
      "000579\n",
      "000583\n",
      "000589\n",
      "000591\n",
      "000597\n",
      "000612\n",
      "000613\n",
      "000625\n",
      "000626\n",
      "000628\n",
      "000633\n",
      "000648\n",
      "000654\n",
      "000677\n",
      "000684\n",
      "000688\n",
      "000690\n",
      "000694\n",
      "000695\n",
      "000702\n",
      "000709\n",
      "000717\n",
      "000726\n",
      "000731\n",
      "000733\n",
      "000739\n",
      "000742\n",
      "000750\n",
      "000752\n",
      "000753\n",
      "000755\n",
      "000760\n",
      "000770\n",
      "000777\n",
      "000782\n",
      "000786\n",
      "000793\n",
      "000797\n",
      "000799\n",
      "000802\n",
      "000805\n",
      "000806\n",
      "000810\n",
      "000812\n",
      "000814\n",
      "000816\n",
      "000828\n",
      "000829\n",
      "000834\n",
      "000842\n",
      "000843\n",
      "000847\n",
      "000848\n",
      "000851\n",
      "000854\n",
      "000855\n",
      "000859\n",
      "000860\n",
      "000862\n",
      "000865\n",
      "000874\n",
      "000878\n",
      "000879\n",
      "000885\n",
      "000892\n",
      "000895\n",
      "000898\n",
      "000902\n",
      "000903\n",
      "000904\n",
      "000906\n",
      "000911\n",
      "000915\n",
      "000918\n",
      "000920\n",
      "000926\n",
      "000937\n",
      "000943\n",
      "000948\n",
      "000949\n",
      "000966\n",
      "000967\n",
      "000971\n",
      "000982\n",
      "000987\n",
      "000991\n",
      "000996\n",
      "000999\n",
      "001001\n",
      "001011\n",
      "001014\n",
      "001017\n",
      "001024\n",
      "001028\n",
      "001036\n",
      "001042\n",
      "001050\n",
      "001057\n",
      "001060\n",
      "001061\n",
      "001064\n",
      "001066\n",
      "001071\n",
      "001072\n",
      "001079\n",
      "001084\n",
      "001091\n",
      "001092\n",
      "001097\n",
      "001101\n",
      "001109\n",
      "001113\n",
      "001125\n",
      "001129\n",
      "001137\n",
      "001140\n",
      "001145\n",
      "001147\n",
      "001151\n",
      "001152\n",
      "001164\n",
      "001168\n",
      "001170\n",
      "001171\n",
      "001175\n",
      "001185\n",
      "001186\n",
      "001201\n",
      "001206\n",
      "001212\n",
      "001221\n",
      "001224\n",
      "001229\n",
      "001234\n",
      "001236\n",
      "001240\n",
      "001241\n",
      "001248\n",
      "001254\n",
      "001259\n",
      "001265\n",
      "001266\n",
      "001269\n",
      "001272\n",
      "001279\n",
      "001281\n",
      "001284\n",
      "001287\n",
      "001292\n",
      "001298\n",
      "001304\n",
      "001309\n",
      "001310\n",
      "001311\n",
      "001315\n",
      "001325\n",
      "001327\n",
      "001330\n",
      "001333\n",
      "001337\n",
      "001346\n",
      "001350\n",
      "001352\n",
      "001362\n",
      "001378\n",
      "001388\n",
      "001390\n",
      "001393\n",
      "001405\n",
      "001406\n",
      "001408\n",
      "001409\n",
      "001414\n",
      "001420\n",
      "001421\n",
      "001426\n",
      "001427\n",
      "001430\n",
      "001434\n",
      "001445\n",
      "001450\n",
      "001451\n",
      "001455\n",
      "001460\n",
      "001463\n",
      "001472\n",
      "001475\n",
      "001479\n",
      "001480\n",
      "001484\n",
      "001485\n",
      "001493\n",
      "001498\n",
      "001499\n",
      "001501\n",
      "001504\n",
      "001509\n",
      "001510\n",
      "001514\n",
      "001521\n",
      "001523\n",
      "001524\n",
      "001526\n",
      "001531\n",
      "001532\n",
      "001536\n",
      "001537\n",
      "001544\n",
      "001548\n",
      "001554\n",
      "001557\n",
      "001561\n",
      "001563\n",
      "001571\n",
      "001577\n",
      "001579\n",
      "001580\n",
      "001586\n",
      "001593\n",
      "001594\n",
      "001603\n",
      "001608\n",
      "001611\n",
      "001617\n",
      "001622\n",
      "001627\n",
      "001628\n",
      "001630\n",
      "001640\n",
      "001649\n",
      "001651\n",
      "001654\n",
      "001678\n",
      "001682\n",
      "001686\n",
      "001689\n",
      "001690\n",
      "001691\n",
      "001711\n",
      "001717\n",
      "001721\n",
      "001723\n",
      "001725\n",
      "001726\n",
      "001727\n",
      "001729\n",
      "001730\n",
      "001749\n",
      "001754\n",
      "001756\n",
      "001758\n",
      "001759\n",
      "001766\n",
      "001772\n",
      "001775\n",
      "001784\n",
      "001785\n",
      "001789\n",
      "001793\n",
      "001795\n",
      "001797\n",
      "001799\n",
      "001800\n",
      "001801\n",
      "001806\n",
      "001807\n",
      "001809\n",
      "001810\n",
      "001828\n",
      "001830\n",
      "001832\n",
      "001833\n",
      "001836\n",
      "001837\n",
      "001841\n",
      "001843\n",
      "001849\n",
      "001854\n",
      "001861\n",
      "001864\n",
      "001881\n",
      "001887\n",
      "001894\n",
      "001899\n",
      "001902\n",
      "001903\n",
      "001904\n",
      "001918\n",
      "001920\n",
      "001927\n",
      "001930\n",
      "001932\n",
      "001938\n",
      "001941\n",
      "001944\n",
      "001945\n",
      "001950\n",
      "001958\n",
      "001963\n",
      "001977\n",
      "001978\n",
      "001980\n",
      "001989\n",
      "001999\n",
      "002000\n",
      "002012\n",
      "002022\n",
      "002024\n",
      "002030\n",
      "002051\n",
      "002055\n",
      "002061\n",
      "002069\n",
      "002082\n",
      "002090\n",
      "002091\n",
      "002096\n",
      "002104\n",
      "002108\n",
      "002114\n",
      "002116\n",
      "002117\n",
      "002120\n",
      "002142\n",
      "002145\n",
      "002156\n",
      "002163\n",
      "002169\n",
      "002170\n",
      "002171\n",
      "002172\n",
      "002174\n",
      "002176\n",
      "002180\n",
      "002183\n",
      "002186\n",
      "002187\n",
      "002190\n",
      "002193\n",
      "002194\n",
      "002213\n",
      "002214\n",
      "002219\n",
      "002220\n",
      "002221\n",
      "002224\n",
      "002226\n",
      "002233\n",
      "002238\n",
      "002241\n",
      "002244\n",
      "002256\n",
      "002261\n",
      "002265\n",
      "002272\n",
      "002273\n",
      "002277\n",
      "002281\n",
      "002285\n",
      "002288\n",
      "002290\n",
      "002293\n",
      "002302\n",
      "002306\n",
      "002307\n",
      "002311\n",
      "002324\n",
      "002329\n",
      "002332\n",
      "002337\n",
      "002342\n",
      "002361\n",
      "002362\n",
      "002367\n",
      "002372\n",
      "002378\n",
      "002382\n",
      "002384\n",
      "002385\n",
      "002387\n",
      "002401\n",
      "002403\n",
      "002405\n",
      "002410\n",
      "002413\n",
      "002419\n",
      "002425\n",
      "002433\n",
      "002443\n",
      "002444\n",
      "002448\n",
      "002450\n",
      "002458\n",
      "002459\n",
      "002460\n",
      "002472\n",
      "002476\n",
      "002477\n",
      "002481\n",
      "002483\n",
      "002491\n",
      "002497\n",
      "002501\n",
      "002504\n",
      "002514\n",
      "002518\n",
      "002520\n",
      "002524\n",
      "002529\n",
      "002533\n",
      "002539\n",
      "002540\n",
      "002542\n",
      "002545\n",
      "002554\n",
      "002555\n",
      "002558\n",
      "002561\n",
      "002563\n",
      "002564\n",
      "002565\n",
      "002566\n",
      "002572\n",
      "002584\n",
      "002585\n",
      "002589\n",
      "002595\n",
      "002611\n",
      "002613\n",
      "002625\n",
      "002632\n",
      "002633\n",
      "002636\n",
      "002637\n",
      "002641\n",
      "002643\n",
      "002649\n",
      "002657\n",
      "002658\n",
      "002662\n",
      "002668\n",
      "002669\n",
      "002675\n",
      "002678\n",
      "002689\n",
      "002691\n",
      "002693\n",
      "002696\n",
      "002710\n",
      "002713\n",
      "002714\n",
      "002715\n",
      "002723\n",
      "002727\n",
      "002730\n",
      "002735\n",
      "002741\n",
      "002744\n",
      "002751\n",
      "002755\n",
      "002759\n",
      "002767\n",
      "002776\n",
      "002786\n",
      "002795\n",
      "002798\n",
      "002803\n",
      "002804\n",
      "002807\n",
      "002815\n",
      "002826\n",
      "002827\n",
      "002834\n",
      "002838\n",
      "002841\n",
      "002842\n",
      "002855\n",
      "002858\n",
      "002859\n",
      "002864\n",
      "002867\n",
      "002870\n",
      "002879\n",
      "002881\n",
      "002884\n",
      "002886\n",
      "002889\n",
      "002899\n",
      "002901\n",
      "002912\n",
      "002913\n",
      "002914\n",
      "002915\n",
      "002916\n",
      "002931\n",
      "002932\n",
      "002933\n",
      "002935\n",
      "002939\n",
      "002940\n",
      "002954\n",
      "002958\n",
      "002960\n",
      "002962\n",
      "002965\n",
      "002975\n",
      "002976\n",
      "002987\n",
      "002988\n",
      "002990\n",
      "002992\n",
      "003002\n",
      "003003\n",
      "003017\n",
      "003023\n",
      "003024\n",
      "003027\n",
      "003031\n",
      "003034\n",
      "003039\n",
      "003042\n",
      "003044\n",
      "003057\n",
      "003058\n",
      "003063\n",
      "003064\n",
      "003065\n",
      "003077\n",
      "003078\n",
      "003086\n",
      "003088\n",
      "003089\n",
      "003100\n",
      "003102\n",
      "003106\n",
      "003107\n",
      "003118\n",
      "003121\n",
      "003122\n",
      "003124\n",
      "003129\n",
      "003134\n",
      "003138\n",
      "003140\n",
      "003145\n",
      "003147\n",
      "003150\n",
      "003159\n",
      "003162\n",
      "003170\n",
      "003175\n",
      "003181\n",
      "003185\n",
      "003188\n",
      "003189\n",
      "003205\n",
      "003210\n",
      "003213\n",
      "003218\n",
      "003228\n",
      "003236\n",
      "003247\n",
      "003253\n",
      "003259\n",
      "003260\n",
      "003261\n",
      "003270\n",
      "003280\n",
      "003282\n",
      "003284\n",
      "003290\n",
      "003292\n",
      "003296\n",
      "003301\n",
      "003303\n",
      "003311\n",
      "003313\n",
      "003320\n",
      "003327\n",
      "003331\n",
      "003335\n",
      "003344\n",
      "003350\n",
      "003355\n",
      "003360\n",
      "003362\n",
      "003367\n",
      "003369\n",
      "003373\n",
      "003376\n",
      "003380\n",
      "003382\n",
      "003386\n",
      "003390\n",
      "003391\n",
      "003395\n",
      "003396\n",
      "003397\n",
      "003403\n",
      "003406\n",
      "003407\n",
      "003412\n",
      "003416\n",
      "003417\n",
      "003422\n",
      "003424\n",
      "003429\n",
      "003433\n",
      "003435\n",
      "003436\n",
      "003450\n",
      "003453\n",
      "003455\n",
      "003462\n",
      "003465\n",
      "003466\n",
      "003468\n",
      "003469\n",
      "003470\n",
      "003484\n",
      "003487\n",
      "003492\n",
      "003496\n",
      "003510\n",
      "003518\n",
      "003521\n",
      "003528\n",
      "003529\n",
      "003530\n",
      "003536\n",
      "003539\n",
      "003548\n",
      "003550\n",
      "003556\n",
      "003566\n",
      "003567\n",
      "003575\n",
      "003576\n",
      "003577\n",
      "003580\n",
      "003585\n",
      "003589\n",
      "003603\n",
      "003611\n",
      "003618\n",
      "003621\n",
      "003625\n",
      "003628\n",
      "003638\n",
      "003644\n",
      "003645\n",
      "003648\n",
      "003651\n",
      "003654\n",
      "003655\n",
      "003662\n",
      "003663\n",
      "003664\n",
      "003669\n",
      "003678\n",
      "003685\n",
      "003688\n",
      "003690\n",
      "003691\n",
      "003694\n",
      "003696\n",
      "003699\n",
      "003700\n",
      "003703\n",
      "003706\n",
      "003708\n",
      "003711\n",
      "003717\n",
      "003732\n",
      "003735\n",
      "003740\n",
      "003749\n",
      "003751\n",
      "003759\n",
      "003760\n",
      "003779\n",
      "003780\n",
      "003781\n",
      "003797\n",
      "003811\n",
      "003817\n",
      "003820\n",
      "003824\n",
      "003826\n",
      "003827\n",
      "003838\n",
      "003847\n",
      "003849\n",
      "003855\n",
      "003856\n",
      "003859\n",
      "003860\n",
      "003863\n",
      "003865\n",
      "003868\n",
      "003869\n",
      "003877\n",
      "003879\n",
      "003889\n",
      "003891\n",
      "003899\n",
      "003913\n",
      "003915\n",
      "003923\n",
      "003932\n",
      "003939\n",
      "003945\n",
      "003946\n",
      "003948\n",
      "003953\n",
      "003960\n",
      "003961\n",
      "003963\n",
      "003966\n",
      "003970\n",
      "003973\n",
      "003974\n",
      "003986\n",
      "003992\n",
      "003994\n",
      "003996\n",
      "004008\n",
      "004010\n",
      "004012\n",
      "004014\n",
      "004016\n",
      "004017\n",
      "004019\n",
      "004020\n",
      "004028\n",
      "004031\n",
      "004033\n",
      "004039\n",
      "004046\n",
      "004047\n",
      "004051\n",
      "004058\n",
      "004060\n",
      "004067\n",
      "004073\n",
      "004075\n",
      "004077\n",
      "004085\n",
      "004089\n",
      "004091\n",
      "004092\n",
      "004110\n",
      "004111\n",
      "004117\n",
      "004121\n",
      "004122\n",
      "004133\n",
      "004136\n",
      "004137\n",
      "004138\n",
      "004141\n",
      "004142\n",
      "004145\n",
      "004158\n",
      "004168\n",
      "004169\n",
      "004170\n",
      "004191\n",
      "004195\n",
      "004201\n",
      "004205\n",
      "004209\n",
      "004223\n",
      "004224\n",
      "004229\n",
      "004230\n",
      "004231\n",
      "004232\n",
      "004237\n",
      "004244\n",
      "004246\n",
      "004257\n",
      "004258\n",
      "004259\n",
      "004265\n",
      "004269\n",
      "004270\n",
      "004273\n",
      "004275\n",
      "004279\n",
      "004281\n",
      "004287\n",
      "004291\n",
      "004292\n",
      "004296\n",
      "004298\n",
      "004304\n",
      "004321\n",
      "004322\n",
      "004323\n",
      "004327\n",
      "004329\n",
      "004339\n",
      "004346\n",
      "004356\n",
      "004359\n",
      "004360\n",
      "004361\n",
      "004370\n",
      "004372\n",
      "004380\n",
      "004384\n",
      "004387\n",
      "004389\n",
      "004392\n",
      "004397\n",
      "004404\n",
      "004424\n",
      "004433\n",
      "004437\n",
      "004438\n",
      "004457\n",
      "004459\n",
      "004463\n",
      "004464\n",
      "004466\n",
      "004470\n",
      "004471\n",
      "004479\n",
      "004481\n",
      "004487\n",
      "004494\n",
      "004495\n",
      "004496\n",
      "004499\n",
      "004508\n",
      "004509\n",
      "004512\n",
      "004517\n",
      "004518\n",
      "004520\n",
      "004528\n",
      "004530\n",
      "004535\n",
      "004537\n",
      "004542\n",
      "004553\n",
      "004562\n",
      "004563\n",
      "004571\n",
      "004574\n",
      "004588\n",
      "004595\n",
      "004601\n",
      "004604\n",
      "004605\n",
      "004607\n",
      "004609\n",
      "004611\n",
      "004622\n",
      "004630\n",
      "004631\n",
      "004647\n",
      "004648\n",
      "004653\n",
      "004656\n",
      "004660\n",
      "004662\n",
      "004672\n",
      "004673\n",
      "004676\n",
      "004679\n",
      "004685\n",
      "004686\n",
      "004687\n",
      "004692\n",
      "004693\n",
      "004694\n",
      "004699\n",
      "004706\n",
      "004707\n",
      "004715\n",
      "004718\n",
      "004722\n",
      "004723\n",
      "004727\n",
      "004732\n",
      "004742\n",
      "004750\n",
      "004753\n",
      "004760\n",
      "004761\n",
      "004770\n",
      "004779\n",
      "004782\n",
      "004788\n",
      "004789\n",
      "004790\n",
      "004796\n",
      "004797\n",
      "004801\n",
      "004812\n",
      "004815\n",
      "004826\n",
      "004828\n",
      "004830\n",
      "004831\n",
      "004832\n",
      "004834\n",
      "004840\n",
      "004846\n",
      "004852\n",
      "004857\n",
      "004859\n",
      "004868\n",
      "004872\n",
      "004879\n",
      "004882\n",
      "004886\n",
      "004897\n",
      "004898\n",
      "004902\n",
      "004913\n",
      "004916\n",
      "004928\n",
      "004931\n",
      "004935\n",
      "004939\n",
      "004943\n",
      "004946\n",
      "004950\n",
      "004953\n",
      "004954\n",
      "004968\n",
      "004976\n",
      "004977\n",
      "004985\n",
      "004987\n",
      "004990\n",
      "004994\n",
      "004995\n",
      "005014\n",
      "005024\n",
      "005027\n",
      "005029\n",
      "005036\n",
      "005037\n",
      "005039\n",
      "005042\n",
      "005045\n",
      "005052\n",
      "005054\n",
      "005057\n",
      "005061\n",
      "005063\n",
      "005064\n",
      "005081\n",
      "005086\n",
      "005090\n",
      "005093\n",
      "005097\n",
      "005102\n",
      "005107\n",
      "005111\n",
      "005114\n",
      "005116\n",
      "005122\n",
      "005124\n",
      "005129\n",
      "005134\n",
      "005144\n",
      "005146\n",
      "005156\n",
      "005159\n",
      "005160\n",
      "005161\n",
      "005173\n",
      "005176\n",
      "005189\n",
      "005190\n",
      "005195\n",
      "005208\n",
      "005219\n",
      "005220\n",
      "005222\n",
      "005229\n",
      "005230\n",
      "005231\n",
      "005236\n",
      "005239\n",
      "005242\n",
      "005245\n",
      "005248\n",
      "005257\n",
      "005260\n",
      "005268\n",
      "005269\n",
      "005273\n",
      "005274\n",
      "005278\n",
      "005281\n",
      "005283\n",
      "005297\n",
      "005298\n",
      "005303\n",
      "005304\n",
      "005306\n",
      "005307\n",
      "005310\n",
      "005311\n",
      "005315\n",
      "005326\n",
      "005327\n",
      "005331\n",
      "005336\n",
      "005343\n",
      "005344\n",
      "005346\n",
      "005348\n",
      "005351\n",
      "005368\n",
      "005371\n",
      "005374\n",
      "005383\n",
      "005384\n",
      "005385\n",
      "005396\n",
      "005398\n",
      "005405\n",
      "005410\n",
      "005413\n",
      "005414\n",
      "005417\n",
      "005419\n",
      "005421\n",
      "005424\n",
      "005429\n",
      "005430\n",
      "005439\n",
      "005441\n",
      "005448\n",
      "005450\n",
      "005451\n",
      "005457\n",
      "005461\n",
      "005467\n",
      "005471\n",
      "005475\n",
      "005478\n",
      "005485\n",
      "005487\n",
      "005496\n",
      "005499\n",
      "005507\n",
      "005508\n",
      "005509\n",
      "005511\n",
      "005522\n",
      "005526\n",
      "005527\n",
      "005536\n",
      "005541\n",
      "005542\n",
      "005547\n",
      "005550\n",
      "005552\n",
      "005554\n",
      "005563\n",
      "005573\n",
      "005574\n",
      "005582\n",
      "005583\n",
      "005585\n",
      "005593\n",
      "005599\n",
      "005600\n",
      "005606\n",
      "005608\n",
      "005611\n",
      "005613\n",
      "005615\n",
      "005618\n",
      "005631\n",
      "005636\n",
      "005639\n",
      "005644\n",
      "005645\n",
      "005648\n",
      "005652\n",
      "005653\n",
      "005657\n",
      "005658\n",
      "005660\n",
      "005662\n",
      "005664\n",
      "005669\n",
      "005672\n",
      "005680\n",
      "005682\n",
      "005693\n",
      "005695\n",
      "005700\n",
      "005702\n",
      "005704\n",
      "005714\n",
      "005716\n",
      "005719\n",
      "005728\n",
      "005729\n",
      "005731\n",
      "005732\n",
      "005738\n",
      "005740\n",
      "005741\n",
      "005747\n",
      "005757\n",
      "005761\n",
      "005764\n",
      "005765\n",
      "005768\n",
      "005773\n",
      "005780\n",
      "005784\n",
      "005786\n",
      "005788\n",
      "005790\n",
      "005796\n",
      "005799\n",
      "005811\n",
      "005814\n",
      "005818\n",
      "005824\n",
      "005826\n",
      "005829\n",
      "005836\n",
      "005840\n",
      "005845\n",
      "005850\n",
      "005852\n",
      "005856\n",
      "005859\n",
      "005863\n",
      "005873\n",
      "005881\n",
      "005884\n",
      "005885\n",
      "005889\n",
      "005903\n",
      "005905\n",
      "005906\n",
      "005908\n",
      "005910\n",
      "005919\n",
      "005923\n",
      "005928\n",
      "005930\n",
      "005938\n",
      "005940\n",
      "005948\n",
      "005951\n",
      "005956\n",
      "005961\n",
      "005971\n",
      "005975\n",
      "005979\n",
      "005984\n",
      "005985\n",
      "005988\n",
      "005989\n",
      "005990\n",
      "005991\n",
      "005992\n",
      "006004\n",
      "006005\n",
      "006011\n",
      "006028\n",
      "006029\n",
      "006033\n",
      "006038\n",
      "006041\n",
      "006043\n",
      "006071\n",
      "006073\n",
      "006095\n",
      "006103\n",
      "006104\n",
      "006117\n",
      "006128\n",
      "006130\n",
      "006133\n",
      "006140\n",
      "006148\n",
      "006150\n",
      "006151\n",
      "006156\n",
      "006158\n",
      "006161\n",
      "006163\n",
      "006171\n",
      "006172\n",
      "006175\n",
      "006176\n",
      "006177\n",
      "006179\n",
      "006180\n",
      "006185\n",
      "006187\n",
      "006198\n",
      "006209\n",
      "006210\n",
      "006221\n",
      "006225\n",
      "006229\n",
      "006233\n",
      "006235\n",
      "006236\n",
      "006238\n",
      "006240\n",
      "006243\n",
      "006259\n",
      "006272\n",
      "006276\n",
      "006282\n",
      "006284\n",
      "006285\n",
      "006286\n",
      "006289\n",
      "006296\n",
      "006300\n",
      "006320\n",
      "006339\n",
      "006341\n",
      "006344\n",
      "006346\n",
      "006351\n",
      "006352\n",
      "006355\n",
      "006369\n",
      "006381\n",
      "006387\n",
      "006391\n",
      "006392\n",
      "006395\n",
      "006398\n",
      "006409\n",
      "006427\n",
      "006428\n",
      "006433\n",
      "006440\n",
      "006442\n",
      "006444\n",
      "006445\n",
      "006450\n",
      "006455\n",
      "006459\n",
      "006463\n",
      "006465\n",
      "006475\n",
      "006476\n",
      "006482\n",
      "006483\n",
      "006486\n",
      "006488\n",
      "006492\n",
      "006497\n",
      "006503\n",
      "006506\n",
      "006519\n",
      "006529\n",
      "006532\n",
      "006536\n",
      "006542\n",
      "006550\n",
      "006551\n",
      "006560\n",
      "006572\n",
      "006576\n",
      "006578\n",
      "006583\n",
      "006584\n",
      "006585\n",
      "006587\n",
      "006588\n",
      "006599\n",
      "006602\n",
      "006609\n",
      "006610\n",
      "006611\n",
      "006612\n",
      "006617\n",
      "006618\n",
      "006621\n",
      "006625\n",
      "006626\n",
      "006631\n",
      "006637\n",
      "006645\n",
      "006647\n",
      "006648\n",
      "006658\n",
      "006661\n",
      "006664\n",
      "006666\n",
      "006667\n",
      "006668\n",
      "006670\n",
      "006677\n",
      "006682\n",
      "006690\n",
      "006695\n",
      "006696\n",
      "006697\n",
      "006698\n",
      "006699\n",
      "006703\n",
      "006707\n",
      "006709\n",
      "006714\n",
      "006718\n",
      "006722\n",
      "006727\n",
      "006731\n",
      "006735\n",
      "006738\n",
      "006739\n",
      "006748\n",
      "006753\n",
      "006762\n",
      "006766\n",
      "006768\n",
      "006772\n",
      "006777\n",
      "006782\n",
      "006783\n",
      "006784\n",
      "006786\n",
      "006789\n",
      "006797\n",
      "006800\n",
      "006802\n",
      "006803\n",
      "006808\n",
      "006810\n",
      "006821\n",
      "006824\n",
      "006833\n",
      "006835\n",
      "006836\n",
      "006838\n",
      "006840\n",
      "006841\n",
      "006847\n",
      "006850\n",
      "006855\n",
      "006858\n",
      "006862\n",
      "006864\n",
      "006866\n",
      "006868\n",
      "006874\n",
      "006876\n",
      "006880\n",
      "006887\n",
      "006896\n",
      "006903\n",
      "006908\n",
      "006910\n",
      "006911\n",
      "006912\n",
      "006919\n",
      "006924\n",
      "006932\n",
      "006934\n",
      "006939\n",
      "006943\n",
      "006945\n",
      "006948\n",
      "006950\n",
      "006956\n",
      "006958\n",
      "006959\n",
      "006963\n",
      "006965\n",
      "006968\n",
      "006981\n",
      "006983\n",
      "006987\n",
      "006988\n",
      "006989\n",
      "006990\n",
      "007004\n",
      "007007\n",
      "007016\n",
      "007020\n",
      "007023\n",
      "007031\n",
      "007035\n",
      "007036\n",
      "007040\n",
      "007045\n",
      "007046\n",
      "007048\n",
      "007049\n",
      "007052\n",
      "007054\n",
      "007059\n",
      "007073\n",
      "007079\n",
      "007080\n",
      "007093\n",
      "007108\n",
      "007109\n",
      "007114\n",
      "007117\n",
      "007121\n",
      "007122\n",
      "007128\n",
      "007129\n",
      "007138\n",
      "007139\n",
      "007140\n",
      "007147\n",
      "007148\n",
      "007150\n",
      "007153\n",
      "007154\n",
      "007159\n",
      "007163\n",
      "007167\n",
      "007172\n",
      "007182\n",
      "007184\n",
      "007187\n",
      "007189\n",
      "007200\n",
      "007208\n",
      "007213\n",
      "007214\n",
      "007216\n",
      "007227\n",
      "007234\n",
      "007236\n",
      "007243\n",
      "007244\n",
      "007256\n",
      "007260\n",
      "007261\n",
      "007266\n",
      "007270\n",
      "007271\n",
      "007276\n",
      "007284\n",
      "007292\n",
      "007296\n",
      "007298\n",
      "007305\n",
      "007308\n",
      "007314\n",
      "007318\n",
      "007325\n",
      "007330\n",
      "007336\n",
      "007346\n",
      "007350\n",
      "007359\n",
      "007369\n",
      "007375\n",
      "007381\n",
      "007388\n",
      "007398\n",
      "007408\n",
      "007410\n",
      "007413\n",
      "007414\n",
      "007436\n",
      "007438\n",
      "007445\n",
      "007446\n",
      "007448\n",
      "007449\n",
      "007457\n",
      "007466\n",
      "007475\n",
      "007480\n",
      "007482\n",
      "007483\n",
      "007484\n",
      "007493\n",
      "007497\n",
      "007498\n",
      "007511\n",
      "007526\n",
      "007535\n",
      "007540\n",
      "007543\n",
      "007547\n",
      "007558\n",
      "007559\n",
      "007563\n",
      "007566\n",
      "007568\n",
      "007570\n",
      "007571\n",
      "007572\n",
      "007576\n",
      "007578\n",
      "007579\n",
      "007586\n",
      "007590\n",
      "007592\n",
      "007594\n",
      "007605\n",
      "007606\n",
      "007611\n",
      "007618\n",
      "007619\n",
      "007629\n",
      "007631\n",
      "007639\n",
      "007647\n",
      "007649\n",
      "007653\n",
      "007656\n",
      "007657\n",
      "007668\n",
      "007675\n",
      "007683\n",
      "007687\n",
      "007694\n",
      "007697\n",
      "007699\n",
      "007709\n",
      "007712\n",
      "007718\n",
      "007723\n",
      "007727\n",
      "007735\n",
      "007745\n",
      "007746\n",
      "007748\n",
      "007751\n",
      "007760\n",
      "007763\n",
      "007768\n",
      "007777\n",
      "007786\n",
      "007793\n",
      "007795\n",
      "007799\n",
      "007809\n",
      "007810\n",
      "007813\n",
      "007814\n",
      "007815\n",
      "007821\n",
      "007836\n",
      "007841\n",
      "007856\n",
      "007872\n",
      "007877\n",
      "007878\n",
      "007884\n",
      "007889\n",
      "007897\n",
      "007901\n",
      "007902\n",
      "007910\n",
      "007911\n",
      "007915\n",
      "007919\n",
      "007928\n",
      "007931\n",
      "007932\n",
      "007940\n",
      "007943\n",
      "007946\n",
      "007953\n",
      "007963\n",
      "007964\n",
      "007970\n",
      "007971\n",
      "007979\n",
      "007984\n",
      "007987\n",
      "007996\n",
      "007997\n",
      "007998\n",
      "008002\n",
      "008008\n",
      "008019\n",
      "008023\n",
      "008024\n",
      "008026\n",
      "008033\n",
      "008040\n",
      "008042\n",
      "008043\n",
      "008048\n",
      "008053\n",
      "008069\n",
      "008072\n",
      "008079\n",
      "008082\n",
      "008083\n",
      "008086\n",
      "008091\n",
      "008096\n",
      "008103\n",
      "008105\n",
      "008108\n",
      "008122\n",
      "008125\n",
      "008127\n",
      "008130\n",
      "008132\n",
      "008137\n",
      "008139\n",
      "008142\n",
      "008160\n",
      "008163\n",
      "008164\n",
      "008171\n",
      "008175\n",
      "008177\n",
      "008180\n",
      "008188\n",
      "008190\n",
      "008199\n",
      "008204\n",
      "008209\n",
      "008211\n",
      "008216\n",
      "008218\n",
      "008220\n",
      "008224\n",
      "008225\n",
      "008232\n",
      "008241\n",
      "008248\n",
      "008253\n",
      "008261\n",
      "008263\n",
      "008272\n",
      "008279\n",
      "008281\n",
      "008284\n",
      "008285\n",
      "008293\n",
      "008294\n",
      "008301\n",
      "008302\n",
      "008307\n",
      "008312\n",
      "008313\n",
      "008315\n",
      "008327\n",
      "008332\n",
      "008341\n",
      "008360\n",
      "008368\n",
      "008374\n",
      "008376\n",
      "008381\n",
      "008385\n",
      "008387\n",
      "008390\n",
      "008391\n",
      "008397\n",
      "008398\n",
      "008410\n",
      "008413\n",
      "008430\n",
      "008433\n",
      "008437\n",
      "008444\n",
      "008445\n",
      "008449\n",
      "008452\n",
      "008453\n",
      "008461\n",
      "008467\n",
      "008470\n",
      "008475\n",
      "008482\n",
      "008485\n",
      "008503\n",
      "008509\n",
      "008513\n",
      "008514\n",
      "008526\n",
      "008529\n",
      "008541\n",
      "008550\n",
      "008559\n",
      "008562\n",
      "008573\n",
      "008576\n",
      "008582\n",
      "008585\n",
      "008587\n",
      "008592\n",
      "008596\n",
      "008602\n",
      "008604\n",
      "008612\n",
      "008617\n",
      "008618\n",
      "008628\n",
      "008638\n",
      "008644\n",
      "008647\n",
      "008663\n",
      "008665\n",
      "008676\n",
      "008690\n",
      "008692\n",
      "008698\n",
      "008699\n",
      "008701\n",
      "008702\n",
      "008706\n",
      "008718\n",
      "008728\n",
      "008731\n",
      "008738\n",
      "008739\n",
      "008742\n",
      "008744\n",
      "008748\n",
      "008752\n",
      "008766\n",
      "008768\n",
      "008769\n",
      "008771\n",
      "008775\n",
      "008794\n",
      "008801\n",
      "008805\n",
      "008810\n",
      "008813\n",
      "008817\n",
      "008833\n",
      "008836\n",
      "008838\n",
      "008862\n",
      "008865\n",
      "008867\n",
      "008872\n",
      "008876\n",
      "008879\n",
      "008884\n",
      "008886\n",
      "008891\n",
      "008892\n",
      "008900\n",
      "008911\n",
      "008920\n",
      "008921\n",
      "008931\n",
      "008932\n",
      "008943\n",
      "008944\n",
      "008953\n",
      "008960\n",
      "008961\n",
      "008965\n",
      "008975\n",
      "008979\n",
      "008983\n",
      "009000\n",
      "009005\n",
      "009020\n",
      "009022\n",
      "009024\n",
      "009034\n",
      "009036\n",
      "009045\n",
      "009049\n",
      "009064\n",
      "009068\n",
      "009073\n",
      "009078\n",
      "009079\n",
      "009087\n",
      "009089\n",
      "009091\n",
      "009114\n",
      "009123\n",
      "009128\n",
      "009136\n",
      "009141\n",
      "009153\n",
      "009160\n",
      "009161\n",
      "009174\n",
      "009180\n",
      "009184\n",
      "009193\n",
      "009197\n",
      "009202\n",
      "009208\n",
      "009209\n",
      "009214\n",
      "009224\n",
      "009230\n",
      "009236\n",
      "009238\n",
      "009239\n",
      "009242\n",
      "009251\n",
      "009259\n",
      "009273\n",
      "009283\n",
      "009286\n",
      "009287\n",
      "009288\n",
      "009290\n",
      "009291\n",
      "009295\n",
      "009296\n",
      "009299\n",
      "009303\n",
      "009306\n",
      "009308\n",
      "009312\n",
      "009316\n",
      "009318\n",
      "009327\n",
      "009330\n",
      "009331\n",
      "009336\n",
      "009342\n",
      "009343\n",
      "009349\n",
      "009358\n",
      "009362\n",
      "009368\n",
      "009374\n",
      "009375\n",
      "009377\n",
      "009382\n",
      "009393\n",
      "009394\n",
      "009407\n",
      "009410\n",
      "009411\n",
      "009412\n",
      "009413\n",
      "009414\n",
      "009418\n",
      "009420\n",
      "009421\n",
      "009424\n",
      "009433\n",
      "009434\n",
      "009437\n",
      "009440\n",
      "009448\n",
      "009456\n",
      "009460\n",
      "009468\n",
      "009469\n",
      "009477\n",
      "009479\n",
      "009481\n",
      "009490\n",
      "009494\n",
      "009496\n",
      "009497\n",
      "009499\n",
      "009500\n",
      "009504\n",
      "009512\n",
      "009517\n",
      "009518\n",
      "009520\n",
      "009524\n",
      "009526\n",
      "009531\n",
      "009532\n",
      "009537\n",
      "009541\n",
      "009542\n",
      "009546\n",
      "009551\n",
      "009557\n",
      "009558\n",
      "009565\n",
      "009567\n",
      "009568\n",
      "009573\n",
      "009577\n",
      "009579\n",
      "009585\n",
      "009591\n",
      "009596\n",
      "009600\n",
      "009603\n",
      "009609\n",
      "009611\n",
      "009613\n",
      "009614\n",
      "009617\n",
      "009618\n",
      "009638\n",
      "009641\n",
      "009647\n",
      "009649\n",
      "009654\n",
      "009655\n",
      "009656\n",
      "009659\n",
      "009666\n",
      "009668\n",
      "009671\n",
      "009676\n",
      "009684\n",
      "009687\n",
      "009691\n",
      "009693\n",
      "009698\n",
      "009703\n",
      "009711\n",
      "009712\n",
      "009713\n",
      "009717\n",
      "009718\n",
      "009721\n",
      "009726\n",
      "009732\n",
      "009734\n",
      "009738\n",
      "009747\n",
      "009754\n",
      "009755\n",
      "009756\n",
      "009762\n",
      "009767\n",
      "009773\n",
      "009776\n",
      "009780\n",
      "009781\n",
      "009789\n",
      "009792\n",
      "009796\n",
      "009800\n",
      "009809\n",
      "009813\n",
      "009822\n",
      "009828\n",
      "009841\n",
      "009845\n",
      "009848\n",
      "009851\n",
      "009859\n",
      "009867\n",
      "009868\n",
      "009869\n",
      "009874\n",
      "009877\n",
      "009878\n",
      "009879\n",
      "009880\n",
      "009881\n",
      "009882\n",
      "009900\n",
      "009902\n",
      "009917\n",
      "009918\n",
      "009926\n",
      "009935\n",
      "009942\n",
      "009944\n",
      "009946\n",
      "009947\n",
      "009949\n",
      "009950\n",
      "009954\n",
      "009958\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 128  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n",
      "32\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4868, 0.7560, 0.5120, 0.7756, 0.0000],\n",
       "        [0.4195, 0.7667, 0.5457, 0.8825, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazeFace(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (4): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (5): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (7): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (8): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (9): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BlazeFaceExtra(\n",
      "  (features): Sequential(\n",
      "    (0): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "SSD(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(96, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.blazeface import SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 128,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 128],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(96, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface128_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 5.5479 || 10iter: 5.3336 sec.\n",
      "Iteration 20 || Loss: 5.7275 || 10iter: 2.7481 sec.\n",
      "Iteration 30 || Loss: 6.0663 || 10iter: 2.5544 sec.\n",
      "Iteration 40 || Loss: 5.1433 || 10iter: 2.4950 sec.\n",
      "Iteration 50 || Loss: 6.2372 || 10iter: 2.5299 sec.\n",
      "Iteration 60 || Loss: 5.6652 || 10iter: 1.5303 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:381.4997 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1708 sec.\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 70 || Loss: 5.8343 || 10iter: 3.5246 sec.\n",
      "Iteration 80 || Loss: 5.7348 || 10iter: 2.6142 sec.\n",
      "Iteration 90 || Loss: 5.6922 || 10iter: 2.2431 sec.\n",
      "Iteration 100 || Loss: 5.8039 || 10iter: 2.1730 sec.\n",
      "Iteration 110 || Loss: 6.2829 || 10iter: 2.2542 sec.\n",
      "Iteration 120 || Loss: 6.0590 || 10iter: 2.2100 sec.\n",
      "Iteration 130 || Loss: 6.0433 || 10iter: 1.4679 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:370.6800 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9388 sec.\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 140 || Loss: 5.7259 || 10iter: 4.6616 sec.\n",
      "Iteration 150 || Loss: 5.7017 || 10iter: 2.6851 sec.\n",
      "Iteration 160 || Loss: 6.0777 || 10iter: 1.9705 sec.\n",
      "Iteration 170 || Loss: 5.2033 || 10iter: 2.3723 sec.\n",
      "Iteration 180 || Loss: 5.0934 || 10iter: 2.2945 sec.\n",
      "Iteration 190 || Loss: 5.6759 || 10iter: 1.6005 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:371.3637 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7520 sec.\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 200 || Loss: 5.3693 || 10iter: 2.7516 sec.\n",
      "Iteration 210 || Loss: 5.4502 || 10iter: 2.8320 sec.\n",
      "Iteration 220 || Loss: 5.5503 || 10iter: 2.5722 sec.\n",
      "Iteration 230 || Loss: 5.7204 || 10iter: 2.0340 sec.\n",
      "Iteration 240 || Loss: 5.2631 || 10iter: 2.1608 sec.\n",
      "Iteration 250 || Loss: 6.0306 || 10iter: 2.2689 sec.\n",
      "Iteration 260 || Loss: 5.6778 || 10iter: 1.3765 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:361.6775 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6516 sec.\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 270 || Loss: 5.4423 || 10iter: 4.1347 sec.\n",
      "Iteration 280 || Loss: 5.5828 || 10iter: 2.6130 sec.\n",
      "Iteration 290 || Loss: 5.1244 || 10iter: 2.4363 sec.\n",
      "Iteration 300 || Loss: 5.2593 || 10iter: 2.2210 sec.\n",
      "Iteration 310 || Loss: 5.4994 || 10iter: 2.5037 sec.\n",
      "Iteration 320 || Loss: 5.3579 || 10iter: 1.8597 sec.\n",
      "Iteration 330 || Loss: 5.8109 || 10iter: 1.3610 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:359.7139 ||Epoch_VAL_Loss:178.6669\n",
      "timer:  21.3097 sec.\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 340 || Loss: 5.4477 || 10iter: 5.2089 sec.\n",
      "Iteration 350 || Loss: 5.3882 || 10iter: 2.5897 sec.\n",
      "Iteration 360 || Loss: 5.4964 || 10iter: 1.9917 sec.\n",
      "Iteration 370 || Loss: 5.1042 || 10iter: 2.2729 sec.\n",
      "Iteration 380 || Loss: 5.3114 || 10iter: 2.2567 sec.\n",
      "Iteration 390 || Loss: 5.6757 || 10iter: 1.4220 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:361.8249 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6328 sec.\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 400 || Loss: 5.7782 || 10iter: 3.6683 sec.\n",
      "Iteration 410 || Loss: 5.6636 || 10iter: 2.3567 sec.\n",
      "Iteration 420 || Loss: 4.9363 || 10iter: 2.3521 sec.\n",
      "Iteration 430 || Loss: 4.8121 || 10iter: 2.1638 sec.\n",
      "Iteration 440 || Loss: 5.6995 || 10iter: 2.5756 sec.\n",
      "Iteration 450 || Loss: 5.4929 || 10iter: 2.0581 sec.\n",
      "Iteration 460 || Loss: 4.9782 || 10iter: 1.4410 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:350.3883 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0207 sec.\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 470 || Loss: 5.2930 || 10iter: 4.8202 sec.\n",
      "Iteration 480 || Loss: 5.4677 || 10iter: 2.5088 sec.\n",
      "Iteration 490 || Loss: 5.5563 || 10iter: 2.0304 sec.\n",
      "Iteration 500 || Loss: 5.4911 || 10iter: 2.3478 sec.\n",
      "Iteration 510 || Loss: 5.4011 || 10iter: 2.2830 sec.\n",
      "Iteration 520 || Loss: 5.1043 || 10iter: 1.5997 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:352.5889 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7769 sec.\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 530 || Loss: 5.4231 || 10iter: 2.5224 sec.\n",
      "Iteration 540 || Loss: 4.9497 || 10iter: 3.4428 sec.\n",
      "Iteration 550 || Loss: 4.5482 || 10iter: 2.2555 sec.\n",
      "Iteration 560 || Loss: 5.5765 || 10iter: 2.1696 sec.\n",
      "Iteration 570 || Loss: 5.2601 || 10iter: 2.2773 sec.\n",
      "Iteration 580 || Loss: 5.0973 || 10iter: 2.2606 sec.\n",
      "Iteration 590 || Loss: 5.3270 || 10iter: 1.3350 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:342.4322 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8927 sec.\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 600 || Loss: 5.1530 || 10iter: 4.0443 sec.\n",
      "Iteration 610 || Loss: 5.3476 || 10iter: 2.4299 sec.\n",
      "Iteration 620 || Loss: 4.9570 || 10iter: 2.2212 sec.\n",
      "Iteration 630 || Loss: 5.4103 || 10iter: 2.1877 sec.\n",
      "Iteration 640 || Loss: 5.1454 || 10iter: 2.4775 sec.\n",
      "Iteration 650 || Loss: 5.1869 || 10iter: 2.0083 sec.\n",
      "Iteration 660 || Loss: 4.9438 || 10iter: 1.3948 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:342.8637 ||Epoch_VAL_Loss:168.7043\n",
      "timer:  20.9068 sec.\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 670 || Loss: 5.0275 || 10iter: 5.4529 sec.\n",
      "Iteration 680 || Loss: 5.3470 || 10iter: 2.2877 sec.\n",
      "Iteration 690 || Loss: 4.9022 || 10iter: 2.2562 sec.\n",
      "Iteration 700 || Loss: 5.2591 || 10iter: 2.7267 sec.\n",
      "Iteration 710 || Loss: 6.2243 || 10iter: 2.4174 sec.\n",
      "Iteration 720 || Loss: 5.6509 || 10iter: 1.5412 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:343.5796 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.6134 sec.\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 730 || Loss: 5.0263 || 10iter: 3.4397 sec.\n",
      "Iteration 740 || Loss: 5.0307 || 10iter: 2.6264 sec.\n",
      "Iteration 750 || Loss: 5.0744 || 10iter: 2.2401 sec.\n",
      "Iteration 760 || Loss: 5.4857 || 10iter: 2.1204 sec.\n",
      "Iteration 770 || Loss: 4.9824 || 10iter: 2.5020 sec.\n",
      "Iteration 780 || Loss: 5.2534 || 10iter: 1.9840 sec.\n",
      "Iteration 790 || Loss: 5.0092 || 10iter: 1.3407 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:340.1990 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5820 sec.\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 800 || Loss: 5.0962 || 10iter: 4.7964 sec.\n",
      "Iteration 810 || Loss: 5.3842 || 10iter: 2.5323 sec.\n",
      "Iteration 820 || Loss: 5.1613 || 10iter: 2.1660 sec.\n",
      "Iteration 830 || Loss: 5.3555 || 10iter: 2.1741 sec.\n",
      "Iteration 840 || Loss: 5.1347 || 10iter: 2.3993 sec.\n",
      "Iteration 850 || Loss: 5.1532 || 10iter: 1.7369 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:340.4833 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2280 sec.\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n",
      "Iteration 860 || Loss: 5.1278 || 10iter: 2.8288 sec.\n",
      "Iteration 870 || Loss: 4.7065 || 10iter: 2.8649 sec.\n",
      "Iteration 880 || Loss: 4.6455 || 10iter: 2.4800 sec.\n",
      "Iteration 890 || Loss: 5.7687 || 10iter: 2.0998 sec.\n",
      "Iteration 900 || Loss: 5.3760 || 10iter: 2.3429 sec.\n",
      "Iteration 910 || Loss: 4.9327 || 10iter: 2.0976 sec.\n",
      "Iteration 920 || Loss: 4.7118 || 10iter: 1.3857 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:339.9219 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7692 sec.\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 930 || Loss: 5.0891 || 10iter: 4.1774 sec.\n",
      "Iteration 940 || Loss: 4.8935 || 10iter: 2.5661 sec.\n",
      "Iteration 950 || Loss: 5.1078 || 10iter: 2.3606 sec.\n",
      "Iteration 960 || Loss: 5.2167 || 10iter: 1.9984 sec.\n",
      "Iteration 970 || Loss: 5.5440 || 10iter: 2.5557 sec.\n",
      "Iteration 980 || Loss: 5.3640 || 10iter: 1.8135 sec.\n",
      "Iteration 990 || Loss: 4.5175 || 10iter: 1.3106 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:337.7575 ||Epoch_VAL_Loss:166.8805\n",
      "timer:  20.9661 sec.\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1000 || Loss: 4.9067 || 10iter: 5.4316 sec.\n",
      "Iteration 1010 || Loss: 5.3833 || 10iter: 2.2321 sec.\n",
      "Iteration 1020 || Loss: 4.9477 || 10iter: 2.1519 sec.\n",
      "Iteration 1030 || Loss: 5.5054 || 10iter: 2.3760 sec.\n",
      "Iteration 1040 || Loss: 5.0178 || 10iter: 2.3507 sec.\n",
      "Iteration 1050 || Loss: 5.6542 || 10iter: 1.5804 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:336.9681 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1564 sec.\n",
      "-------------\n",
      "Epoch 17/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1060 || Loss: 4.9188 || 10iter: 3.5861 sec.\n",
      "Iteration 1070 || Loss: 5.2545 || 10iter: 2.4362 sec.\n",
      "Iteration 1080 || Loss: 4.9524 || 10iter: 2.3262 sec.\n",
      "Iteration 1090 || Loss: 5.3221 || 10iter: 2.2201 sec.\n",
      "Iteration 1100 || Loss: 4.9313 || 10iter: 2.6234 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1110 || Loss: 5.4519 || 10iter: 2.0038 sec.\n",
      "Iteration 1120 || Loss: 5.1497 || 10iter: 1.3904 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:335.8707 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0043 sec.\n",
      "-------------\n",
      "Epoch 18/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1130 || Loss: 5.0034 || 10iter: 4.7680 sec.\n",
      "Iteration 1140 || Loss: 5.0973 || 10iter: 2.4057 sec.\n",
      "Iteration 1150 || Loss: 5.4013 || 10iter: 2.2585 sec.\n",
      "Iteration 1160 || Loss: 4.9067 || 10iter: 1.9922 sec.\n",
      "Iteration 1170 || Loss: 4.6039 || 10iter: 2.3056 sec.\n",
      "Iteration 1180 || Loss: 4.3366 || 10iter: 1.6923 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:333.2262 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6123 sec.\n",
      "-------------\n",
      "Epoch 19/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1190 || Loss: 5.7233 || 10iter: 2.9204 sec.\n",
      "Iteration 1200 || Loss: 5.0196 || 10iter: 2.9912 sec.\n",
      "Iteration 1210 || Loss: 5.1865 || 10iter: 2.1841 sec.\n",
      "Iteration 1220 || Loss: 4.9481 || 10iter: 2.3066 sec.\n",
      "Iteration 1230 || Loss: 5.6606 || 10iter: 2.5458 sec.\n",
      "Iteration 1240 || Loss: 5.2679 || 10iter: 2.0879 sec.\n",
      "Iteration 1250 || Loss: 4.7419 || 10iter: 1.3968 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:336.3044 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0753 sec.\n",
      "-------------\n",
      "Epoch 20/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1260 || Loss: 5.0868 || 10iter: 4.3151 sec.\n",
      "Iteration 1270 || Loss: 4.9359 || 10iter: 2.5342 sec.\n",
      "Iteration 1280 || Loss: 5.4711 || 10iter: 2.2462 sec.\n",
      "Iteration 1290 || Loss: 4.7886 || 10iter: 2.2417 sec.\n",
      "Iteration 1300 || Loss: 4.5485 || 10iter: 2.4065 sec.\n",
      "Iteration 1310 || Loss: 4.8824 || 10iter: 1.7099 sec.\n",
      "Iteration 1320 || Loss: 5.1941 || 10iter: 1.3326 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:333.8391 ||Epoch_VAL_Loss:164.1797\n",
      "timer:  20.9130 sec.\n",
      "-------------\n",
      "Epoch 21/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1330 || Loss: 5.0024 || 10iter: 5.4007 sec.\n",
      "Iteration 1340 || Loss: 4.9855 || 10iter: 2.1798 sec.\n",
      "Iteration 1350 || Loss: 4.9593 || 10iter: 2.1484 sec.\n",
      "Iteration 1360 || Loss: 4.9292 || 10iter: 2.6196 sec.\n",
      "Iteration 1370 || Loss: 5.2502 || 10iter: 2.7847 sec.\n",
      "Iteration 1380 || Loss: 5.0165 || 10iter: 1.7732 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:330.5980 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.9121 sec.\n",
      "-------------\n",
      "Epoch 22/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1390 || Loss: 5.2903 || 10iter: 3.6234 sec.\n",
      "Iteration 1400 || Loss: 5.0116 || 10iter: 2.6711 sec.\n",
      "Iteration 1410 || Loss: 5.0274 || 10iter: 2.4070 sec.\n",
      "Iteration 1420 || Loss: 5.0214 || 10iter: 2.0692 sec.\n",
      "Iteration 1430 || Loss: 5.1939 || 10iter: 2.4354 sec.\n",
      "Iteration 1440 || Loss: 4.7507 || 10iter: 2.1512 sec.\n",
      "Iteration 1450 || Loss: 5.3375 || 10iter: 1.3692 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:327.2368 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1245 sec.\n",
      "-------------\n",
      "Epoch 23/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1460 || Loss: 5.3373 || 10iter: 4.8491 sec.\n",
      "Iteration 1470 || Loss: 4.4513 || 10iter: 2.4588 sec.\n",
      "Iteration 1480 || Loss: 4.8382 || 10iter: 2.1659 sec.\n",
      "Iteration 1490 || Loss: 5.0186 || 10iter: 2.1462 sec.\n",
      "Iteration 1500 || Loss: 4.7891 || 10iter: 2.5406 sec.\n",
      "Iteration 1510 || Loss: 4.9596 || 10iter: 1.6639 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:327.8264 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0038 sec.\n",
      "-------------\n",
      "Epoch 24/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1520 || Loss: 4.6721 || 10iter: 2.9132 sec.\n",
      "Iteration 1530 || Loss: 5.1509 || 10iter: 2.8237 sec.\n",
      "Iteration 1540 || Loss: 4.9665 || 10iter: 2.2979 sec.\n",
      "Iteration 1550 || Loss: 4.6467 || 10iter: 2.1979 sec.\n",
      "Iteration 1560 || Loss: 5.0975 || 10iter: 2.2635 sec.\n",
      "Iteration 1570 || Loss: 4.9207 || 10iter: 2.4088 sec.\n",
      "Iteration 1580 || Loss: 4.8085 || 10iter: 1.5342 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:329.4564 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1779 sec.\n",
      "-------------\n",
      "Epoch 25/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1590 || Loss: 5.0289 || 10iter: 4.2659 sec.\n",
      "Iteration 1600 || Loss: 5.0019 || 10iter: 2.4923 sec.\n",
      "Iteration 1610 || Loss: 4.9899 || 10iter: 2.2497 sec.\n",
      "Iteration 1620 || Loss: 4.8745 || 10iter: 2.1864 sec.\n",
      "Iteration 1630 || Loss: 4.7298 || 10iter: 2.4640 sec.\n",
      "Iteration 1640 || Loss: 5.0705 || 10iter: 1.8185 sec.\n",
      "Iteration 1650 || Loss: 5.0192 || 10iter: 1.3319 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:328.8988 ||Epoch_VAL_Loss:163.9312\n",
      "timer:  20.9816 sec.\n",
      "-------------\n",
      "Epoch 26/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1660 || Loss: 4.6869 || 10iter: 5.6606 sec.\n",
      "Iteration 1670 || Loss: 4.5711 || 10iter: 2.3383 sec.\n",
      "Iteration 1680 || Loss: 4.9967 || 10iter: 2.1139 sec.\n",
      "Iteration 1690 || Loss: 4.5949 || 10iter: 2.3645 sec.\n",
      "Iteration 1700 || Loss: 4.6759 || 10iter: 2.1612 sec.\n",
      "Iteration 1710 || Loss: 4.7977 || 10iter: 1.4175 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:325.6965 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9725 sec.\n",
      "-------------\n",
      "Epoch 27/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1720 || Loss: 5.0313 || 10iter: 3.4774 sec.\n",
      "Iteration 1730 || Loss: 4.9027 || 10iter: 2.7069 sec.\n",
      "Iteration 1740 || Loss: 4.6926 || 10iter: 2.2415 sec.\n",
      "Iteration 1750 || Loss: 4.7629 || 10iter: 2.1676 sec.\n",
      "Iteration 1760 || Loss: 4.4731 || 10iter: 2.3448 sec.\n",
      "Iteration 1770 || Loss: 4.6263 || 10iter: 1.9873 sec.\n",
      "Iteration 1780 || Loss: 5.0261 || 10iter: 1.4200 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:325.6358 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7484 sec.\n",
      "-------------\n",
      "Epoch 28/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1790 || Loss: 4.6326 || 10iter: 4.5784 sec.\n",
      "Iteration 1800 || Loss: 4.7266 || 10iter: 2.5681 sec.\n",
      "Iteration 1810 || Loss: 5.6468 || 10iter: 2.3992 sec.\n",
      "Iteration 1820 || Loss: 5.0880 || 10iter: 2.1730 sec.\n",
      "Iteration 1830 || Loss: 4.9991 || 10iter: 2.3634 sec.\n",
      "Iteration 1840 || Loss: 4.9055 || 10iter: 1.7071 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:328.9956 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9917 sec.\n",
      "-------------\n",
      "Epoch 29/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1850 || Loss: 5.2013 || 10iter: 2.8059 sec.\n",
      "Iteration 1860 || Loss: 4.9615 || 10iter: 3.0620 sec.\n",
      "Iteration 1870 || Loss: 4.7790 || 10iter: 2.3386 sec.\n",
      "Iteration 1880 || Loss: 5.3274 || 10iter: 2.0855 sec.\n",
      "Iteration 1890 || Loss: 4.7061 || 10iter: 2.2179 sec.\n",
      "Iteration 1900 || Loss: 4.7222 || 10iter: 2.2113 sec.\n",
      "Iteration 1910 || Loss: 5.0859 || 10iter: 1.3441 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:323.9243 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7057 sec.\n",
      "-------------\n",
      "Epoch 30/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1920 || Loss: 4.9649 || 10iter: 3.9684 sec.\n",
      "Iteration 1930 || Loss: 4.8104 || 10iter: 2.7618 sec.\n",
      "Iteration 1940 || Loss: 4.9723 || 10iter: 2.2229 sec.\n",
      "Iteration 1950 || Loss: 5.2117 || 10iter: 2.2099 sec.\n",
      "Iteration 1960 || Loss: 4.7823 || 10iter: 2.5379 sec.\n",
      "Iteration 1970 || Loss: 4.9782 || 10iter: 1.8871 sec.\n",
      "Iteration 1980 || Loss: 4.8138 || 10iter: 1.3872 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:322.8083 ||Epoch_VAL_Loss:162.5434\n",
      "timer:  21.2249 sec.\n",
      "-------------\n",
      "Epoch 31/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1990 || Loss: 4.6884 || 10iter: 5.4238 sec.\n",
      "Iteration 2000 || Loss: 5.3556 || 10iter: 2.2550 sec.\n",
      "Iteration 2010 || Loss: 5.3306 || 10iter: 2.0479 sec.\n",
      "Iteration 2020 || Loss: 4.8743 || 10iter: 2.1610 sec.\n",
      "Iteration 2030 || Loss: 4.4872 || 10iter: 2.7972 sec.\n",
      "Iteration 2040 || Loss: 5.2312 || 10iter: 1.6029 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:323.7060 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2115 sec.\n",
      "-------------\n",
      "Epoch 32/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2050 || Loss: 5.4566 || 10iter: 3.7414 sec.\n",
      "Iteration 2060 || Loss: 4.7687 || 10iter: 2.5950 sec.\n",
      "Iteration 2070 || Loss: 5.0425 || 10iter: 2.3610 sec.\n",
      "Iteration 2080 || Loss: 5.0751 || 10iter: 2.0378 sec.\n",
      "Iteration 2090 || Loss: 4.5952 || 10iter: 2.4099 sec.\n",
      "Iteration 2100 || Loss: 4.9043 || 10iter: 1.9281 sec.\n",
      "Iteration 2110 || Loss: 4.7036 || 10iter: 1.4016 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:323.9456 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8638 sec.\n",
      "-------------\n",
      "Epoch 33/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2120 || Loss: 4.7336 || 10iter: 5.0133 sec.\n",
      "Iteration 2130 || Loss: 5.1044 || 10iter: 2.2347 sec.\n",
      "Iteration 2140 || Loss: 4.6155 || 10iter: 2.1513 sec.\n",
      "Iteration 2150 || Loss: 4.7763 || 10iter: 2.1380 sec.\n",
      "Iteration 2160 || Loss: 4.6284 || 10iter: 2.5208 sec.\n",
      "Iteration 2170 || Loss: 4.8736 || 10iter: 1.6819 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:320.8157 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0587 sec.\n",
      "-------------\n",
      "Epoch 34/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2180 || Loss: 4.5463 || 10iter: 2.3014 sec.\n",
      "Iteration 2190 || Loss: 4.9079 || 10iter: 3.4273 sec.\n",
      "Iteration 2200 || Loss: 5.0587 || 10iter: 2.1637 sec.\n",
      "Iteration 2210 || Loss: 5.0225 || 10iter: 2.0253 sec.\n",
      "Iteration 2220 || Loss: 4.9863 || 10iter: 2.3422 sec.\n",
      "Iteration 2230 || Loss: 4.7356 || 10iter: 2.1090 sec.\n",
      "Iteration 2240 || Loss: 4.7322 || 10iter: 1.4050 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:324.4297 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4637 sec.\n",
      "-------------\n",
      "Epoch 35/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2250 || Loss: 4.7422 || 10iter: 4.5168 sec.\n",
      "Iteration 2260 || Loss: 5.2936 || 10iter: 2.3569 sec.\n",
      "Iteration 2270 || Loss: 4.6589 || 10iter: 2.2384 sec.\n",
      "Iteration 2280 || Loss: 4.4572 || 10iter: 2.0014 sec.\n",
      "Iteration 2290 || Loss: 4.9295 || 10iter: 2.3099 sec.\n",
      "Iteration 2300 || Loss: 4.6395 || 10iter: 1.9981 sec.\n",
      "Iteration 2310 || Loss: 5.4465 || 10iter: 1.2673 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:320.5236 ||Epoch_VAL_Loss:161.0798\n",
      "timer:  20.8583 sec.\n",
      "-------------\n",
      "Epoch 36/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2320 || Loss: 5.0303 || 10iter: 5.3824 sec.\n",
      "Iteration 2330 || Loss: 4.9010 || 10iter: 2.4615 sec.\n",
      "Iteration 2340 || Loss: 4.7951 || 10iter: 2.0198 sec.\n",
      "Iteration 2350 || Loss: 4.9254 || 10iter: 2.3252 sec.\n",
      "Iteration 2360 || Loss: 5.0175 || 10iter: 2.3990 sec.\n",
      "Iteration 2370 || Loss: 5.3065 || 10iter: 1.4031 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:321.5077 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8765 sec.\n",
      "-------------\n",
      "Epoch 37/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2380 || Loss: 4.7572 || 10iter: 3.6692 sec.\n",
      "Iteration 2390 || Loss: 4.6710 || 10iter: 2.4761 sec.\n",
      "Iteration 2400 || Loss: 4.6889 || 10iter: 2.2531 sec.\n",
      "Iteration 2410 || Loss: 4.7792 || 10iter: 2.0350 sec.\n",
      "Iteration 2420 || Loss: 4.6167 || 10iter: 2.4608 sec.\n",
      "Iteration 2430 || Loss: 4.9115 || 10iter: 1.8533 sec.\n",
      "Iteration 2440 || Loss: 4.8146 || 10iter: 1.3568 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:320.4042 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4765 sec.\n",
      "-------------\n",
      "Epoch 38/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2450 || Loss: 4.7234 || 10iter: 5.0081 sec.\n",
      "Iteration 2460 || Loss: 4.9703 || 10iter: 2.2155 sec.\n",
      "Iteration 2470 || Loss: 4.7454 || 10iter: 2.1142 sec.\n",
      "Iteration 2480 || Loss: 5.0044 || 10iter: 2.2693 sec.\n",
      "Iteration 2490 || Loss: 4.8169 || 10iter: 2.3591 sec.\n",
      "Iteration 2500 || Loss: 4.9699 || 10iter: 1.7362 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:321.8560 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9995 sec.\n",
      "-------------\n",
      "Epoch 39/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2510 || Loss: 4.8081 || 10iter: 2.8110 sec.\n",
      "Iteration 2520 || Loss: 5.1524 || 10iter: 2.8712 sec.\n",
      "Iteration 2530 || Loss: 4.9022 || 10iter: 2.3036 sec.\n",
      "Iteration 2540 || Loss: 4.8834 || 10iter: 1.9712 sec.\n",
      "Iteration 2550 || Loss: 4.8400 || 10iter: 2.4916 sec.\n",
      "Iteration 2560 || Loss: 4.7552 || 10iter: 2.0791 sec.\n",
      "Iteration 2570 || Loss: 4.7374 || 10iter: 1.3831 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:321.0162 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5330 sec.\n",
      "-------------\n",
      "Epoch 40/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2580 || Loss: 5.1331 || 10iter: 4.2662 sec.\n",
      "Iteration 2590 || Loss: 4.9432 || 10iter: 2.3945 sec.\n",
      "Iteration 2600 || Loss: 4.4512 || 10iter: 2.1556 sec.\n",
      "Iteration 2610 || Loss: 4.5210 || 10iter: 2.1256 sec.\n",
      "Iteration 2620 || Loss: 4.8840 || 10iter: 2.4856 sec.\n",
      "Iteration 2630 || Loss: 4.9320 || 10iter: 1.9710 sec.\n",
      "Iteration 2640 || Loss: 4.9828 || 10iter: 1.2898 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:320.6681 ||Epoch_VAL_Loss:158.9397\n",
      "timer:  20.8544 sec.\n",
      "-------------\n",
      "Epoch 41/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2650 || Loss: 4.5037 || 10iter: 5.5835 sec.\n",
      "Iteration 2660 || Loss: 4.6127 || 10iter: 2.2814 sec.\n",
      "Iteration 2670 || Loss: 4.4679 || 10iter: 2.2086 sec.\n",
      "Iteration 2680 || Loss: 4.7703 || 10iter: 2.5653 sec.\n",
      "Iteration 2690 || Loss: 5.0943 || 10iter: 2.3568 sec.\n",
      "Iteration 2700 || Loss: 4.8311 || 10iter: 1.6786 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:317.4536 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.8008 sec.\n",
      "-------------\n",
      "Epoch 42/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2710 || Loss: 4.8686 || 10iter: 3.7532 sec.\n",
      "Iteration 2720 || Loss: 4.9779 || 10iter: 2.5616 sec.\n",
      "Iteration 2730 || Loss: 4.9630 || 10iter: 2.3117 sec.\n",
      "Iteration 2740 || Loss: 5.0793 || 10iter: 2.1527 sec.\n",
      "Iteration 2750 || Loss: 5.1850 || 10iter: 2.3517 sec.\n",
      "Iteration 2760 || Loss: 5.1951 || 10iter: 2.0023 sec.\n",
      "Iteration 2770 || Loss: 5.1459 || 10iter: 1.4436 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:321.6005 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9685 sec.\n",
      "-------------\n",
      "Epoch 43/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2780 || Loss: 4.7890 || 10iter: 4.6137 sec.\n",
      "Iteration 2790 || Loss: 4.7135 || 10iter: 2.6868 sec.\n",
      "Iteration 2800 || Loss: 5.0905 || 10iter: 1.9508 sec.\n",
      "Iteration 2810 || Loss: 4.7638 || 10iter: 2.0704 sec.\n",
      "Iteration 2820 || Loss: 4.6105 || 10iter: 2.5058 sec.\n",
      "Iteration 2830 || Loss: 4.5280 || 10iter: 1.7027 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:317.3726 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7816 sec.\n",
      "-------------\n",
      "Epoch 44/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2840 || Loss: 4.7413 || 10iter: 2.7770 sec.\n",
      "Iteration 2850 || Loss: 4.5166 || 10iter: 3.0627 sec.\n",
      "Iteration 2860 || Loss: 4.5946 || 10iter: 2.3587 sec.\n",
      "Iteration 2870 || Loss: 5.1743 || 10iter: 2.2151 sec.\n",
      "Iteration 2880 || Loss: 4.5035 || 10iter: 2.3909 sec.\n",
      "Iteration 2890 || Loss: 4.5779 || 10iter: 2.2127 sec.\n",
      "Iteration 2900 || Loss: 4.7894 || 10iter: 1.3993 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:317.2392 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0532 sec.\n",
      "-------------\n",
      "Epoch 45/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2910 || Loss: 5.0840 || 10iter: 4.1834 sec.\n",
      "Iteration 2920 || Loss: 5.6374 || 10iter: 2.4964 sec.\n",
      "Iteration 2930 || Loss: 4.7623 || 10iter: 2.1298 sec.\n",
      "Iteration 2940 || Loss: 4.5837 || 10iter: 2.0524 sec.\n",
      "Iteration 2950 || Loss: 4.6639 || 10iter: 2.4980 sec.\n",
      "Iteration 2960 || Loss: 5.3805 || 10iter: 1.7672 sec.\n",
      "Iteration 2970 || Loss: 4.5271 || 10iter: 1.3054 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:315.9787 ||Epoch_VAL_Loss:159.3848\n",
      "timer:  20.5949 sec.\n",
      "-------------\n",
      "Epoch 46/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2980 || Loss: 4.5323 || 10iter: 5.5283 sec.\n",
      "Iteration 2990 || Loss: 4.7681 || 10iter: 2.1983 sec.\n",
      "Iteration 3000 || Loss: 4.7545 || 10iter: 2.1168 sec.\n",
      "Iteration 3010 || Loss: 4.5486 || 10iter: 2.4648 sec.\n",
      "Iteration 3020 || Loss: 5.0391 || 10iter: 2.3865 sec.\n",
      "Iteration 3030 || Loss: 4.7925 || 10iter: 1.5516 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:315.7667 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2483 sec.\n",
      "-------------\n",
      "Epoch 47/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3040 || Loss: 4.5487 || 10iter: 3.3754 sec.\n",
      "Iteration 3050 || Loss: 4.7655 || 10iter: 2.9189 sec.\n",
      "Iteration 3060 || Loss: 4.8659 || 10iter: 2.2000 sec.\n",
      "Iteration 3070 || Loss: 4.6554 || 10iter: 2.0861 sec.\n",
      "Iteration 3080 || Loss: 4.8772 || 10iter: 2.2848 sec.\n",
      "Iteration 3090 || Loss: 4.9792 || 10iter: 2.1367 sec.\n",
      "Iteration 3100 || Loss: 4.9854 || 10iter: 1.3919 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:316.4172 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7606 sec.\n",
      "-------------\n",
      "Epoch 48/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3110 || Loss: 4.8854 || 10iter: 4.9056 sec.\n",
      "Iteration 3120 || Loss: 4.7766 || 10iter: 2.4086 sec.\n",
      "Iteration 3130 || Loss: 4.7697 || 10iter: 2.2086 sec.\n",
      "Iteration 3140 || Loss: 4.5634 || 10iter: 2.1282 sec.\n",
      "Iteration 3150 || Loss: 4.5846 || 10iter: 2.4936 sec.\n",
      "Iteration 3160 || Loss: 4.5674 || 10iter: 1.6786 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:315.2171 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9987 sec.\n",
      "-------------\n",
      "Epoch 49/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3170 || Loss: 4.6062 || 10iter: 2.6457 sec.\n",
      "Iteration 3180 || Loss: 4.6204 || 10iter: 3.1826 sec.\n",
      "Iteration 3190 || Loss: 5.0596 || 10iter: 2.2804 sec.\n",
      "Iteration 3200 || Loss: 5.1558 || 10iter: 2.1644 sec.\n",
      "Iteration 3210 || Loss: 4.6287 || 10iter: 2.4992 sec.\n",
      "Iteration 3220 || Loss: 4.7430 || 10iter: 2.1633 sec.\n",
      "Iteration 3230 || Loss: 4.8803 || 10iter: 1.5112 sec.\n",
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:314.8426 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2115 sec.\n",
      "-------------\n",
      "Epoch 50/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3240 || Loss: 5.0369 || 10iter: 4.3845 sec.\n",
      "Iteration 3250 || Loss: 4.6279 || 10iter: 2.2927 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3260 || Loss: 4.8205 || 10iter: 2.3328 sec.\n",
      "Iteration 3270 || Loss: 4.7717 || 10iter: 2.2597 sec.\n",
      "Iteration 3280 || Loss: 4.8389 || 10iter: 2.5852 sec.\n",
      "Iteration 3290 || Loss: 4.4125 || 10iter: 1.7970 sec.\n",
      "Iteration 3300 || Loss: 4.3829 || 10iter: 1.3396 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:316.2955 ||Epoch_VAL_Loss:160.5629\n",
      "timer:  21.2334 sec.\n",
      "-------------\n",
      "Epoch 51/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3310 || Loss: 4.6881 || 10iter: 5.5153 sec.\n",
      "Iteration 3320 || Loss: 4.8523 || 10iter: 2.2427 sec.\n",
      "Iteration 3330 || Loss: 4.8341 || 10iter: 2.0781 sec.\n",
      "Iteration 3340 || Loss: 5.0084 || 10iter: 2.3230 sec.\n",
      "Iteration 3350 || Loss: 4.9174 || 10iter: 2.4714 sec.\n",
      "Iteration 3360 || Loss: 4.9544 || 10iter: 1.5800 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:317.6861 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1723 sec.\n",
      "-------------\n",
      "Epoch 52/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3370 || Loss: 5.0660 || 10iter: 3.7769 sec.\n",
      "Iteration 3380 || Loss: 4.8326 || 10iter: 3.0666 sec.\n",
      "Iteration 3390 || Loss: 4.4508 || 10iter: 2.3935 sec.\n",
      "Iteration 3400 || Loss: 5.0054 || 10iter: 2.1796 sec.\n",
      "Iteration 3410 || Loss: 4.2978 || 10iter: 2.2787 sec.\n",
      "Iteration 3420 || Loss: 4.9781 || 10iter: 1.9668 sec.\n",
      "Iteration 3430 || Loss: 4.9486 || 10iter: 1.4253 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:315.7138 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4724 sec.\n",
      "-------------\n",
      "Epoch 53/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3440 || Loss: 4.8550 || 10iter: 4.6796 sec.\n",
      "Iteration 3450 || Loss: 5.1809 || 10iter: 2.6374 sec.\n",
      "Iteration 3460 || Loss: 4.7452 || 10iter: 2.1307 sec.\n",
      "Iteration 3470 || Loss: 4.7015 || 10iter: 2.1793 sec.\n",
      "Iteration 3480 || Loss: 5.1641 || 10iter: 2.3527 sec.\n",
      "Iteration 3490 || Loss: 4.5515 || 10iter: 1.7074 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:319.3401 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8830 sec.\n",
      "-------------\n",
      "Epoch 54/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3500 || Loss: 5.2088 || 10iter: 3.1140 sec.\n",
      "Iteration 3510 || Loss: 4.8444 || 10iter: 2.6314 sec.\n",
      "Iteration 3520 || Loss: 4.6884 || 10iter: 2.1970 sec.\n",
      "Iteration 3530 || Loss: 5.1687 || 10iter: 2.2104 sec.\n",
      "Iteration 3540 || Loss: 4.6658 || 10iter: 2.2847 sec.\n",
      "Iteration 3550 || Loss: 4.9680 || 10iter: 2.1721 sec.\n",
      "Iteration 3560 || Loss: 4.6549 || 10iter: 1.4436 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:316.7422 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7738 sec.\n",
      "-------------\n",
      "Epoch 55/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3570 || Loss: 4.8784 || 10iter: 4.0527 sec.\n",
      "Iteration 3580 || Loss: 4.5627 || 10iter: 2.7392 sec.\n",
      "Iteration 3590 || Loss: 5.2151 || 10iter: 2.3457 sec.\n",
      "Iteration 3600 || Loss: 5.0567 || 10iter: 2.1125 sec.\n",
      "Iteration 3610 || Loss: 4.4252 || 10iter: 2.4792 sec.\n",
      "Iteration 3620 || Loss: 4.8477 || 10iter: 1.7335 sec.\n",
      "Iteration 3630 || Loss: 4.9904 || 10iter: 1.3558 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:312.5602 ||Epoch_VAL_Loss:157.7928\n",
      "timer:  20.9714 sec.\n",
      "-------------\n",
      "Epoch 56/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3640 || Loss: 4.7949 || 10iter: 5.1451 sec.\n",
      "Iteration 3650 || Loss: 4.8306 || 10iter: 2.6023 sec.\n",
      "Iteration 3660 || Loss: 4.5395 || 10iter: 1.9394 sec.\n",
      "Iteration 3670 || Loss: 4.5211 || 10iter: 2.2305 sec.\n",
      "Iteration 3680 || Loss: 4.5892 || 10iter: 2.5815 sec.\n",
      "Iteration 3690 || Loss: 4.9274 || 10iter: 1.4649 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:316.2563 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8589 sec.\n",
      "-------------\n",
      "Epoch 57/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3700 || Loss: 4.8606 || 10iter: 3.7983 sec.\n",
      "Iteration 3710 || Loss: 5.0289 || 10iter: 2.6897 sec.\n",
      "Iteration 3720 || Loss: 4.4320 || 10iter: 2.1606 sec.\n",
      "Iteration 3730 || Loss: 4.4124 || 10iter: 2.2468 sec.\n",
      "Iteration 3740 || Loss: 5.1920 || 10iter: 2.3999 sec.\n",
      "Iteration 3750 || Loss: 5.0452 || 10iter: 2.0784 sec.\n",
      "Iteration 3760 || Loss: 4.7636 || 10iter: 1.5637 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:315.2230 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4041 sec.\n",
      "-------------\n",
      "Epoch 58/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3770 || Loss: 5.0231 || 10iter: 4.7662 sec.\n",
      "Iteration 3780 || Loss: 4.7175 || 10iter: 2.4288 sec.\n",
      "Iteration 3790 || Loss: 4.7096 || 10iter: 2.3168 sec.\n",
      "Iteration 3800 || Loss: 4.2769 || 10iter: 2.2108 sec.\n",
      "Iteration 3810 || Loss: 4.6176 || 10iter: 2.3853 sec.\n",
      "Iteration 3820 || Loss: 5.1116 || 10iter: 1.7167 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:312.3402 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0852 sec.\n",
      "-------------\n",
      "Epoch 59/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3830 || Loss: 4.5551 || 10iter: 2.5306 sec.\n",
      "Iteration 3840 || Loss: 4.5455 || 10iter: 3.2187 sec.\n",
      "Iteration 3850 || Loss: 4.8684 || 10iter: 2.2543 sec.\n",
      "Iteration 3860 || Loss: 5.1171 || 10iter: 2.0922 sec.\n",
      "Iteration 3870 || Loss: 4.2062 || 10iter: 2.4542 sec.\n",
      "Iteration 3880 || Loss: 4.5841 || 10iter: 2.1978 sec.\n",
      "Iteration 3890 || Loss: 4.8751 || 10iter: 1.4641 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:310.3749 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8777 sec.\n",
      "-------------\n",
      "Epoch 60/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3900 || Loss: 4.6910 || 10iter: 4.1681 sec.\n",
      "Iteration 3910 || Loss: 4.6344 || 10iter: 2.5587 sec.\n",
      "Iteration 3920 || Loss: 4.8623 || 10iter: 2.2310 sec.\n",
      "Iteration 3930 || Loss: 4.8215 || 10iter: 2.1848 sec.\n",
      "Iteration 3940 || Loss: 4.3836 || 10iter: 2.3842 sec.\n",
      "Iteration 3950 || Loss: 5.2454 || 10iter: 1.8728 sec.\n",
      "Iteration 3960 || Loss: 5.0965 || 10iter: 1.3504 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:314.2619 ||Epoch_VAL_Loss:157.2004\n",
      "timer:  20.8951 sec.\n",
      "-------------\n",
      "Epoch 61/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3970 || Loss: 4.5987 || 10iter: 5.2768 sec.\n",
      "Iteration 3980 || Loss: 4.4719 || 10iter: 2.3703 sec.\n",
      "Iteration 3990 || Loss: 4.7945 || 10iter: 2.1983 sec.\n",
      "Iteration 4000 || Loss: 4.6279 || 10iter: 2.0970 sec.\n",
      "Iteration 4010 || Loss: 4.9199 || 10iter: 2.5545 sec.\n",
      "Iteration 4020 || Loss: 4.6779 || 10iter: 1.5292 sec.\n",
      "-------------\n",
      "epoch 61 || Epoch_TRAIN_Loss:309.7503 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9556 sec.\n",
      "-------------\n",
      "Epoch 62/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4030 || Loss: 4.7477 || 10iter: 3.8002 sec.\n",
      "Iteration 4040 || Loss: 4.7706 || 10iter: 3.1404 sec.\n",
      "Iteration 4050 || Loss: 4.1862 || 10iter: 2.7825 sec.\n",
      "Iteration 4060 || Loss: 4.9728 || 10iter: 2.1552 sec.\n",
      "Iteration 4070 || Loss: 4.6556 || 10iter: 2.4723 sec.\n",
      "Iteration 4080 || Loss: 4.4157 || 10iter: 2.0138 sec.\n",
      "Iteration 4090 || Loss: 5.0267 || 10iter: 1.5133 sec.\n",
      "-------------\n",
      "epoch 62 || Epoch_TRAIN_Loss:309.3078 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3460 sec.\n",
      "-------------\n",
      "Epoch 63/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4100 || Loss: 4.6027 || 10iter: 4.8998 sec.\n",
      "Iteration 4110 || Loss: 4.6941 || 10iter: 2.3056 sec.\n",
      "Iteration 4120 || Loss: 4.5065 || 10iter: 2.1302 sec.\n",
      "Iteration 4130 || Loss: 4.2292 || 10iter: 2.3319 sec.\n",
      "Iteration 4140 || Loss: 4.5039 || 10iter: 2.4644 sec.\n",
      "Iteration 4150 || Loss: 4.5709 || 10iter: 1.6055 sec.\n",
      "-------------\n",
      "epoch 63 || Epoch_TRAIN_Loss:312.5349 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9754 sec.\n",
      "-------------\n",
      "Epoch 64/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4160 || Loss: 4.8314 || 10iter: 2.5209 sec.\n",
      "Iteration 4170 || Loss: 4.3590 || 10iter: 3.2860 sec.\n",
      "Iteration 4180 || Loss: 4.9141 || 10iter: 2.2523 sec.\n",
      "Iteration 4190 || Loss: 4.4356 || 10iter: 2.0362 sec.\n",
      "Iteration 4200 || Loss: 4.6500 || 10iter: 2.2812 sec.\n",
      "Iteration 4210 || Loss: 5.2739 || 10iter: 2.1954 sec.\n",
      "Iteration 4220 || Loss: 4.6228 || 10iter: 1.3592 sec.\n",
      "-------------\n",
      "epoch 64 || Epoch_TRAIN_Loss:315.0081 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5768 sec.\n",
      "-------------\n",
      "Epoch 65/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4230 || Loss: 5.1290 || 10iter: 4.0164 sec.\n",
      "Iteration 4240 || Loss: 4.6273 || 10iter: 2.8016 sec.\n",
      "Iteration 4250 || Loss: 4.2903 || 10iter: 2.1301 sec.\n",
      "Iteration 4260 || Loss: 4.7838 || 10iter: 2.2695 sec.\n",
      "Iteration 4270 || Loss: 4.0119 || 10iter: 2.4850 sec.\n",
      "Iteration 4280 || Loss: 4.9352 || 10iter: 1.9318 sec.\n",
      "Iteration 4290 || Loss: 4.9372 || 10iter: 1.3560 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 65 || Epoch_TRAIN_Loss:310.3062 ||Epoch_VAL_Loss:157.6275\n",
      "timer:  21.1814 sec.\n",
      "-------------\n",
      "Epoch 66/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4300 || Loss: 4.5523 || 10iter: 5.4193 sec.\n",
      "Iteration 4310 || Loss: 4.5373 || 10iter: 2.2571 sec.\n",
      "Iteration 4320 || Loss: 4.9162 || 10iter: 2.2763 sec.\n",
      "Iteration 4330 || Loss: 4.8392 || 10iter: 2.2241 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4340 || Loss: 4.7246 || 10iter: 2.5413 sec.\n",
      "Iteration 4350 || Loss: 4.3699 || 10iter: 1.4315 sec.\n",
      "-------------\n",
      "epoch 66 || Epoch_TRAIN_Loss:312.4897 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0397 sec.\n",
      "-------------\n",
      "Epoch 67/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4360 || Loss: 4.5869 || 10iter: 3.7335 sec.\n",
      "Iteration 4370 || Loss: 4.8574 || 10iter: 2.3365 sec.\n",
      "Iteration 4380 || Loss: 5.0291 || 10iter: 2.3660 sec.\n",
      "Iteration 4390 || Loss: 4.5140 || 10iter: 2.1661 sec.\n",
      "Iteration 4400 || Loss: 4.6502 || 10iter: 2.3280 sec.\n",
      "Iteration 4410 || Loss: 4.8736 || 10iter: 1.9497 sec.\n",
      "Iteration 4420 || Loss: 4.6039 || 10iter: 1.4333 sec.\n",
      "-------------\n",
      "epoch 67 || Epoch_TRAIN_Loss:311.4416 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6899 sec.\n",
      "-------------\n",
      "Epoch 68/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4430 || Loss: 4.9156 || 10iter: 4.6087 sec.\n",
      "Iteration 4440 || Loss: 4.6256 || 10iter: 2.6268 sec.\n",
      "Iteration 4450 || Loss: 4.6531 || 10iter: 2.0691 sec.\n",
      "Iteration 4460 || Loss: 4.7710 || 10iter: 2.3273 sec.\n",
      "Iteration 4470 || Loss: 4.6132 || 10iter: 2.4410 sec.\n",
      "Iteration 4480 || Loss: 4.4912 || 10iter: 1.6478 sec.\n",
      "-------------\n",
      "epoch 68 || Epoch_TRAIN_Loss:313.2315 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9126 sec.\n",
      "-------------\n",
      "Epoch 69/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4490 || Loss: 5.2151 || 10iter: 3.1813 sec.\n",
      "Iteration 4500 || Loss: 4.6493 || 10iter: 2.6431 sec.\n",
      "Iteration 4510 || Loss: 5.2775 || 10iter: 2.3808 sec.\n",
      "Iteration 4520 || Loss: 4.3991 || 10iter: 2.0615 sec.\n",
      "Iteration 4530 || Loss: 4.6612 || 10iter: 2.4365 sec.\n",
      "Iteration 4540 || Loss: 5.2362 || 10iter: 2.2480 sec.\n",
      "Iteration 4550 || Loss: 5.1051 || 10iter: 1.3765 sec.\n",
      "-------------\n",
      "epoch 69 || Epoch_TRAIN_Loss:314.1329 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9301 sec.\n",
      "-------------\n",
      "Epoch 70/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4560 || Loss: 4.6534 || 10iter: 4.4551 sec.\n",
      "Iteration 4570 || Loss: 4.6922 || 10iter: 2.4039 sec.\n",
      "Iteration 4580 || Loss: 4.7353 || 10iter: 2.3986 sec.\n",
      "Iteration 4590 || Loss: 4.4477 || 10iter: 2.1172 sec.\n",
      "Iteration 4600 || Loss: 4.7659 || 10iter: 2.4212 sec.\n",
      "Iteration 4610 || Loss: 4.9545 || 10iter: 1.8759 sec.\n",
      "Iteration 4620 || Loss: 5.1738 || 10iter: 1.3829 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 70 || Epoch_TRAIN_Loss:311.6712 ||Epoch_VAL_Loss:156.8341\n",
      "timer:  21.1797 sec.\n",
      "-------------\n",
      "Epoch 71/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4630 || Loss: 4.8205 || 10iter: 5.3332 sec.\n",
      "Iteration 4640 || Loss: 4.7374 || 10iter: 2.5258 sec.\n",
      "Iteration 4650 || Loss: 4.8819 || 10iter: 2.0789 sec.\n",
      "Iteration 4660 || Loss: 4.6124 || 10iter: 2.3366 sec.\n",
      "Iteration 4670 || Loss: 4.7934 || 10iter: 2.2763 sec.\n",
      "Iteration 4680 || Loss: 4.3508 || 10iter: 1.4919 sec.\n",
      "-------------\n",
      "epoch 71 || Epoch_TRAIN_Loss:311.3408 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9356 sec.\n",
      "-------------\n",
      "Epoch 72/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4690 || Loss: 4.4645 || 10iter: 3.7821 sec.\n",
      "Iteration 4700 || Loss: 4.2943 || 10iter: 2.8597 sec.\n",
      "Iteration 4710 || Loss: 5.2096 || 10iter: 2.7086 sec.\n",
      "Iteration 4720 || Loss: 4.7155 || 10iter: 2.0514 sec.\n",
      "Iteration 4730 || Loss: 4.7617 || 10iter: 2.4362 sec.\n",
      "Iteration 4740 || Loss: 5.0785 || 10iter: 2.2030 sec.\n",
      "Iteration 4750 || Loss: 4.5680 || 10iter: 1.3684 sec.\n",
      "-------------\n",
      "epoch 72 || Epoch_TRAIN_Loss:312.2224 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.7924 sec.\n",
      "-------------\n",
      "Epoch 73/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4760 || Loss: 4.7630 || 10iter: 4.7799 sec.\n",
      "Iteration 4770 || Loss: 4.6653 || 10iter: 2.4455 sec.\n",
      "Iteration 4780 || Loss: 4.8767 || 10iter: 2.1102 sec.\n",
      "Iteration 4790 || Loss: 4.6084 || 10iter: 2.3538 sec.\n",
      "Iteration 4800 || Loss: 4.5574 || 10iter: 2.4297 sec.\n",
      "Iteration 4810 || Loss: 4.3954 || 10iter: 1.7546 sec.\n",
      "-------------\n",
      "epoch 73 || Epoch_TRAIN_Loss:309.3643 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2009 sec.\n",
      "-------------\n",
      "Epoch 74/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4820 || Loss: 4.4724 || 10iter: 2.8846 sec.\n",
      "Iteration 4830 || Loss: 4.4334 || 10iter: 2.5922 sec.\n",
      "Iteration 4840 || Loss: 4.6535 || 10iter: 2.2995 sec.\n",
      "Iteration 4850 || Loss: 4.3269 || 10iter: 2.0776 sec.\n",
      "Iteration 4860 || Loss: 4.5125 || 10iter: 2.4428 sec.\n",
      "Iteration 4870 || Loss: 4.7443 || 10iter: 2.0429 sec.\n",
      "Iteration 4880 || Loss: 4.7317 || 10iter: 1.4112 sec.\n",
      "-------------\n",
      "epoch 74 || Epoch_TRAIN_Loss:308.3547 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4315 sec.\n",
      "-------------\n",
      "Epoch 75/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4890 || Loss: 4.7138 || 10iter: 4.2431 sec.\n",
      "Iteration 4900 || Loss: 4.5511 || 10iter: 2.5494 sec.\n",
      "Iteration 4910 || Loss: 4.7889 || 10iter: 2.2332 sec.\n",
      "Iteration 4920 || Loss: 4.4055 || 10iter: 2.2433 sec.\n",
      "Iteration 4930 || Loss: 4.7327 || 10iter: 2.6247 sec.\n",
      "Iteration 4940 || Loss: 5.2935 || 10iter: 1.7183 sec.\n",
      "Iteration 4950 || Loss: 4.2816 || 10iter: 1.3554 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 75 || Epoch_TRAIN_Loss:310.4135 ||Epoch_VAL_Loss:155.6165\n",
      "timer:  21.1459 sec.\n",
      "-------------\n",
      "Epoch 76/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4960 || Loss: 5.1766 || 10iter: 5.4464 sec.\n",
      "Iteration 4970 || Loss: 4.9996 || 10iter: 2.2611 sec.\n",
      "Iteration 4980 || Loss: 4.3692 || 10iter: 2.1497 sec.\n",
      "Iteration 4990 || Loss: 4.5676 || 10iter: 2.3330 sec.\n",
      "Iteration 5000 || Loss: 4.7747 || 10iter: 2.3326 sec.\n",
      "Iteration 5010 || Loss: 4.7298 || 10iter: 1.6534 sec.\n",
      "-------------\n",
      "epoch 76 || Epoch_TRAIN_Loss:309.0645 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2426 sec.\n",
      "-------------\n",
      "Epoch 77/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5020 || Loss: 5.1278 || 10iter: 3.5367 sec.\n",
      "Iteration 5030 || Loss: 4.3727 || 10iter: 2.8945 sec.\n",
      "Iteration 5040 || Loss: 4.4112 || 10iter: 2.3712 sec.\n",
      "Iteration 5050 || Loss: 4.4180 || 10iter: 2.1507 sec.\n",
      "Iteration 5060 || Loss: 5.0791 || 10iter: 2.4007 sec.\n",
      "Iteration 5070 || Loss: 4.6472 || 10iter: 2.0552 sec.\n",
      "Iteration 5080 || Loss: 5.0643 || 10iter: 1.4059 sec.\n",
      "-------------\n",
      "epoch 77 || Epoch_TRAIN_Loss:310.9817 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1935 sec.\n",
      "-------------\n",
      "Epoch 78/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5090 || Loss: 4.3817 || 10iter: 4.9398 sec.\n",
      "Iteration 5100 || Loss: 4.5682 || 10iter: 2.3924 sec.\n",
      "Iteration 5110 || Loss: 4.2303 || 10iter: 2.1357 sec.\n",
      "Iteration 5120 || Loss: 4.4695 || 10iter: 2.3357 sec.\n",
      "Iteration 5130 || Loss: 4.7254 || 10iter: 2.5443 sec.\n",
      "Iteration 5140 || Loss: 5.1889 || 10iter: 1.7182 sec.\n",
      "-------------\n",
      "epoch 78 || Epoch_TRAIN_Loss:307.5240 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.3463 sec.\n",
      "-------------\n",
      "Epoch 79/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5150 || Loss: 4.7789 || 10iter: 2.9821 sec.\n",
      "Iteration 5160 || Loss: 4.8335 || 10iter: 2.7312 sec.\n",
      "Iteration 5170 || Loss: 5.0423 || 10iter: 2.3866 sec.\n",
      "Iteration 5180 || Loss: 4.5908 || 10iter: 2.1403 sec.\n",
      "Iteration 5190 || Loss: 4.7053 || 10iter: 2.3349 sec.\n",
      "Iteration 5200 || Loss: 4.7347 || 10iter: 2.1753 sec.\n",
      "Iteration 5210 || Loss: 4.9090 || 10iter: 1.3761 sec.\n",
      "-------------\n",
      "epoch 79 || Epoch_TRAIN_Loss:307.7790 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7790 sec.\n",
      "-------------\n",
      "Epoch 80/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5220 || Loss: 4.6144 || 10iter: 3.9006 sec.\n",
      "Iteration 5230 || Loss: 4.6183 || 10iter: 2.7745 sec.\n",
      "Iteration 5240 || Loss: 4.8643 || 10iter: 2.3619 sec.\n",
      "Iteration 5250 || Loss: 4.2246 || 10iter: 1.9914 sec.\n",
      "Iteration 5260 || Loss: 4.9626 || 10iter: 2.4197 sec.\n",
      "Iteration 5270 || Loss: 4.8388 || 10iter: 1.8789 sec.\n",
      "Iteration 5280 || Loss: 4.2420 || 10iter: 1.3152 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 80 || Epoch_TRAIN_Loss:307.4899 ||Epoch_VAL_Loss:156.7722\n",
      "timer:  20.8057 sec.\n",
      "-------------\n",
      "Epoch 81/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5290 || Loss: 4.5132 || 10iter: 5.5598 sec.\n",
      "Iteration 5300 || Loss: 4.6669 || 10iter: 2.3853 sec.\n",
      "Iteration 5310 || Loss: 4.6458 || 10iter: 2.1596 sec.\n",
      "Iteration 5320 || Loss: 4.8026 || 10iter: 2.4290 sec.\n",
      "Iteration 5330 || Loss: 4.9112 || 10iter: 2.3195 sec.\n",
      "Iteration 5340 || Loss: 4.7279 || 10iter: 1.6363 sec.\n",
      "-------------\n",
      "epoch 81 || Epoch_TRAIN_Loss:308.9428 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.5328 sec.\n",
      "-------------\n",
      "Epoch 82/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5350 || Loss: 4.8715 || 10iter: 3.7698 sec.\n",
      "Iteration 5360 || Loss: 4.4646 || 10iter: 3.0125 sec.\n",
      "Iteration 5370 || Loss: 4.5349 || 10iter: 2.7318 sec.\n",
      "Iteration 5380 || Loss: 4.5502 || 10iter: 2.2083 sec.\n",
      "Iteration 5390 || Loss: 4.5796 || 10iter: 2.3636 sec.\n",
      "Iteration 5400 || Loss: 4.8216 || 10iter: 2.1320 sec.\n",
      "Iteration 5410 || Loss: 4.6206 || 10iter: 1.4474 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "epoch 82 || Epoch_TRAIN_Loss:309.7373 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.0307 sec.\n",
      "-------------\n",
      "Epoch 83/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5420 || Loss: 4.7697 || 10iter: 4.9633 sec.\n",
      "Iteration 5430 || Loss: 5.0441 || 10iter: 2.3998 sec.\n",
      "Iteration 5440 || Loss: 4.7789 || 10iter: 2.1568 sec.\n",
      "Iteration 5450 || Loss: 4.5560 || 10iter: 2.4892 sec.\n",
      "Iteration 5460 || Loss: 4.8104 || 10iter: 2.4033 sec.\n",
      "Iteration 5470 || Loss: 4.5592 || 10iter: 1.8012 sec.\n",
      "-------------\n",
      "epoch 83 || Epoch_TRAIN_Loss:308.4125 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4567 sec.\n",
      "-------------\n",
      "Epoch 84/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5480 || Loss: 4.3788 || 10iter: 2.8583 sec.\n",
      "Iteration 5490 || Loss: 4.5880 || 10iter: 2.8561 sec.\n",
      "Iteration 5500 || Loss: 4.6354 || 10iter: 2.2688 sec.\n",
      "Iteration 5510 || Loss: 4.5599 || 10iter: 2.1345 sec.\n",
      "Iteration 5520 || Loss: 4.8535 || 10iter: 2.2668 sec.\n",
      "Iteration 5530 || Loss: 4.7059 || 10iter: 2.2144 sec.\n",
      "Iteration 5540 || Loss: 4.4593 || 10iter: 1.3694 sec.\n",
      "-------------\n",
      "epoch 84 || Epoch_TRAIN_Loss:306.5827 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6039 sec.\n",
      "-------------\n",
      "Epoch 85/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5550 || Loss: 4.4341 || 10iter: 4.0509 sec.\n",
      "Iteration 5560 || Loss: 4.7649 || 10iter: 2.5518 sec.\n",
      "Iteration 5570 || Loss: 4.5870 || 10iter: 2.4511 sec.\n",
      "Iteration 5580 || Loss: 4.4747 || 10iter: 2.0644 sec.\n",
      "Iteration 5590 || Loss: 5.2678 || 10iter: 2.4028 sec.\n",
      "Iteration 5600 || Loss: 4.7692 || 10iter: 1.8562 sec.\n",
      "Iteration 5610 || Loss: 4.6726 || 10iter: 1.3120 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 85 || Epoch_TRAIN_Loss:305.8133 ||Epoch_VAL_Loss:155.5386\n",
      "timer:  20.7297 sec.\n",
      "-------------\n",
      "Epoch 86/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5620 || Loss: 4.5869 || 10iter: 5.4587 sec.\n",
      "Iteration 5630 || Loss: 4.7804 || 10iter: 2.2608 sec.\n",
      "Iteration 5640 || Loss: 4.7459 || 10iter: 2.1015 sec.\n",
      "Iteration 5650 || Loss: 4.8605 || 10iter: 2.2265 sec.\n",
      "Iteration 5660 || Loss: 4.5990 || 10iter: 2.3673 sec.\n",
      "Iteration 5670 || Loss: 4.9390 || 10iter: 1.5923 sec.\n",
      "-------------\n",
      "epoch 86 || Epoch_TRAIN_Loss:308.4657 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0327 sec.\n",
      "-------------\n",
      "Epoch 87/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5680 || Loss: 4.3651 || 10iter: 3.5957 sec.\n",
      "Iteration 5690 || Loss: 4.6873 || 10iter: 2.8630 sec.\n",
      "Iteration 5700 || Loss: 4.2731 || 10iter: 2.0782 sec.\n",
      "Iteration 5710 || Loss: 4.9907 || 10iter: 2.2393 sec.\n",
      "Iteration 5720 || Loss: 4.5020 || 10iter: 2.4205 sec.\n",
      "Iteration 5730 || Loss: 4.7117 || 10iter: 1.9324 sec.\n",
      "Iteration 5740 || Loss: 4.6238 || 10iter: 1.3979 sec.\n",
      "-------------\n",
      "epoch 87 || Epoch_TRAIN_Loss:306.8338 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9428 sec.\n",
      "-------------\n",
      "Epoch 88/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5750 || Loss: 4.7949 || 10iter: 4.6874 sec.\n",
      "Iteration 5760 || Loss: 4.4487 || 10iter: 2.5186 sec.\n",
      "Iteration 5770 || Loss: 4.6196 || 10iter: 2.1135 sec.\n",
      "Iteration 5780 || Loss: 4.4496 || 10iter: 2.1105 sec.\n",
      "Iteration 5790 || Loss: 4.5245 || 10iter: 2.5110 sec.\n",
      "Iteration 5800 || Loss: 4.9943 || 10iter: 1.6431 sec.\n",
      "-------------\n",
      "epoch 88 || Epoch_TRAIN_Loss:306.6552 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7270 sec.\n",
      "-------------\n",
      "Epoch 89/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5810 || Loss: 4.6797 || 10iter: 2.5845 sec.\n",
      "Iteration 5820 || Loss: 4.6180 || 10iter: 3.2523 sec.\n",
      "Iteration 5830 || Loss: 4.5616 || 10iter: 2.2906 sec.\n",
      "Iteration 5840 || Loss: 4.4927 || 10iter: 2.0786 sec.\n",
      "Iteration 5850 || Loss: 4.2793 || 10iter: 2.5135 sec.\n",
      "Iteration 5860 || Loss: 4.7524 || 10iter: 2.1688 sec.\n",
      "Iteration 5870 || Loss: 4.4057 || 10iter: 1.5435 sec.\n",
      "-------------\n",
      "epoch 89 || Epoch_TRAIN_Loss:307.8277 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1710 sec.\n",
      "-------------\n",
      "Epoch 90/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5880 || Loss: 4.5782 || 10iter: 3.9740 sec.\n",
      "Iteration 5890 || Loss: 4.4340 || 10iter: 2.6657 sec.\n",
      "Iteration 5900 || Loss: 4.6276 || 10iter: 2.2968 sec.\n",
      "Iteration 5910 || Loss: 4.6268 || 10iter: 2.2400 sec.\n",
      "Iteration 5920 || Loss: 5.0523 || 10iter: 2.4318 sec.\n",
      "Iteration 5930 || Loss: 4.3056 || 10iter: 1.8173 sec.\n",
      "Iteration 5940 || Loss: 4.3866 || 10iter: 1.3405 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 90 || Epoch_TRAIN_Loss:303.4294 ||Epoch_VAL_Loss:154.3453\n",
      "timer:  21.0846 sec.\n",
      "-------------\n",
      "Epoch 91/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5950 || Loss: 4.3556 || 10iter: 5.5221 sec.\n",
      "Iteration 5960 || Loss: 4.2281 || 10iter: 2.0736 sec.\n",
      "Iteration 5970 || Loss: 4.4651 || 10iter: 2.3667 sec.\n",
      "Iteration 5980 || Loss: 4.8368 || 10iter: 2.2456 sec.\n",
      "Iteration 5990 || Loss: 5.1193 || 10iter: 2.2893 sec.\n",
      "Iteration 6000 || Loss: 4.3774 || 10iter: 1.4438 sec.\n",
      "-------------\n",
      "epoch 91 || Epoch_TRAIN_Loss:306.9966 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8554 sec.\n",
      "-------------\n",
      "Epoch 92/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6010 || Loss: 4.5910 || 10iter: 3.6323 sec.\n",
      "Iteration 6020 || Loss: 4.5183 || 10iter: 3.0629 sec.\n",
      "Iteration 6030 || Loss: 4.7522 || 10iter: 2.8493 sec.\n",
      "Iteration 6040 || Loss: 4.9495 || 10iter: 2.4359 sec.\n",
      "Iteration 6050 || Loss: 4.7136 || 10iter: 2.6251 sec.\n",
      "Iteration 6060 || Loss: 5.0205 || 10iter: 2.0526 sec.\n",
      "Iteration 6070 || Loss: 4.6386 || 10iter: 1.4356 sec.\n",
      "-------------\n",
      "epoch 92 || Epoch_TRAIN_Loss:307.7026 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5228 sec.\n",
      "-------------\n",
      "Epoch 93/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6080 || Loss: 4.2526 || 10iter: 4.8546 sec.\n",
      "Iteration 6090 || Loss: 4.8466 || 10iter: 2.5587 sec.\n",
      "Iteration 6100 || Loss: 5.0260 || 10iter: 1.9872 sec.\n",
      "Iteration 6110 || Loss: 4.8923 || 10iter: 2.2174 sec.\n",
      "Iteration 6120 || Loss: 4.2767 || 10iter: 2.3832 sec.\n",
      "Iteration 6130 || Loss: 4.7517 || 10iter: 1.6681 sec.\n",
      "-------------\n",
      "epoch 93 || Epoch_TRAIN_Loss:306.3091 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8332 sec.\n",
      "-------------\n",
      "Epoch 94/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6140 || Loss: 4.4608 || 10iter: 2.9749 sec.\n",
      "Iteration 6150 || Loss: 4.4684 || 10iter: 2.8892 sec.\n",
      "Iteration 6160 || Loss: 4.2137 || 10iter: 2.2861 sec.\n",
      "Iteration 6170 || Loss: 4.6755 || 10iter: 1.9798 sec.\n",
      "Iteration 6180 || Loss: 4.6534 || 10iter: 2.5108 sec.\n",
      "Iteration 6190 || Loss: 4.8363 || 10iter: 2.2313 sec.\n",
      "Iteration 6200 || Loss: 4.7827 || 10iter: 1.4533 sec.\n",
      "-------------\n",
      "epoch 94 || Epoch_TRAIN_Loss:304.3590 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9854 sec.\n",
      "-------------\n",
      "Epoch 95/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6210 || Loss: 4.5476 || 10iter: 4.3186 sec.\n",
      "Iteration 6220 || Loss: 4.7569 || 10iter: 2.4246 sec.\n",
      "Iteration 6230 || Loss: 4.4955 || 10iter: 2.1013 sec.\n",
      "Iteration 6240 || Loss: 4.6662 || 10iter: 2.0498 sec.\n",
      "Iteration 6250 || Loss: 4.7245 || 10iter: 2.5578 sec.\n",
      "Iteration 6260 || Loss: 4.5479 || 10iter: 1.9149 sec.\n",
      "Iteration 6270 || Loss: 4.4470 || 10iter: 1.3134 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 95 || Epoch_TRAIN_Loss:306.0269 ||Epoch_VAL_Loss:153.2355\n",
      "timer:  20.7746 sec.\n",
      "-------------\n",
      "Epoch 96/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6280 || Loss: 4.4505 || 10iter: 5.3626 sec.\n",
      "Iteration 6290 || Loss: 4.7172 || 10iter: 2.3614 sec.\n",
      "Iteration 6300 || Loss: 4.4257 || 10iter: 2.0600 sec.\n",
      "Iteration 6310 || Loss: 4.9967 || 10iter: 2.4267 sec.\n",
      "Iteration 6320 || Loss: 5.0613 || 10iter: 2.3171 sec.\n",
      "Iteration 6330 || Loss: 4.4573 || 10iter: 1.4816 sec.\n",
      "-------------\n",
      "epoch 96 || Epoch_TRAIN_Loss:303.7191 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9292 sec.\n",
      "-------------\n",
      "Epoch 97/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6340 || Loss: 4.6447 || 10iter: 3.8123 sec.\n",
      "Iteration 6350 || Loss: 4.4248 || 10iter: 2.5290 sec.\n",
      "Iteration 6360 || Loss: 5.0942 || 10iter: 2.2402 sec.\n",
      "Iteration 6370 || Loss: 4.5138 || 10iter: 2.1504 sec.\n",
      "Iteration 6380 || Loss: 4.4201 || 10iter: 2.4547 sec.\n",
      "Iteration 6390 || Loss: 4.7651 || 10iter: 2.1585 sec.\n",
      "Iteration 6400 || Loss: 4.9134 || 10iter: 1.5103 sec.\n",
      "-------------\n",
      "epoch 97 || Epoch_TRAIN_Loss:306.5941 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.3620 sec.\n",
      "-------------\n",
      "Epoch 98/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6410 || Loss: 4.9451 || 10iter: 4.7335 sec.\n",
      "Iteration 6420 || Loss: 4.7946 || 10iter: 2.4843 sec.\n",
      "Iteration 6430 || Loss: 4.2962 || 10iter: 2.2220 sec.\n",
      "Iteration 6440 || Loss: 4.2482 || 10iter: 2.1244 sec.\n",
      "Iteration 6450 || Loss: 4.5797 || 10iter: 2.6247 sec.\n",
      "Iteration 6460 || Loss: 4.4549 || 10iter: 1.6864 sec.\n",
      "-------------\n",
      "epoch 98 || Epoch_TRAIN_Loss:305.2583 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1262 sec.\n",
      "-------------\n",
      "Epoch 99/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6470 || Loss: 4.4956 || 10iter: 2.9507 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6480 || Loss: 4.7884 || 10iter: 3.0408 sec.\n",
      "Iteration 6490 || Loss: 4.9510 || 10iter: 2.1804 sec.\n",
      "Iteration 6500 || Loss: 4.5493 || 10iter: 2.1051 sec.\n",
      "Iteration 6510 || Loss: 4.6512 || 10iter: 2.4549 sec.\n",
      "Iteration 6520 || Loss: 4.8263 || 10iter: 2.3281 sec.\n",
      "Iteration 6530 || Loss: 4.5884 || 10iter: 1.4813 sec.\n",
      "-------------\n",
      "epoch 99 || Epoch_TRAIN_Loss:305.6786 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2216 sec.\n",
      "-------------\n",
      "Epoch 100/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6540 || Loss: 4.2910 || 10iter: 4.1422 sec.\n",
      "Iteration 6550 || Loss: 4.5101 || 10iter: 2.5436 sec.\n",
      "Iteration 6560 || Loss: 4.3623 || 10iter: 2.2609 sec.\n",
      "Iteration 6570 || Loss: 4.6787 || 10iter: 1.9871 sec.\n",
      "Iteration 6580 || Loss: 4.4590 || 10iter: 2.6039 sec.\n",
      "Iteration 6590 || Loss: 4.5930 || 10iter: 1.8111 sec.\n",
      "Iteration 6600 || Loss: 4.5495 || 10iter: 1.3175 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 100 || Epoch_TRAIN_Loss:306.7189 ||Epoch_VAL_Loss:152.8684\n",
      "timer:  20.8388 sec.\n",
      "-------------\n",
      "Epoch 101/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6610 || Loss: 4.5316 || 10iter: 5.3982 sec.\n",
      "Iteration 6620 || Loss: 4.7617 || 10iter: 2.1865 sec.\n",
      "Iteration 6630 || Loss: 4.4623 || 10iter: 2.1300 sec.\n",
      "Iteration 6640 || Loss: 4.3660 || 10iter: 2.1945 sec.\n",
      "Iteration 6650 || Loss: 5.0197 || 10iter: 2.4141 sec.\n",
      "Iteration 6660 || Loss: 4.6162 || 10iter: 1.5279 sec.\n",
      "-------------\n",
      "epoch 101 || Epoch_TRAIN_Loss:305.0762 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7500 sec.\n",
      "-------------\n",
      "Epoch 102/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6670 || Loss: 4.7859 || 10iter: 3.5814 sec.\n",
      "Iteration 6680 || Loss: 4.7354 || 10iter: 3.0257 sec.\n",
      "Iteration 6690 || Loss: 4.4545 || 10iter: 2.7792 sec.\n",
      "Iteration 6700 || Loss: 4.7083 || 10iter: 2.4394 sec.\n",
      "Iteration 6710 || Loss: 4.5882 || 10iter: 2.3056 sec.\n",
      "Iteration 6720 || Loss: 5.0224 || 10iter: 2.1213 sec.\n",
      "Iteration 6730 || Loss: 4.6907 || 10iter: 1.4504 sec.\n",
      "-------------\n",
      "epoch 102 || Epoch_TRAIN_Loss:303.5704 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1147 sec.\n",
      "-------------\n",
      "Epoch 103/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6740 || Loss: 4.4458 || 10iter: 4.7869 sec.\n",
      "Iteration 6750 || Loss: 4.3470 || 10iter: 2.2687 sec.\n",
      "Iteration 6760 || Loss: 4.6186 || 10iter: 2.3935 sec.\n",
      "Iteration 6770 || Loss: 4.6891 || 10iter: 1.9784 sec.\n",
      "Iteration 6780 || Loss: 4.4022 || 10iter: 2.3820 sec.\n",
      "Iteration 6790 || Loss: 4.6878 || 10iter: 1.6705 sec.\n",
      "-------------\n",
      "epoch 103 || Epoch_TRAIN_Loss:304.8644 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6564 sec.\n",
      "-------------\n",
      "Epoch 104/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6800 || Loss: 4.8476 || 10iter: 2.8746 sec.\n",
      "Iteration 6810 || Loss: 4.6872 || 10iter: 2.8342 sec.\n",
      "Iteration 6820 || Loss: 4.5186 || 10iter: 2.3327 sec.\n",
      "Iteration 6830 || Loss: 4.5749 || 10iter: 2.1659 sec.\n",
      "Iteration 6840 || Loss: 4.5546 || 10iter: 2.4089 sec.\n",
      "Iteration 6850 || Loss: 4.9338 || 10iter: 2.2978 sec.\n",
      "Iteration 6860 || Loss: 4.3217 || 10iter: 1.3648 sec.\n",
      "-------------\n",
      "epoch 104 || Epoch_TRAIN_Loss:305.6524 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9173 sec.\n",
      "-------------\n",
      "Epoch 105/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6870 || Loss: 4.7817 || 10iter: 4.2868 sec.\n",
      "Iteration 6880 || Loss: 4.7496 || 10iter: 2.5063 sec.\n",
      "Iteration 6890 || Loss: 4.8650 || 10iter: 2.2860 sec.\n",
      "Iteration 6900 || Loss: 4.7589 || 10iter: 2.0333 sec.\n",
      "Iteration 6910 || Loss: 4.4692 || 10iter: 2.5177 sec.\n",
      "Iteration 6920 || Loss: 4.5887 || 10iter: 1.9031 sec.\n",
      "Iteration 6930 || Loss: 4.9822 || 10iter: 1.3799 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 105 || Epoch_TRAIN_Loss:305.4632 ||Epoch_VAL_Loss:152.5230\n",
      "timer:  21.1413 sec.\n",
      "-------------\n",
      "Epoch 106/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6940 || Loss: 4.7598 || 10iter: 5.4154 sec.\n",
      "Iteration 6950 || Loss: 4.6551 || 10iter: 2.2234 sec.\n",
      "Iteration 6960 || Loss: 4.8446 || 10iter: 2.1127 sec.\n",
      "Iteration 6970 || Loss: 4.4357 || 10iter: 2.1646 sec.\n",
      "Iteration 6980 || Loss: 4.8669 || 10iter: 2.4143 sec.\n",
      "Iteration 6990 || Loss: 4.8283 || 10iter: 1.4882 sec.\n",
      "-------------\n",
      "epoch 106 || Epoch_TRAIN_Loss:302.0929 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7191 sec.\n",
      "-------------\n",
      "Epoch 107/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7000 || Loss: 4.4199 || 10iter: 3.5789 sec.\n",
      "Iteration 7010 || Loss: 5.0019 || 10iter: 2.7351 sec.\n",
      "Iteration 7020 || Loss: 4.4446 || 10iter: 2.3602 sec.\n",
      "Iteration 7030 || Loss: 4.9053 || 10iter: 2.1097 sec.\n",
      "Iteration 7040 || Loss: 4.5073 || 10iter: 2.3738 sec.\n",
      "Iteration 7050 || Loss: 4.5170 || 10iter: 2.0490 sec.\n",
      "Iteration 7060 || Loss: 4.6857 || 10iter: 1.3574 sec.\n",
      "-------------\n",
      "epoch 107 || Epoch_TRAIN_Loss:306.6507 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9194 sec.\n",
      "-------------\n",
      "Epoch 108/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7070 || Loss: 4.5594 || 10iter: 4.9896 sec.\n",
      "Iteration 7080 || Loss: 4.9910 || 10iter: 2.3805 sec.\n",
      "Iteration 7090 || Loss: 4.5011 || 10iter: 2.2985 sec.\n",
      "Iteration 7100 || Loss: 4.5433 || 10iter: 2.2571 sec.\n",
      "Iteration 7110 || Loss: 4.9598 || 10iter: 2.4135 sec.\n",
      "Iteration 7120 || Loss: 4.3146 || 10iter: 1.7708 sec.\n",
      "-------------\n",
      "epoch 108 || Epoch_TRAIN_Loss:305.1552 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4994 sec.\n",
      "-------------\n",
      "Epoch 109/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7130 || Loss: 4.4305 || 10iter: 2.9386 sec.\n",
      "Iteration 7140 || Loss: 4.3297 || 10iter: 3.0160 sec.\n",
      "Iteration 7150 || Loss: 4.6067 || 10iter: 2.1759 sec.\n",
      "Iteration 7160 || Loss: 4.4605 || 10iter: 2.0246 sec.\n",
      "Iteration 7170 || Loss: 4.4440 || 10iter: 2.4855 sec.\n",
      "Iteration 7180 || Loss: 4.4570 || 10iter: 2.3087 sec.\n",
      "Iteration 7190 || Loss: 4.4044 || 10iter: 1.4356 sec.\n",
      "-------------\n",
      "epoch 109 || Epoch_TRAIN_Loss:303.4795 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0662 sec.\n",
      "-------------\n",
      "Epoch 110/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7200 || Loss: 4.7726 || 10iter: 4.2175 sec.\n",
      "Iteration 7210 || Loss: 4.2295 || 10iter: 2.5144 sec.\n",
      "Iteration 7220 || Loss: 4.3825 || 10iter: 2.3180 sec.\n",
      "Iteration 7230 || Loss: 4.4207 || 10iter: 2.0884 sec.\n",
      "Iteration 7240 || Loss: 4.5602 || 10iter: 2.4870 sec.\n",
      "Iteration 7250 || Loss: 4.4672 || 10iter: 1.9358 sec.\n",
      "Iteration 7260 || Loss: 4.4655 || 10iter: 1.3575 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 110 || Epoch_TRAIN_Loss:304.0058 ||Epoch_VAL_Loss:153.0866\n",
      "timer:  21.0302 sec.\n",
      "-------------\n",
      "Epoch 111/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7270 || Loss: 4.7140 || 10iter: 5.5571 sec.\n",
      "Iteration 7280 || Loss: 4.7468 || 10iter: 2.2498 sec.\n",
      "Iteration 7290 || Loss: 4.2770 || 10iter: 2.2243 sec.\n",
      "Iteration 7300 || Loss: 4.4369 || 10iter: 2.2417 sec.\n",
      "Iteration 7310 || Loss: 4.8590 || 10iter: 2.3475 sec.\n",
      "Iteration 7320 || Loss: 4.6311 || 10iter: 1.4888 sec.\n",
      "-------------\n",
      "epoch 111 || Epoch_TRAIN_Loss:301.9409 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0307 sec.\n",
      "-------------\n",
      "Epoch 112/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7330 || Loss: 4.4091 || 10iter: 3.7198 sec.\n",
      "Iteration 7340 || Loss: 4.7141 || 10iter: 2.4231 sec.\n",
      "Iteration 7350 || Loss: 4.5945 || 10iter: 2.8202 sec.\n",
      "Iteration 7360 || Loss: 4.6882 || 10iter: 2.4096 sec.\n",
      "Iteration 7370 || Loss: 4.4988 || 10iter: 2.6441 sec.\n",
      "Iteration 7380 || Loss: 4.4683 || 10iter: 2.0741 sec.\n",
      "Iteration 7390 || Loss: 4.6674 || 10iter: 1.3789 sec.\n",
      "-------------\n",
      "epoch 112 || Epoch_TRAIN_Loss:302.5629 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.8028 sec.\n",
      "-------------\n",
      "Epoch 113/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7400 || Loss: 5.1215 || 10iter: 4.8575 sec.\n",
      "Iteration 7410 || Loss: 5.2068 || 10iter: 2.7430 sec.\n",
      "Iteration 7420 || Loss: 4.4216 || 10iter: 2.0472 sec.\n",
      "Iteration 7430 || Loss: 4.4247 || 10iter: 2.2233 sec.\n",
      "Iteration 7440 || Loss: 4.0583 || 10iter: 2.5560 sec.\n",
      "Iteration 7450 || Loss: 4.5515 || 10iter: 1.7769 sec.\n",
      "-------------\n",
      "epoch 113 || Epoch_TRAIN_Loss:304.4297 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4683 sec.\n",
      "-------------\n",
      "Epoch 114/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7460 || Loss: 4.4635 || 10iter: 2.8197 sec.\n",
      "Iteration 7470 || Loss: 4.4172 || 10iter: 3.1506 sec.\n",
      "Iteration 7480 || Loss: 4.6063 || 10iter: 2.2050 sec.\n",
      "Iteration 7490 || Loss: 4.6715 || 10iter: 2.1070 sec.\n",
      "Iteration 7500 || Loss: 4.5447 || 10iter: 2.3287 sec.\n",
      "Iteration 7510 || Loss: 4.9247 || 10iter: 2.1794 sec.\n",
      "Iteration 7520 || Loss: 4.3838 || 10iter: 1.4266 sec.\n",
      "-------------\n",
      "epoch 114 || Epoch_TRAIN_Loss:305.7293 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8778 sec.\n",
      "-------------\n",
      "Epoch 115/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7530 || Loss: 4.7731 || 10iter: 3.9463 sec.\n",
      "Iteration 7540 || Loss: 4.6258 || 10iter: 2.7915 sec.\n",
      "Iteration 7550 || Loss: 4.6498 || 10iter: 2.1302 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7560 || Loss: 4.5154 || 10iter: 2.1321 sec.\n",
      "Iteration 7570 || Loss: 4.8515 || 10iter: 2.1889 sec.\n",
      "Iteration 7580 || Loss: 4.4173 || 10iter: 1.8869 sec.\n",
      "Iteration 7590 || Loss: 4.4182 || 10iter: 1.3500 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 115 || Epoch_TRAIN_Loss:301.9436 ||Epoch_VAL_Loss:151.0422\n",
      "timer:  20.5263 sec.\n",
      "-------------\n",
      "Epoch 116/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7600 || Loss: 4.4811 || 10iter: 5.5576 sec.\n",
      "Iteration 7610 || Loss: 4.4640 || 10iter: 2.1663 sec.\n",
      "Iteration 7620 || Loss: 4.5312 || 10iter: 2.1842 sec.\n",
      "Iteration 7630 || Loss: 4.5918 || 10iter: 2.2586 sec.\n",
      "Iteration 7640 || Loss: 4.7543 || 10iter: 2.4812 sec.\n",
      "Iteration 7650 || Loss: 4.5563 || 10iter: 1.4837 sec.\n",
      "-------------\n",
      "epoch 116 || Epoch_TRAIN_Loss:301.4105 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0518 sec.\n",
      "-------------\n",
      "Epoch 117/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7660 || Loss: 4.5067 || 10iter: 3.9002 sec.\n",
      "Iteration 7670 || Loss: 4.0012 || 10iter: 2.3114 sec.\n",
      "Iteration 7680 || Loss: 4.6074 || 10iter: 2.3127 sec.\n",
      "Iteration 7690 || Loss: 4.4967 || 10iter: 2.1304 sec.\n",
      "Iteration 7700 || Loss: 4.4480 || 10iter: 2.5260 sec.\n",
      "Iteration 7710 || Loss: 5.0908 || 10iter: 1.9789 sec.\n",
      "Iteration 7720 || Loss: 4.5369 || 10iter: 1.3644 sec.\n",
      "-------------\n",
      "epoch 117 || Epoch_TRAIN_Loss:299.6897 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8830 sec.\n",
      "-------------\n",
      "Epoch 118/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7730 || Loss: 4.3786 || 10iter: 4.6154 sec.\n",
      "Iteration 7740 || Loss: 4.5621 || 10iter: 2.8479 sec.\n",
      "Iteration 7750 || Loss: 4.3833 || 10iter: 2.1417 sec.\n",
      "Iteration 7760 || Loss: 4.5272 || 10iter: 2.3967 sec.\n",
      "Iteration 7770 || Loss: 4.2596 || 10iter: 2.3756 sec.\n",
      "Iteration 7780 || Loss: 4.5289 || 10iter: 1.7873 sec.\n",
      "-------------\n",
      "epoch 118 || Epoch_TRAIN_Loss:302.8535 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4976 sec.\n",
      "-------------\n",
      "Epoch 119/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7790 || Loss: 4.3635 || 10iter: 3.0899 sec.\n",
      "Iteration 7800 || Loss: 4.5715 || 10iter: 2.8661 sec.\n",
      "Iteration 7810 || Loss: 4.6390 || 10iter: 2.3477 sec.\n",
      "Iteration 7820 || Loss: 4.7352 || 10iter: 2.1616 sec.\n",
      "Iteration 7830 || Loss: 4.4655 || 10iter: 2.2580 sec.\n",
      "Iteration 7840 || Loss: 4.8499 || 10iter: 2.2831 sec.\n",
      "Iteration 7850 || Loss: 4.4834 || 10iter: 1.4360 sec.\n",
      "-------------\n",
      "epoch 119 || Epoch_TRAIN_Loss:301.8980 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1109 sec.\n",
      "-------------\n",
      "Epoch 120/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7860 || Loss: 4.4880 || 10iter: 4.3442 sec.\n",
      "Iteration 7870 || Loss: 4.5760 || 10iter: 2.2509 sec.\n",
      "Iteration 7880 || Loss: 4.4416 || 10iter: 2.4352 sec.\n",
      "Iteration 7890 || Loss: 4.4810 || 10iter: 2.0179 sec.\n",
      "Iteration 7900 || Loss: 5.0352 || 10iter: 2.4456 sec.\n",
      "Iteration 7910 || Loss: 4.5287 || 10iter: 1.7653 sec.\n",
      "Iteration 7920 || Loss: 4.2287 || 10iter: 1.3192 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 120 || Epoch_TRAIN_Loss:300.7631 ||Epoch_VAL_Loss:151.2494\n",
      "timer:  20.7539 sec.\n",
      "-------------\n",
      "Epoch 121/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7930 || Loss: 4.6721 || 10iter: 5.3853 sec.\n",
      "Iteration 7940 || Loss: 4.5897 || 10iter: 2.3482 sec.\n",
      "Iteration 7950 || Loss: 4.5518 || 10iter: 2.1613 sec.\n",
      "Iteration 7960 || Loss: 4.6274 || 10iter: 2.4843 sec.\n",
      "Iteration 7970 || Loss: 4.5601 || 10iter: 2.1959 sec.\n",
      "Iteration 7980 || Loss: 4.2603 || 10iter: 1.5430 sec.\n",
      "-------------\n",
      "epoch 121 || Epoch_TRAIN_Loss:302.1569 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1375 sec.\n",
      "-------------\n",
      "Epoch 122/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7990 || Loss: 4.3763 || 10iter: 3.6020 sec.\n",
      "Iteration 8000 || Loss: 4.4261 || 10iter: 2.5835 sec.\n",
      "Iteration 8010 || Loss: 4.9375 || 10iter: 2.5300 sec.\n",
      "Iteration 8020 || Loss: 4.8423 || 10iter: 2.4117 sec.\n",
      "Iteration 8030 || Loss: 4.6218 || 10iter: 3.0104 sec.\n",
      "Iteration 8040 || Loss: 4.5636 || 10iter: 2.0268 sec.\n",
      "Iteration 8050 || Loss: 4.3133 || 10iter: 1.4194 sec.\n",
      "-------------\n",
      "epoch 122 || Epoch_TRAIN_Loss:302.7042 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.9955 sec.\n",
      "-------------\n",
      "Epoch 123/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8060 || Loss: 4.5964 || 10iter: 4.6705 sec.\n",
      "Iteration 8070 || Loss: 4.7557 || 10iter: 2.4655 sec.\n",
      "Iteration 8080 || Loss: 4.2173 || 10iter: 1.8878 sec.\n",
      "Iteration 8090 || Loss: 4.6729 || 10iter: 2.3034 sec.\n",
      "Iteration 8100 || Loss: 4.3160 || 10iter: 2.2748 sec.\n",
      "Iteration 8110 || Loss: 4.7333 || 10iter: 1.7785 sec.\n",
      "-------------\n",
      "epoch 123 || Epoch_TRAIN_Loss:299.1954 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5669 sec.\n",
      "-------------\n",
      "Epoch 124/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8120 || Loss: 4.5190 || 10iter: 2.8216 sec.\n",
      "Iteration 8130 || Loss: 4.4742 || 10iter: 2.9796 sec.\n",
      "Iteration 8140 || Loss: 4.6498 || 10iter: 2.3191 sec.\n",
      "Iteration 8150 || Loss: 4.7128 || 10iter: 1.9210 sec.\n",
      "Iteration 8160 || Loss: 4.9228 || 10iter: 2.2662 sec.\n",
      "Iteration 8170 || Loss: 4.2815 || 10iter: 2.1517 sec.\n",
      "Iteration 8180 || Loss: 4.5115 || 10iter: 1.3693 sec.\n",
      "-------------\n",
      "epoch 124 || Epoch_TRAIN_Loss:299.1196 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4407 sec.\n",
      "-------------\n",
      "Epoch 125/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8190 || Loss: 4.6480 || 10iter: 4.1339 sec.\n",
      "Iteration 8200 || Loss: 4.6633 || 10iter: 2.4156 sec.\n",
      "Iteration 8210 || Loss: 5.0115 || 10iter: 2.3679 sec.\n",
      "Iteration 8220 || Loss: 4.0498 || 10iter: 2.2065 sec.\n",
      "Iteration 8230 || Loss: 4.4681 || 10iter: 2.5825 sec.\n",
      "Iteration 8240 || Loss: 4.4303 || 10iter: 1.8222 sec.\n",
      "Iteration 8250 || Loss: 4.8094 || 10iter: 1.3127 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 125 || Epoch_TRAIN_Loss:302.0650 ||Epoch_VAL_Loss:150.6763\n",
      "timer:  21.0263 sec.\n",
      "-------------\n",
      "Epoch 126/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8260 || Loss: 4.4266 || 10iter: 5.2293 sec.\n",
      "Iteration 8270 || Loss: 4.7821 || 10iter: 2.2222 sec.\n",
      "Iteration 8280 || Loss: 4.8741 || 10iter: 2.2553 sec.\n",
      "Iteration 8290 || Loss: 4.5828 || 10iter: 2.2891 sec.\n",
      "Iteration 8300 || Loss: 4.7909 || 10iter: 2.3502 sec.\n",
      "Iteration 8310 || Loss: 4.4661 || 10iter: 1.6254 sec.\n",
      "-------------\n",
      "epoch 126 || Epoch_TRAIN_Loss:300.0728 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9332 sec.\n",
      "-------------\n",
      "Epoch 127/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8320 || Loss: 4.4659 || 10iter: 3.4709 sec.\n",
      "Iteration 8330 || Loss: 4.8372 || 10iter: 2.8412 sec.\n",
      "Iteration 8340 || Loss: 4.5464 || 10iter: 2.2606 sec.\n",
      "Iteration 8350 || Loss: 4.8381 || 10iter: 2.0350 sec.\n",
      "Iteration 8360 || Loss: 4.3683 || 10iter: 2.4531 sec.\n",
      "Iteration 8370 || Loss: 4.4718 || 10iter: 1.9208 sec.\n",
      "Iteration 8380 || Loss: 4.3054 || 10iter: 1.3580 sec.\n",
      "-------------\n",
      "epoch 127 || Epoch_TRAIN_Loss:300.3619 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7295 sec.\n",
      "-------------\n",
      "Epoch 128/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8390 || Loss: 4.6165 || 10iter: 4.6872 sec.\n",
      "Iteration 8400 || Loss: 4.6357 || 10iter: 2.7558 sec.\n",
      "Iteration 8410 || Loss: 4.6372 || 10iter: 1.9932 sec.\n",
      "Iteration 8420 || Loss: 4.5266 || 10iter: 2.2067 sec.\n",
      "Iteration 8430 || Loss: 4.9500 || 10iter: 2.5664 sec.\n",
      "Iteration 8440 || Loss: 4.5202 || 10iter: 1.7067 sec.\n",
      "-------------\n",
      "epoch 128 || Epoch_TRAIN_Loss:302.1739 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0630 sec.\n",
      "-------------\n",
      "Epoch 129/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8450 || Loss: 4.9422 || 10iter: 2.5768 sec.\n",
      "Iteration 8460 || Loss: 4.3876 || 10iter: 3.3347 sec.\n",
      "Iteration 8470 || Loss: 4.3722 || 10iter: 2.2981 sec.\n",
      "Iteration 8480 || Loss: 4.5779 || 10iter: 2.2383 sec.\n",
      "Iteration 8490 || Loss: 4.6359 || 10iter: 2.2451 sec.\n",
      "Iteration 8500 || Loss: 4.5768 || 10iter: 2.2775 sec.\n",
      "Iteration 8510 || Loss: 4.4463 || 10iter: 1.5136 sec.\n",
      "-------------\n",
      "epoch 129 || Epoch_TRAIN_Loss:298.7107 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1849 sec.\n",
      "-------------\n",
      "Epoch 130/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8520 || Loss: 4.5608 || 10iter: 4.0680 sec.\n",
      "Iteration 8530 || Loss: 4.8747 || 10iter: 2.7609 sec.\n",
      "Iteration 8540 || Loss: 4.2726 || 10iter: 2.3077 sec.\n",
      "Iteration 8550 || Loss: 4.2334 || 10iter: 2.1183 sec.\n",
      "Iteration 8560 || Loss: 4.3042 || 10iter: 2.5132 sec.\n",
      "Iteration 8570 || Loss: 4.7466 || 10iter: 1.8622 sec.\n",
      "Iteration 8580 || Loss: 5.0241 || 10iter: 1.3203 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 130 || Epoch_TRAIN_Loss:297.6022 ||Epoch_VAL_Loss:151.3038\n",
      "timer:  21.0481 sec.\n",
      "-------------\n",
      "Epoch 131/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8590 || Loss: 4.5902 || 10iter: 5.5527 sec.\n",
      "Iteration 8600 || Loss: 4.7086 || 10iter: 2.3383 sec.\n",
      "Iteration 8610 || Loss: 4.7338 || 10iter: 2.0541 sec.\n",
      "Iteration 8620 || Loss: 4.3684 || 10iter: 2.2885 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8630 || Loss: 4.9351 || 10iter: 2.3324 sec.\n",
      "Iteration 8640 || Loss: 4.6432 || 10iter: 1.4247 sec.\n",
      "-------------\n",
      "epoch 131 || Epoch_TRAIN_Loss:302.2578 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9052 sec.\n",
      "-------------\n",
      "Epoch 132/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8650 || Loss: 4.8135 || 10iter: 3.7547 sec.\n",
      "Iteration 8660 || Loss: 4.9286 || 10iter: 2.6882 sec.\n",
      "Iteration 8670 || Loss: 4.4193 || 10iter: 2.2351 sec.\n",
      "Iteration 8680 || Loss: 4.5862 || 10iter: 2.5956 sec.\n",
      "Iteration 8690 || Loss: 5.0433 || 10iter: 2.7568 sec.\n",
      "Iteration 8700 || Loss: 5.1447 || 10iter: 2.2791 sec.\n",
      "Iteration 8710 || Loss: 4.5182 || 10iter: 1.4500 sec.\n",
      "-------------\n",
      "epoch 132 || Epoch_TRAIN_Loss:298.1517 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1904 sec.\n",
      "-------------\n",
      "Epoch 133/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8720 || Loss: 4.7536 || 10iter: 5.0130 sec.\n",
      "Iteration 8730 || Loss: 4.6141 || 10iter: 2.4638 sec.\n",
      "Iteration 8740 || Loss: 4.5556 || 10iter: 2.1375 sec.\n",
      "Iteration 8750 || Loss: 4.5395 || 10iter: 2.3800 sec.\n",
      "Iteration 8760 || Loss: 4.5475 || 10iter: 2.4002 sec.\n",
      "Iteration 8770 || Loss: 5.0199 || 10iter: 1.7157 sec.\n",
      "-------------\n",
      "epoch 133 || Epoch_TRAIN_Loss:300.6754 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.3009 sec.\n",
      "-------------\n",
      "Epoch 134/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8780 || Loss: 4.4696 || 10iter: 3.0883 sec.\n",
      "Iteration 8790 || Loss: 4.4572 || 10iter: 2.7847 sec.\n",
      "Iteration 8800 || Loss: 4.2936 || 10iter: 2.1645 sec.\n",
      "Iteration 8810 || Loss: 4.8406 || 10iter: 2.4692 sec.\n",
      "Iteration 8820 || Loss: 4.4758 || 10iter: 2.2360 sec.\n",
      "Iteration 8830 || Loss: 4.7772 || 10iter: 2.2873 sec.\n",
      "Iteration 8840 || Loss: 4.6735 || 10iter: 1.4089 sec.\n",
      "-------------\n",
      "epoch 134 || Epoch_TRAIN_Loss:299.2154 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0857 sec.\n",
      "-------------\n",
      "Epoch 135/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8850 || Loss: 4.6741 || 10iter: 4.2532 sec.\n",
      "Iteration 8860 || Loss: 4.4369 || 10iter: 2.3839 sec.\n",
      "Iteration 8870 || Loss: 4.3935 || 10iter: 2.2123 sec.\n",
      "Iteration 8880 || Loss: 4.2699 || 10iter: 2.4050 sec.\n",
      "Iteration 8890 || Loss: 4.5544 || 10iter: 2.4829 sec.\n",
      "Iteration 8900 || Loss: 4.5009 || 10iter: 1.7888 sec.\n",
      "Iteration 8910 || Loss: 4.3504 || 10iter: 1.3197 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 135 || Epoch_TRAIN_Loss:300.1826 ||Epoch_VAL_Loss:151.1830\n",
      "timer:  20.9798 sec.\n",
      "-------------\n",
      "Epoch 136/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8920 || Loss: 4.4851 || 10iter: 5.3690 sec.\n",
      "Iteration 8930 || Loss: 4.4571 || 10iter: 2.4063 sec.\n",
      "Iteration 8940 || Loss: 4.5010 || 10iter: 1.9831 sec.\n",
      "Iteration 8950 || Loss: 4.4998 || 10iter: 2.2575 sec.\n",
      "Iteration 8960 || Loss: 4.6346 || 10iter: 2.3403 sec.\n",
      "Iteration 8970 || Loss: 4.4838 || 10iter: 1.4896 sec.\n",
      "-------------\n",
      "epoch 136 || Epoch_TRAIN_Loss:300.3271 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7519 sec.\n",
      "-------------\n",
      "Epoch 137/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8980 || Loss: 5.2179 || 10iter: 3.6751 sec.\n",
      "Iteration 8990 || Loss: 4.3813 || 10iter: 2.6484 sec.\n",
      "Iteration 9000 || Loss: 4.3758 || 10iter: 2.2229 sec.\n",
      "Iteration 9010 || Loss: 4.2354 || 10iter: 2.1156 sec.\n",
      "Iteration 9020 || Loss: 4.3279 || 10iter: 2.5006 sec.\n",
      "Iteration 9030 || Loss: 4.6201 || 10iter: 2.0716 sec.\n",
      "Iteration 9040 || Loss: 4.8701 || 10iter: 1.5336 sec.\n",
      "-------------\n",
      "epoch 137 || Epoch_TRAIN_Loss:298.4007 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2194 sec.\n",
      "-------------\n",
      "Epoch 138/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9050 || Loss: 4.7495 || 10iter: 4.5639 sec.\n",
      "Iteration 9060 || Loss: 4.2160 || 10iter: 2.5388 sec.\n",
      "Iteration 9070 || Loss: 4.6919 || 10iter: 2.2832 sec.\n",
      "Iteration 9080 || Loss: 4.3102 || 10iter: 2.0844 sec.\n",
      "Iteration 9090 || Loss: 4.3738 || 10iter: 2.4290 sec.\n",
      "Iteration 9100 || Loss: 4.3916 || 10iter: 1.6997 sec.\n",
      "-------------\n",
      "epoch 138 || Epoch_TRAIN_Loss:300.0292 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8144 sec.\n",
      "-------------\n",
      "Epoch 139/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9110 || Loss: 4.3158 || 10iter: 2.4913 sec.\n",
      "Iteration 9120 || Loss: 4.1706 || 10iter: 3.3819 sec.\n",
      "Iteration 9130 || Loss: 4.7984 || 10iter: 2.1846 sec.\n",
      "Iteration 9140 || Loss: 4.5450 || 10iter: 2.1045 sec.\n",
      "Iteration 9150 || Loss: 4.3391 || 10iter: 2.3597 sec.\n",
      "Iteration 9160 || Loss: 5.0967 || 10iter: 2.0334 sec.\n",
      "Iteration 9170 || Loss: 4.4653 || 10iter: 1.3871 sec.\n",
      "-------------\n",
      "epoch 139 || Epoch_TRAIN_Loss:299.0682 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5839 sec.\n",
      "-------------\n",
      "Epoch 140/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9180 || Loss: 4.2395 || 10iter: 4.3837 sec.\n",
      "Iteration 9190 || Loss: 4.7776 || 10iter: 2.5456 sec.\n",
      "Iteration 9200 || Loss: 4.4692 || 10iter: 2.2042 sec.\n",
      "Iteration 9210 || Loss: 4.5713 || 10iter: 2.1117 sec.\n",
      "Iteration 9220 || Loss: 4.4819 || 10iter: 2.5328 sec.\n",
      "Iteration 9230 || Loss: 4.9591 || 10iter: 1.8384 sec.\n",
      "Iteration 9240 || Loss: 4.1364 || 10iter: 1.4805 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 140 || Epoch_TRAIN_Loss:300.4735 ||Epoch_VAL_Loss:150.5772\n",
      "timer:  21.3779 sec.\n",
      "-------------\n",
      "Epoch 141/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9250 || Loss: 4.7334 || 10iter: 5.4366 sec.\n",
      "Iteration 9260 || Loss: 4.3388 || 10iter: 2.3432 sec.\n",
      "Iteration 9270 || Loss: 4.8665 || 10iter: 2.1808 sec.\n",
      "Iteration 9280 || Loss: 4.7142 || 10iter: 2.3488 sec.\n",
      "Iteration 9290 || Loss: 4.2383 || 10iter: 2.3567 sec.\n",
      "Iteration 9300 || Loss: 4.7442 || 10iter: 1.5670 sec.\n",
      "-------------\n",
      "epoch 141 || Epoch_TRAIN_Loss:298.6170 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1664 sec.\n",
      "-------------\n",
      "Epoch 142/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9310 || Loss: 4.6542 || 10iter: 3.6844 sec.\n",
      "Iteration 9320 || Loss: 4.1706 || 10iter: 2.6317 sec.\n",
      "Iteration 9330 || Loss: 4.7000 || 10iter: 2.3195 sec.\n",
      "Iteration 9340 || Loss: 4.5512 || 10iter: 2.4003 sec.\n",
      "Iteration 9350 || Loss: 4.4822 || 10iter: 2.9164 sec.\n",
      "Iteration 9360 || Loss: 5.0516 || 10iter: 2.0849 sec.\n",
      "Iteration 9370 || Loss: 4.4445 || 10iter: 1.3972 sec.\n",
      "-------------\n",
      "epoch 142 || Epoch_TRAIN_Loss:299.3949 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.7803 sec.\n",
      "-------------\n",
      "Epoch 143/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9380 || Loss: 4.7474 || 10iter: 4.8287 sec.\n",
      "Iteration 9390 || Loss: 4.3790 || 10iter: 2.3394 sec.\n",
      "Iteration 9400 || Loss: 4.6136 || 10iter: 2.2906 sec.\n",
      "Iteration 9410 || Loss: 4.4329 || 10iter: 2.2694 sec.\n",
      "Iteration 9420 || Loss: 4.3622 || 10iter: 2.5047 sec.\n",
      "Iteration 9430 || Loss: 4.4155 || 10iter: 1.7108 sec.\n",
      "-------------\n",
      "epoch 143 || Epoch_TRAIN_Loss:298.0090 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2589 sec.\n",
      "-------------\n",
      "Epoch 144/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9440 || Loss: 4.3172 || 10iter: 3.1924 sec.\n",
      "Iteration 9450 || Loss: 4.2327 || 10iter: 2.7277 sec.\n",
      "Iteration 9460 || Loss: 4.4971 || 10iter: 2.2559 sec.\n",
      "Iteration 9470 || Loss: 4.4586 || 10iter: 2.1590 sec.\n",
      "Iteration 9480 || Loss: 4.6122 || 10iter: 2.4198 sec.\n",
      "Iteration 9490 || Loss: 4.5550 || 10iter: 2.2553 sec.\n",
      "Iteration 9500 || Loss: 4.5089 || 10iter: 1.4527 sec.\n",
      "-------------\n",
      "epoch 144 || Epoch_TRAIN_Loss:297.7061 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1355 sec.\n",
      "-------------\n",
      "Epoch 145/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9510 || Loss: 4.2660 || 10iter: 4.1370 sec.\n",
      "Iteration 9520 || Loss: 4.6415 || 10iter: 2.5785 sec.\n",
      "Iteration 9530 || Loss: 4.4973 || 10iter: 2.0593 sec.\n",
      "Iteration 9540 || Loss: 4.9200 || 10iter: 2.2522 sec.\n",
      "Iteration 9550 || Loss: 3.9832 || 10iter: 2.5434 sec.\n",
      "Iteration 9560 || Loss: 4.3601 || 10iter: 1.7543 sec.\n",
      "Iteration 9570 || Loss: 4.8762 || 10iter: 1.3435 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 145 || Epoch_TRAIN_Loss:300.6993 ||Epoch_VAL_Loss:150.2625\n",
      "timer:  20.8765 sec.\n",
      "-------------\n",
      "Epoch 146/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9580 || Loss: 4.5500 || 10iter: 5.4460 sec.\n",
      "Iteration 9590 || Loss: 4.3998 || 10iter: 2.2530 sec.\n",
      "Iteration 9600 || Loss: 4.8299 || 10iter: 2.3163 sec.\n",
      "Iteration 9610 || Loss: 4.4920 || 10iter: 2.3918 sec.\n",
      "Iteration 9620 || Loss: 4.9036 || 10iter: 2.4609 sec.\n",
      "Iteration 9630 || Loss: 4.2658 || 10iter: 1.5838 sec.\n",
      "-------------\n",
      "epoch 146 || Epoch_TRAIN_Loss:298.2566 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4679 sec.\n",
      "-------------\n",
      "Epoch 147/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9640 || Loss: 4.3303 || 10iter: 3.6445 sec.\n",
      "Iteration 9650 || Loss: 4.5958 || 10iter: 2.6683 sec.\n",
      "Iteration 9660 || Loss: 4.4208 || 10iter: 2.5015 sec.\n",
      "Iteration 9670 || Loss: 4.5751 || 10iter: 2.0813 sec.\n",
      "Iteration 9680 || Loss: 4.3752 || 10iter: 2.4815 sec.\n",
      "Iteration 9690 || Loss: 4.7309 || 10iter: 2.0027 sec.\n",
      "Iteration 9700 || Loss: 4.2752 || 10iter: 1.3645 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "epoch 147 || Epoch_TRAIN_Loss:297.3156 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1154 sec.\n",
      "-------------\n",
      "Epoch 148/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9710 || Loss: 4.4630 || 10iter: 4.8470 sec.\n",
      "Iteration 9720 || Loss: 4.4423 || 10iter: 2.3859 sec.\n",
      "Iteration 9730 || Loss: 4.5677 || 10iter: 1.9757 sec.\n",
      "Iteration 9740 || Loss: 4.4063 || 10iter: 2.2605 sec.\n",
      "Iteration 9750 || Loss: 4.4066 || 10iter: 2.4372 sec.\n",
      "Iteration 9760 || Loss: 4.3836 || 10iter: 1.6356 sec.\n",
      "-------------\n",
      "epoch 148 || Epoch_TRAIN_Loss:295.1534 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7105 sec.\n",
      "-------------\n",
      "Epoch 149/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9770 || Loss: 5.0427 || 10iter: 2.5123 sec.\n",
      "Iteration 9780 || Loss: 4.3893 || 10iter: 3.3045 sec.\n",
      "Iteration 9790 || Loss: 4.8994 || 10iter: 2.4013 sec.\n",
      "Iteration 9800 || Loss: 4.0836 || 10iter: 2.1505 sec.\n",
      "Iteration 9810 || Loss: 4.1599 || 10iter: 2.3849 sec.\n",
      "Iteration 9820 || Loss: 4.4808 || 10iter: 2.1805 sec.\n",
      "Iteration 9830 || Loss: 4.0473 || 10iter: 1.4475 sec.\n",
      "-------------\n",
      "epoch 149 || Epoch_TRAIN_Loss:298.4960 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0970 sec.\n",
      "-------------\n",
      "Epoch 150/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9840 || Loss: 4.8516 || 10iter: 4.0636 sec.\n",
      "Iteration 9850 || Loss: 4.4156 || 10iter: 2.5497 sec.\n",
      "Iteration 9860 || Loss: 4.3383 || 10iter: 2.2573 sec.\n",
      "Iteration 9870 || Loss: 4.5041 || 10iter: 2.0516 sec.\n",
      "Iteration 9880 || Loss: 3.9624 || 10iter: 2.5427 sec.\n",
      "Iteration 9890 || Loss: 4.4033 || 10iter: 1.7634 sec.\n",
      "Iteration 9900 || Loss: 3.8382 || 10iter: 1.3223 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 150 || Epoch_TRAIN_Loss:297.2487 ||Epoch_VAL_Loss:150.6077\n",
      "timer:  20.7717 sec.\n",
      "-------------\n",
      "Epoch 151/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9910 || Loss: 4.4204 || 10iter: 5.5311 sec.\n",
      "Iteration 9920 || Loss: 4.7467 || 10iter: 2.1849 sec.\n",
      "Iteration 9930 || Loss: 4.4393 || 10iter: 2.1124 sec.\n",
      "Iteration 9940 || Loss: 4.3273 || 10iter: 2.1073 sec.\n",
      "Iteration 9950 || Loss: 4.1718 || 10iter: 2.4285 sec.\n",
      "Iteration 9960 || Loss: 4.3739 || 10iter: 1.5067 sec.\n",
      "-------------\n",
      "epoch 151 || Epoch_TRAIN_Loss:299.5432 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7400 sec.\n",
      "-------------\n",
      "Epoch 152/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9970 || Loss: 4.5280 || 10iter: 3.6413 sec.\n",
      "Iteration 9980 || Loss: 4.0025 || 10iter: 2.6587 sec.\n",
      "Iteration 9990 || Loss: 4.7985 || 10iter: 2.3354 sec.\n",
      "Iteration 10000 || Loss: 4.6730 || 10iter: 2.1058 sec.\n",
      "Iteration 10010 || Loss: 4.7681 || 10iter: 3.0902 sec.\n",
      "Iteration 10020 || Loss: 4.5702 || 10iter: 2.3359 sec.\n",
      "Iteration 10030 || Loss: 4.5495 || 10iter: 1.4646 sec.\n",
      "-------------\n",
      "epoch 152 || Epoch_TRAIN_Loss:297.1454 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1443 sec.\n",
      "-------------\n",
      "Epoch 153/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10040 || Loss: 4.3828 || 10iter: 4.6931 sec.\n",
      "Iteration 10050 || Loss: 4.3460 || 10iter: 2.6489 sec.\n",
      "Iteration 10060 || Loss: 4.6132 || 10iter: 2.2256 sec.\n",
      "Iteration 10070 || Loss: 5.0434 || 10iter: 2.3519 sec.\n",
      "Iteration 10080 || Loss: 4.5619 || 10iter: 2.4263 sec.\n",
      "Iteration 10090 || Loss: 4.8695 || 10iter: 1.6385 sec.\n",
      "-------------\n",
      "epoch 153 || Epoch_TRAIN_Loss:296.8781 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1751 sec.\n",
      "-------------\n",
      "Epoch 154/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10100 || Loss: 4.2270 || 10iter: 2.6486 sec.\n",
      "Iteration 10110 || Loss: 4.6902 || 10iter: 3.3035 sec.\n",
      "Iteration 10120 || Loss: 4.6040 || 10iter: 2.2064 sec.\n",
      "Iteration 10130 || Loss: 4.4575 || 10iter: 2.1841 sec.\n",
      "Iteration 10140 || Loss: 4.2990 || 10iter: 2.1577 sec.\n",
      "Iteration 10150 || Loss: 4.6784 || 10iter: 2.3257 sec.\n",
      "Iteration 10160 || Loss: 4.6373 || 10iter: 1.4330 sec.\n",
      "-------------\n",
      "epoch 154 || Epoch_TRAIN_Loss:295.3261 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9622 sec.\n",
      "-------------\n",
      "Epoch 155/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10170 || Loss: 4.3302 || 10iter: 4.0577 sec.\n",
      "Iteration 10180 || Loss: 4.5740 || 10iter: 2.6968 sec.\n",
      "Iteration 10190 || Loss: 4.4603 || 10iter: 2.1189 sec.\n",
      "Iteration 10200 || Loss: 4.7273 || 10iter: 2.0893 sec.\n",
      "Iteration 10210 || Loss: 4.6251 || 10iter: 2.5365 sec.\n",
      "Iteration 10220 || Loss: 4.5617 || 10iter: 1.7535 sec.\n",
      "Iteration 10230 || Loss: 4.3064 || 10iter: 1.3064 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 155 || Epoch_TRAIN_Loss:295.1545 ||Epoch_VAL_Loss:150.7784\n",
      "timer:  20.7554 sec.\n",
      "-------------\n",
      "Epoch 156/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10240 || Loss: 4.6192 || 10iter: 5.4286 sec.\n",
      "Iteration 10250 || Loss: 4.5819 || 10iter: 2.3058 sec.\n",
      "Iteration 10260 || Loss: 4.4067 || 10iter: 2.2383 sec.\n",
      "Iteration 10270 || Loss: 4.5610 || 10iter: 2.2849 sec.\n",
      "Iteration 10280 || Loss: 4.4337 || 10iter: 2.3344 sec.\n",
      "Iteration 10290 || Loss: 4.7594 || 10iter: 1.4976 sec.\n",
      "-------------\n",
      "epoch 156 || Epoch_TRAIN_Loss:297.3808 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9948 sec.\n",
      "-------------\n",
      "Epoch 157/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10300 || Loss: 4.5730 || 10iter: 3.7578 sec.\n",
      "Iteration 10310 || Loss: 4.0999 || 10iter: 2.7966 sec.\n",
      "Iteration 10320 || Loss: 4.8556 || 10iter: 2.1939 sec.\n",
      "Iteration 10330 || Loss: 4.0795 || 10iter: 2.1333 sec.\n",
      "Iteration 10340 || Loss: 4.7640 || 10iter: 2.5069 sec.\n",
      "Iteration 10350 || Loss: 4.5360 || 10iter: 2.0683 sec.\n",
      "Iteration 10360 || Loss: 4.1795 || 10iter: 1.4839 sec.\n",
      "-------------\n",
      "epoch 157 || Epoch_TRAIN_Loss:299.4991 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4026 sec.\n",
      "-------------\n",
      "Epoch 158/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10370 || Loss: 4.6259 || 10iter: 4.7284 sec.\n",
      "Iteration 10380 || Loss: 4.2330 || 10iter: 2.4088 sec.\n",
      "Iteration 10390 || Loss: 4.0418 || 10iter: 2.1212 sec.\n",
      "Iteration 10400 || Loss: 4.3026 || 10iter: 2.1356 sec.\n",
      "Iteration 10410 || Loss: 4.2908 || 10iter: 2.3573 sec.\n",
      "Iteration 10420 || Loss: 4.3836 || 10iter: 1.6192 sec.\n",
      "-------------\n",
      "epoch 158 || Epoch_TRAIN_Loss:298.1717 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5603 sec.\n",
      "-------------\n",
      "Epoch 159/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10430 || Loss: 4.4092 || 10iter: 2.7274 sec.\n",
      "Iteration 10440 || Loss: 4.3232 || 10iter: 2.9011 sec.\n",
      "Iteration 10450 || Loss: 4.0051 || 10iter: 2.3148 sec.\n",
      "Iteration 10460 || Loss: 4.8132 || 10iter: 2.1234 sec.\n",
      "Iteration 10470 || Loss: 4.5268 || 10iter: 2.3269 sec.\n",
      "Iteration 10480 || Loss: 4.2914 || 10iter: 2.1191 sec.\n",
      "Iteration 10490 || Loss: 4.2586 || 10iter: 1.3593 sec.\n",
      "-------------\n",
      "epoch 159 || Epoch_TRAIN_Loss:294.9160 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4887 sec.\n",
      "-------------\n",
      "Epoch 160/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10500 || Loss: 4.5485 || 10iter: 4.2482 sec.\n",
      "Iteration 10510 || Loss: 4.2442 || 10iter: 2.6046 sec.\n",
      "Iteration 10520 || Loss: 4.5137 || 10iter: 2.2090 sec.\n",
      "Iteration 10530 || Loss: 4.6249 || 10iter: 2.3765 sec.\n",
      "Iteration 10540 || Loss: 4.6238 || 10iter: 2.4638 sec.\n",
      "Iteration 10550 || Loss: 4.4725 || 10iter: 1.8142 sec.\n",
      "Iteration 10560 || Loss: 4.8051 || 10iter: 1.4655 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 160 || Epoch_TRAIN_Loss:294.6179 ||Epoch_VAL_Loss:148.9527\n",
      "timer:  21.4640 sec.\n",
      "-------------\n",
      "Epoch 161/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10570 || Loss: 4.3161 || 10iter: 5.2781 sec.\n",
      "Iteration 10580 || Loss: 4.6630 || 10iter: 2.4602 sec.\n",
      "Iteration 10590 || Loss: 4.4520 || 10iter: 2.1169 sec.\n",
      "Iteration 10600 || Loss: 4.4405 || 10iter: 2.3916 sec.\n",
      "Iteration 10610 || Loss: 4.4511 || 10iter: 2.3384 sec.\n",
      "Iteration 10620 || Loss: 4.4391 || 10iter: 1.4919 sec.\n",
      "-------------\n",
      "epoch 161 || Epoch_TRAIN_Loss:297.2214 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0339 sec.\n",
      "-------------\n",
      "Epoch 162/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10630 || Loss: 4.1128 || 10iter: 3.3188 sec.\n",
      "Iteration 10640 || Loss: 4.3172 || 10iter: 3.0525 sec.\n",
      "Iteration 10650 || Loss: 4.6833 || 10iter: 2.3053 sec.\n",
      "Iteration 10660 || Loss: 4.1953 || 10iter: 2.2251 sec.\n",
      "Iteration 10670 || Loss: 4.3793 || 10iter: 2.6444 sec.\n",
      "Iteration 10680 || Loss: 4.2280 || 10iter: 2.1906 sec.\n",
      "Iteration 10690 || Loss: 4.2183 || 10iter: 1.4691 sec.\n",
      "-------------\n",
      "epoch 162 || Epoch_TRAIN_Loss:295.2500 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.5876 sec.\n",
      "-------------\n",
      "Epoch 163/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10700 || Loss: 4.3235 || 10iter: 4.8174 sec.\n",
      "Iteration 10710 || Loss: 4.5939 || 10iter: 2.3081 sec.\n",
      "Iteration 10720 || Loss: 4.7004 || 10iter: 2.0244 sec.\n",
      "Iteration 10730 || Loss: 4.4390 || 10iter: 2.2222 sec.\n",
      "Iteration 10740 || Loss: 4.5538 || 10iter: 2.4226 sec.\n",
      "Iteration 10750 || Loss: 4.2935 || 10iter: 1.7121 sec.\n",
      "-------------\n",
      "epoch 163 || Epoch_TRAIN_Loss:296.6711 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7239 sec.\n",
      "-------------\n",
      "Epoch 164/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10760 || Loss: 4.4370 || 10iter: 2.8726 sec.\n",
      "Iteration 10770 || Loss: 4.5609 || 10iter: 3.1096 sec.\n",
      "Iteration 10780 || Loss: 4.3680 || 10iter: 2.4342 sec.\n",
      "Iteration 10790 || Loss: 4.1130 || 10iter: 1.8528 sec.\n",
      "Iteration 10800 || Loss: 4.6902 || 10iter: 2.3722 sec.\n",
      "Iteration 10810 || Loss: 4.4297 || 10iter: 2.0698 sec.\n",
      "Iteration 10820 || Loss: 4.5315 || 10iter: 1.3778 sec.\n",
      "-------------\n",
      "epoch 164 || Epoch_TRAIN_Loss:295.7237 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7180 sec.\n",
      "-------------\n",
      "Epoch 165/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10830 || Loss: 4.4542 || 10iter: 4.1123 sec.\n",
      "Iteration 10840 || Loss: 4.4850 || 10iter: 2.3882 sec.\n",
      "Iteration 10850 || Loss: 4.3803 || 10iter: 2.3342 sec.\n",
      "Iteration 10860 || Loss: 4.3693 || 10iter: 2.1052 sec.\n",
      "Iteration 10870 || Loss: 4.4533 || 10iter: 2.6252 sec.\n",
      "Iteration 10880 || Loss: 4.7821 || 10iter: 1.9637 sec.\n",
      "Iteration 10890 || Loss: 4.8025 || 10iter: 1.3742 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 165 || Epoch_TRAIN_Loss:294.9470 ||Epoch_VAL_Loss:148.7449\n",
      "timer:  21.1189 sec.\n",
      "-------------\n",
      "Epoch 166/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10900 || Loss: 4.7519 || 10iter: 5.3372 sec.\n",
      "Iteration 10910 || Loss: 4.3749 || 10iter: 2.2627 sec.\n",
      "Iteration 10920 || Loss: 4.3818 || 10iter: 2.3327 sec.\n",
      "Iteration 10930 || Loss: 4.4057 || 10iter: 2.2819 sec.\n",
      "Iteration 10940 || Loss: 4.8134 || 10iter: 2.2907 sec.\n",
      "Iteration 10950 || Loss: 4.1797 || 10iter: 1.4882 sec.\n",
      "-------------\n",
      "epoch 166 || Epoch_TRAIN_Loss:298.4182 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8996 sec.\n",
      "-------------\n",
      "Epoch 167/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10960 || Loss: 4.2653 || 10iter: 3.4658 sec.\n",
      "Iteration 10970 || Loss: 4.3762 || 10iter: 2.8413 sec.\n",
      "Iteration 10980 || Loss: 4.2363 || 10iter: 2.0855 sec.\n",
      "Iteration 10990 || Loss: 4.7800 || 10iter: 2.2004 sec.\n",
      "Iteration 11000 || Loss: 4.5640 || 10iter: 2.5120 sec.\n",
      "Iteration 11010 || Loss: 4.4450 || 10iter: 1.9065 sec.\n",
      "Iteration 11020 || Loss: 4.7212 || 10iter: 1.3521 sec.\n",
      "-------------\n",
      "epoch 167 || Epoch_TRAIN_Loss:294.3379 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7274 sec.\n",
      "-------------\n",
      "Epoch 168/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11030 || Loss: 4.1335 || 10iter: 4.7128 sec.\n",
      "Iteration 11040 || Loss: 4.3098 || 10iter: 2.5273 sec.\n",
      "Iteration 11050 || Loss: 4.5638 || 10iter: 2.2394 sec.\n",
      "Iteration 11060 || Loss: 4.1169 || 10iter: 2.3564 sec.\n",
      "Iteration 11070 || Loss: 4.6013 || 10iter: 2.2300 sec.\n",
      "Iteration 11080 || Loss: 4.5319 || 10iter: 1.8062 sec.\n",
      "-------------\n",
      "epoch 168 || Epoch_TRAIN_Loss:292.1974 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1650 sec.\n",
      "-------------\n",
      "Epoch 169/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11090 || Loss: 4.0749 || 10iter: 2.6360 sec.\n",
      "Iteration 11100 || Loss: 4.5273 || 10iter: 3.0960 sec.\n",
      "Iteration 11110 || Loss: 4.4868 || 10iter: 2.3486 sec.\n",
      "Iteration 11120 || Loss: 4.8067 || 10iter: 2.0922 sec.\n",
      "Iteration 11130 || Loss: 4.5590 || 10iter: 2.3538 sec.\n",
      "Iteration 11140 || Loss: 4.8676 || 10iter: 2.0837 sec.\n",
      "Iteration 11150 || Loss: 4.4880 || 10iter: 1.4131 sec.\n",
      "-------------\n",
      "epoch 169 || Epoch_TRAIN_Loss:294.4863 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6827 sec.\n",
      "-------------\n",
      "Epoch 170/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11160 || Loss: 4.3028 || 10iter: 3.8805 sec.\n",
      "Iteration 11170 || Loss: 4.3331 || 10iter: 2.6913 sec.\n",
      "Iteration 11180 || Loss: 4.2866 || 10iter: 2.4153 sec.\n",
      "Iteration 11190 || Loss: 4.4685 || 10iter: 2.0800 sec.\n",
      "Iteration 11200 || Loss: 4.5296 || 10iter: 2.5161 sec.\n",
      "Iteration 11210 || Loss: 4.5830 || 10iter: 1.8111 sec.\n",
      "Iteration 11220 || Loss: 4.0616 || 10iter: 1.3198 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 170 || Epoch_TRAIN_Loss:294.9212 ||Epoch_VAL_Loss:148.1431\n",
      "timer:  20.8965 sec.\n",
      "-------------\n",
      "Epoch 171/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11230 || Loss: 4.9876 || 10iter: 5.5398 sec.\n",
      "Iteration 11240 || Loss: 4.3309 || 10iter: 2.1628 sec.\n",
      "Iteration 11250 || Loss: 4.3621 || 10iter: 2.1687 sec.\n",
      "Iteration 11260 || Loss: 4.1577 || 10iter: 2.3888 sec.\n",
      "Iteration 11270 || Loss: 4.7014 || 10iter: 2.2518 sec.\n",
      "Iteration 11280 || Loss: 5.0382 || 10iter: 1.6049 sec.\n",
      "-------------\n",
      "epoch 171 || Epoch_TRAIN_Loss:295.0181 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1800 sec.\n",
      "-------------\n",
      "Epoch 172/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11290 || Loss: 4.7074 || 10iter: 3.6015 sec.\n",
      "Iteration 11300 || Loss: 4.4624 || 10iter: 2.6956 sec.\n",
      "Iteration 11310 || Loss: 4.7315 || 10iter: 2.2837 sec.\n",
      "Iteration 11320 || Loss: 4.5310 || 10iter: 2.1429 sec.\n",
      "Iteration 11330 || Loss: 4.6461 || 10iter: 2.4755 sec.\n",
      "Iteration 11340 || Loss: 5.0439 || 10iter: 2.1483 sec.\n",
      "Iteration 11350 || Loss: 4.2408 || 10iter: 1.4716 sec.\n",
      "-------------\n",
      "epoch 172 || Epoch_TRAIN_Loss:295.0756 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2313 sec.\n",
      "-------------\n",
      "Epoch 173/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11360 || Loss: 4.2964 || 10iter: 4.9594 sec.\n",
      "Iteration 11370 || Loss: 4.3139 || 10iter: 2.3606 sec.\n",
      "Iteration 11380 || Loss: 4.8627 || 10iter: 2.2769 sec.\n",
      "Iteration 11390 || Loss: 4.3670 || 10iter: 2.0112 sec.\n",
      "Iteration 11400 || Loss: 4.2983 || 10iter: 2.4810 sec.\n",
      "Iteration 11410 || Loss: 4.5697 || 10iter: 1.8207 sec.\n",
      "-------------\n",
      "epoch 173 || Epoch_TRAIN_Loss:294.0505 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1693 sec.\n",
      "-------------\n",
      "Epoch 174/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11420 || Loss: 4.2769 || 10iter: 2.7326 sec.\n",
      "Iteration 11430 || Loss: 4.4830 || 10iter: 3.2419 sec.\n",
      "Iteration 11440 || Loss: 4.5864 || 10iter: 2.2262 sec.\n",
      "Iteration 11450 || Loss: 4.1657 || 10iter: 2.0592 sec.\n",
      "Iteration 11460 || Loss: 4.4198 || 10iter: 2.6429 sec.\n",
      "Iteration 11470 || Loss: 4.7537 || 10iter: 2.0956 sec.\n",
      "Iteration 11480 || Loss: 4.3028 || 10iter: 1.3432 sec.\n",
      "-------------\n",
      "epoch 174 || Epoch_TRAIN_Loss:293.1400 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9659 sec.\n",
      "-------------\n",
      "Epoch 175/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11490 || Loss: 4.2568 || 10iter: 4.3109 sec.\n",
      "Iteration 11500 || Loss: 4.7478 || 10iter: 2.5529 sec.\n",
      "Iteration 11510 || Loss: 5.0817 || 10iter: 2.1845 sec.\n",
      "Iteration 11520 || Loss: 4.2353 || 10iter: 2.0408 sec.\n",
      "Iteration 11530 || Loss: 4.3846 || 10iter: 2.4947 sec.\n",
      "Iteration 11540 || Loss: 4.5019 || 10iter: 1.8169 sec.\n",
      "Iteration 11550 || Loss: 4.3357 || 10iter: 1.3024 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 175 || Epoch_TRAIN_Loss:296.3324 ||Epoch_VAL_Loss:148.9830\n",
      "timer:  20.8649 sec.\n",
      "-------------\n",
      "Epoch 176/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11560 || Loss: 4.5299 || 10iter: 5.2572 sec.\n",
      "Iteration 11570 || Loss: 4.2020 || 10iter: 2.3097 sec.\n",
      "Iteration 11580 || Loss: 4.4452 || 10iter: 2.1516 sec.\n",
      "Iteration 11590 || Loss: 4.3531 || 10iter: 2.1764 sec.\n",
      "Iteration 11600 || Loss: 4.4543 || 10iter: 2.4654 sec.\n",
      "Iteration 11610 || Loss: 4.5126 || 10iter: 1.5691 sec.\n",
      "-------------\n",
      "epoch 176 || Epoch_TRAIN_Loss:294.8580 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9590 sec.\n",
      "-------------\n",
      "Epoch 177/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11620 || Loss: 4.3778 || 10iter: 3.7845 sec.\n",
      "Iteration 11630 || Loss: 4.6531 || 10iter: 2.4439 sec.\n",
      "Iteration 11640 || Loss: 4.5230 || 10iter: 2.1899 sec.\n",
      "Iteration 11650 || Loss: 4.5265 || 10iter: 2.3705 sec.\n",
      "Iteration 11660 || Loss: 3.9643 || 10iter: 2.3534 sec.\n",
      "Iteration 11670 || Loss: 4.1746 || 10iter: 1.9752 sec.\n",
      "Iteration 11680 || Loss: 4.6074 || 10iter: 1.3805 sec.\n",
      "-------------\n",
      "epoch 177 || Epoch_TRAIN_Loss:295.1525 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8637 sec.\n",
      "-------------\n",
      "Epoch 178/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11690 || Loss: 5.0399 || 10iter: 4.5338 sec.\n",
      "Iteration 11700 || Loss: 4.7846 || 10iter: 2.7475 sec.\n",
      "Iteration 11710 || Loss: 4.3942 || 10iter: 2.0538 sec.\n",
      "Iteration 11720 || Loss: 4.4620 || 10iter: 2.3159 sec.\n",
      "Iteration 11730 || Loss: 4.5199 || 10iter: 2.2763 sec.\n",
      "Iteration 11740 || Loss: 4.3866 || 10iter: 1.5955 sec.\n",
      "-------------\n",
      "epoch 178 || Epoch_TRAIN_Loss:295.6078 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6861 sec.\n",
      "-------------\n",
      "Epoch 179/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11750 || Loss: 4.1265 || 10iter: 2.5680 sec.\n",
      "Iteration 11760 || Loss: 4.1613 || 10iter: 3.1443 sec.\n",
      "Iteration 11770 || Loss: 4.5144 || 10iter: 2.4761 sec.\n",
      "Iteration 11780 || Loss: 4.1163 || 10iter: 2.1248 sec.\n",
      "Iteration 11790 || Loss: 4.4598 || 10iter: 2.4033 sec.\n",
      "Iteration 11800 || Loss: 4.4250 || 10iter: 2.2825 sec.\n",
      "Iteration 11810 || Loss: 4.4780 || 10iter: 1.4976 sec.\n",
      "-------------\n",
      "epoch 179 || Epoch_TRAIN_Loss:294.1999 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1983 sec.\n",
      "-------------\n",
      "Epoch 180/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11820 || Loss: 4.6914 || 10iter: 4.1799 sec.\n",
      "Iteration 11830 || Loss: 4.0038 || 10iter: 2.4575 sec.\n",
      "Iteration 11840 || Loss: 4.2704 || 10iter: 1.9955 sec.\n",
      "Iteration 11850 || Loss: 4.7825 || 10iter: 2.2315 sec.\n",
      "Iteration 11860 || Loss: 4.8792 || 10iter: 2.5235 sec.\n",
      "Iteration 11870 || Loss: 5.3186 || 10iter: 1.8075 sec.\n",
      "Iteration 11880 || Loss: 5.8548 || 10iter: 1.3142 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 180 || Epoch_TRAIN_Loss:299.2495 ||Epoch_VAL_Loss:148.3501\n",
      "timer:  20.6675 sec.\n",
      "-------------\n",
      "Epoch 181/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11890 || Loss: 4.4441 || 10iter: 5.2973 sec.\n",
      "Iteration 11900 || Loss: 4.0554 || 10iter: 2.2467 sec.\n",
      "Iteration 11910 || Loss: 4.1477 || 10iter: 2.1381 sec.\n",
      "Iteration 11920 || Loss: 4.2627 || 10iter: 2.2187 sec.\n",
      "Iteration 11930 || Loss: 4.0483 || 10iter: 2.2512 sec.\n",
      "Iteration 11940 || Loss: 4.2999 || 10iter: 1.5600 sec.\n",
      "-------------\n",
      "epoch 181 || Epoch_TRAIN_Loss:293.8540 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.6483 sec.\n",
      "-------------\n",
      "Epoch 182/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11950 || Loss: 4.7854 || 10iter: 3.4303 sec.\n",
      "Iteration 11960 || Loss: 4.3156 || 10iter: 3.0681 sec.\n",
      "Iteration 11970 || Loss: 4.6210 || 10iter: 2.3031 sec.\n",
      "Iteration 11980 || Loss: 4.3213 || 10iter: 2.1274 sec.\n",
      "Iteration 11990 || Loss: 4.2238 || 10iter: 2.5539 sec.\n",
      "Iteration 12000 || Loss: 4.2517 || 10iter: 2.1172 sec.\n",
      "Iteration 12010 || Loss: 4.3122 || 10iter: 1.6043 sec.\n",
      "-------------\n",
      "epoch 182 || Epoch_TRAIN_Loss:296.6032 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.7175 sec.\n",
      "-------------\n",
      "Epoch 183/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12020 || Loss: 4.2336 || 10iter: 5.0701 sec.\n",
      "Iteration 12030 || Loss: 4.6235 || 10iter: 2.5872 sec.\n",
      "Iteration 12040 || Loss: 4.1202 || 10iter: 2.1260 sec.\n",
      "Iteration 12050 || Loss: 4.5485 || 10iter: 2.2490 sec.\n",
      "Iteration 12060 || Loss: 4.3380 || 10iter: 2.4399 sec.\n",
      "Iteration 12070 || Loss: 4.4950 || 10iter: 1.6186 sec.\n",
      "-------------\n",
      "epoch 183 || Epoch_TRAIN_Loss:290.3251 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.3381 sec.\n",
      "-------------\n",
      "Epoch 184/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12080 || Loss: 4.3610 || 10iter: 2.7057 sec.\n",
      "Iteration 12090 || Loss: 4.5163 || 10iter: 3.0559 sec.\n",
      "Iteration 12100 || Loss: 4.7022 || 10iter: 2.4495 sec.\n",
      "Iteration 12110 || Loss: 4.4324 || 10iter: 2.1582 sec.\n",
      "Iteration 12120 || Loss: 4.4633 || 10iter: 2.2838 sec.\n",
      "Iteration 12130 || Loss: 4.6393 || 10iter: 2.3510 sec.\n",
      "Iteration 12140 || Loss: 4.3263 || 10iter: 1.4727 sec.\n",
      "-------------\n",
      "epoch 184 || Epoch_TRAIN_Loss:291.0585 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1677 sec.\n",
      "-------------\n",
      "Epoch 185/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12150 || Loss: 4.6717 || 10iter: 4.3003 sec.\n",
      "Iteration 12160 || Loss: 4.0935 || 10iter: 2.4409 sec.\n",
      "Iteration 12170 || Loss: 4.5794 || 10iter: 2.1333 sec.\n",
      "Iteration 12180 || Loss: 4.1650 || 10iter: 2.0763 sec.\n",
      "Iteration 12190 || Loss: 4.5371 || 10iter: 2.3666 sec.\n",
      "Iteration 12200 || Loss: 4.8830 || 10iter: 1.9394 sec.\n",
      "Iteration 12210 || Loss: 4.4447 || 10iter: 1.3367 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 185 || Epoch_TRAIN_Loss:294.9496 ||Epoch_VAL_Loss:148.4467\n",
      "timer:  20.7347 sec.\n",
      "-------------\n",
      "Epoch 186/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12220 || Loss: 4.4316 || 10iter: 5.1697 sec.\n",
      "Iteration 12230 || Loss: 4.2800 || 10iter: 2.2284 sec.\n",
      "Iteration 12240 || Loss: 4.3148 || 10iter: 2.1584 sec.\n",
      "Iteration 12250 || Loss: 4.3037 || 10iter: 2.2948 sec.\n",
      "Iteration 12260 || Loss: 4.2075 || 10iter: 2.3677 sec.\n",
      "Iteration 12270 || Loss: 4.5598 || 10iter: 1.4327 sec.\n",
      "-------------\n",
      "epoch 186 || Epoch_TRAIN_Loss:294.7125 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.5451 sec.\n",
      "-------------\n",
      "Epoch 187/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12280 || Loss: 4.6836 || 10iter: 3.6709 sec.\n",
      "Iteration 12290 || Loss: 4.4220 || 10iter: 2.8008 sec.\n",
      "Iteration 12300 || Loss: 4.6295 || 10iter: 1.9880 sec.\n",
      "Iteration 12310 || Loss: 4.6051 || 10iter: 2.1909 sec.\n",
      "Iteration 12320 || Loss: 4.2375 || 10iter: 2.4865 sec.\n",
      "Iteration 12330 || Loss: 4.4901 || 10iter: 2.0903 sec.\n",
      "Iteration 12340 || Loss: 4.2666 || 10iter: 1.5112 sec.\n",
      "-------------\n",
      "epoch 187 || Epoch_TRAIN_Loss:294.5047 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.2033 sec.\n",
      "-------------\n",
      "Epoch 188/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12350 || Loss: 4.7471 || 10iter: 4.7201 sec.\n",
      "Iteration 12360 || Loss: 4.4275 || 10iter: 2.4443 sec.\n",
      "Iteration 12370 || Loss: 4.7256 || 10iter: 2.1618 sec.\n",
      "Iteration 12380 || Loss: 4.2750 || 10iter: 2.1919 sec.\n",
      "Iteration 12390 || Loss: 4.4643 || 10iter: 2.3973 sec.\n",
      "Iteration 12400 || Loss: 4.1708 || 10iter: 1.7357 sec.\n",
      "-------------\n",
      "epoch 188 || Epoch_TRAIN_Loss:292.9429 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8876 sec.\n",
      "-------------\n",
      "Epoch 189/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12410 || Loss: 4.0951 || 10iter: 3.1602 sec.\n",
      "Iteration 12420 || Loss: 4.5250 || 10iter: 2.7525 sec.\n",
      "Iteration 12430 || Loss: 4.0780 || 10iter: 2.3206 sec.\n",
      "Iteration 12440 || Loss: 4.3077 || 10iter: 2.1353 sec.\n",
      "Iteration 12450 || Loss: 4.2796 || 10iter: 2.3593 sec.\n",
      "Iteration 12460 || Loss: 4.5126 || 10iter: 2.2828 sec.\n",
      "Iteration 12470 || Loss: 4.5473 || 10iter: 1.4478 sec.\n",
      "-------------\n",
      "epoch 189 || Epoch_TRAIN_Loss:294.3651 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1241 sec.\n",
      "-------------\n",
      "Epoch 190/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12480 || Loss: 4.5293 || 10iter: 3.9568 sec.\n",
      "Iteration 12490 || Loss: 4.2512 || 10iter: 2.6958 sec.\n",
      "Iteration 12500 || Loss: 4.4182 || 10iter: 2.4451 sec.\n",
      "Iteration 12510 || Loss: 4.7100 || 10iter: 2.0567 sec.\n",
      "Iteration 12520 || Loss: 4.2791 || 10iter: 2.3747 sec.\n",
      "Iteration 12530 || Loss: 4.8318 || 10iter: 1.9898 sec.\n",
      "Iteration 12540 || Loss: 3.9356 || 10iter: 1.4287 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 190 || Epoch_TRAIN_Loss:292.9810 ||Epoch_VAL_Loss:148.9359\n",
      "timer:  21.1946 sec.\n",
      "-------------\n",
      "Epoch 191/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12550 || Loss: 4.5172 || 10iter: 5.6253 sec.\n",
      "Iteration 12560 || Loss: 4.3796 || 10iter: 2.3266 sec.\n",
      "Iteration 12570 || Loss: 4.6555 || 10iter: 2.1224 sec.\n",
      "Iteration 12580 || Loss: 4.7347 || 10iter: 2.3201 sec.\n",
      "Iteration 12590 || Loss: 4.7127 || 10iter: 2.2676 sec.\n",
      "Iteration 12600 || Loss: 4.3435 || 10iter: 1.4460 sec.\n",
      "-------------\n",
      "epoch 191 || Epoch_TRAIN_Loss:292.9367 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.9853 sec.\n",
      "-------------\n",
      "Epoch 192/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12610 || Loss: 4.6453 || 10iter: 3.5877 sec.\n",
      "Iteration 12620 || Loss: 4.4786 || 10iter: 2.7767 sec.\n",
      "Iteration 12630 || Loss: 4.4478 || 10iter: 2.2681 sec.\n",
      "Iteration 12640 || Loss: 3.9873 || 10iter: 2.1010 sec.\n",
      "Iteration 12650 || Loss: 4.4431 || 10iter: 2.6288 sec.\n",
      "Iteration 12660 || Loss: 4.5349 || 10iter: 1.9526 sec.\n",
      "Iteration 12670 || Loss: 4.4284 || 10iter: 1.3501 sec.\n",
      "-------------\n",
      "epoch 192 || Epoch_TRAIN_Loss:292.8070 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.0278 sec.\n",
      "-------------\n",
      "Epoch 193/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12680 || Loss: 4.1385 || 10iter: 5.0313 sec.\n",
      "Iteration 12690 || Loss: 4.2600 || 10iter: 3.0776 sec.\n",
      "Iteration 12700 || Loss: 4.1802 || 10iter: 2.2575 sec.\n",
      "Iteration 12710 || Loss: 4.7844 || 10iter: 2.2168 sec.\n",
      "Iteration 12720 || Loss: 4.5889 || 10iter: 2.4118 sec.\n",
      "Iteration 12730 || Loss: 4.4885 || 10iter: 1.9359 sec.\n",
      "-------------\n",
      "epoch 193 || Epoch_TRAIN_Loss:291.5521 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3180 sec.\n",
      "-------------\n",
      "Epoch 194/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12740 || Loss: 4.2869 || 10iter: 2.7229 sec.\n",
      "Iteration 12750 || Loss: 3.7099 || 10iter: 3.0880 sec.\n",
      "Iteration 12760 || Loss: 4.5831 || 10iter: 2.1502 sec.\n",
      "Iteration 12770 || Loss: 4.3018 || 10iter: 2.0176 sec.\n",
      "Iteration 12780 || Loss: 4.3565 || 10iter: 2.7460 sec.\n",
      "Iteration 12790 || Loss: 4.8512 || 10iter: 2.0239 sec.\n",
      "Iteration 12800 || Loss: 4.4596 || 10iter: 1.3470 sec.\n",
      "-------------\n",
      "epoch 194 || Epoch_TRAIN_Loss:293.0760 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.7472 sec.\n",
      "-------------\n",
      "Epoch 195/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12810 || Loss: 4.3387 || 10iter: 4.0104 sec.\n",
      "Iteration 12820 || Loss: 4.3527 || 10iter: 2.6737 sec.\n",
      "Iteration 12830 || Loss: 4.6430 || 10iter: 2.2257 sec.\n",
      "Iteration 12840 || Loss: 4.5104 || 10iter: 2.2792 sec.\n",
      "Iteration 12850 || Loss: 4.5702 || 10iter: 2.3295 sec.\n",
      "Iteration 12860 || Loss: 4.9373 || 10iter: 1.7071 sec.\n",
      "Iteration 12870 || Loss: 4.6082 || 10iter: 1.3002 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 195 || Epoch_TRAIN_Loss:293.0361 ||Epoch_VAL_Loss:146.8578\n",
      "timer:  20.7413 sec.\n",
      "-------------\n",
      "Epoch 196/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12880 || Loss: 4.4553 || 10iter: 5.5509 sec.\n",
      "Iteration 12890 || Loss: 4.0621 || 10iter: 2.3493 sec.\n",
      "Iteration 12900 || Loss: 4.4599 || 10iter: 2.2236 sec.\n",
      "Iteration 12910 || Loss: 4.2909 || 10iter: 2.2726 sec.\n",
      "Iteration 12920 || Loss: 4.5345 || 10iter: 2.4211 sec.\n",
      "Iteration 12930 || Loss: 4.3317 || 10iter: 1.4589 sec.\n",
      "-------------\n",
      "epoch 196 || Epoch_TRAIN_Loss:293.8351 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.1330 sec.\n",
      "-------------\n",
      "Epoch 197/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12940 || Loss: 4.3146 || 10iter: 3.4398 sec.\n",
      "Iteration 12950 || Loss: 4.4954 || 10iter: 2.5989 sec.\n",
      "Iteration 12960 || Loss: 4.2119 || 10iter: 2.1560 sec.\n",
      "Iteration 12970 || Loss: 4.2234 || 10iter: 2.1202 sec.\n",
      "Iteration 12980 || Loss: 4.1818 || 10iter: 2.3297 sec.\n",
      "Iteration 12990 || Loss: 4.0273 || 10iter: 2.0510 sec.\n",
      "Iteration 13000 || Loss: 4.5769 || 10iter: 1.3603 sec.\n",
      "-------------\n",
      "epoch 197 || Epoch_TRAIN_Loss:289.5578 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.4109 sec.\n",
      "-------------\n",
      "Epoch 198/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13010 || Loss: 4.3665 || 10iter: 4.8966 sec.\n",
      "Iteration 13020 || Loss: 4.5615 || 10iter: 2.4770 sec.\n",
      "Iteration 13030 || Loss: 4.9107 || 10iter: 2.1351 sec.\n",
      "Iteration 13040 || Loss: 4.1757 || 10iter: 2.4304 sec.\n",
      "Iteration 13050 || Loss: 4.2917 || 10iter: 2.4645 sec.\n",
      "Iteration 13060 || Loss: 4.6337 || 10iter: 1.7256 sec.\n",
      "-------------\n",
      "epoch 198 || Epoch_TRAIN_Loss:292.8075 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.4271 sec.\n",
      "-------------\n",
      "Epoch 199/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13070 || Loss: 4.5026 || 10iter: 2.6669 sec.\n",
      "Iteration 13080 || Loss: 4.7912 || 10iter: 3.1174 sec.\n",
      "Iteration 13090 || Loss: 4.2183 || 10iter: 2.3942 sec.\n",
      "Iteration 13100 || Loss: 5.0564 || 10iter: 2.0487 sec.\n",
      "Iteration 13110 || Loss: 4.4809 || 10iter: 2.2083 sec.\n",
      "Iteration 13120 || Loss: 4.0811 || 10iter: 2.3586 sec.\n",
      "Iteration 13130 || Loss: 4.3308 || 10iter: 1.4255 sec.\n",
      "-------------\n",
      "epoch 199 || Epoch_TRAIN_Loss:293.0663 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  16.8906 sec.\n",
      "-------------\n",
      "Epoch 200/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13140 || Loss: 4.4624 || 10iter: 4.1557 sec.\n",
      "Iteration 13150 || Loss: 4.3610 || 10iter: 2.7478 sec.\n",
      "Iteration 13160 || Loss: 4.6614 || 10iter: 2.1230 sec.\n",
      "Iteration 13170 || Loss: 4.8148 || 10iter: 2.3253 sec.\n",
      "Iteration 13180 || Loss: 4.5848 || 10iter: 2.4467 sec.\n",
      "Iteration 13190 || Loss: 5.1140 || 10iter: 1.7901 sec.\n",
      "Iteration 13200 || Loss: 4.6754 || 10iter: 1.3101 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 200 || Epoch_TRAIN_Loss:293.6192 ||Epoch_VAL_Loss:147.7686\n",
      "timer:  21.0739 sec.\n",
      "-------------\n",
      "Epoch 201/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13210 || Loss: 4.3214 || 10iter: 5.5073 sec.\n",
      "Iteration 13220 || Loss: 4.6413 || 10iter: 2.4480 sec.\n",
      "Iteration 13230 || Loss: 4.5327 || 10iter: 2.2147 sec.\n",
      "Iteration 13240 || Loss: 4.5541 || 10iter: 2.4051 sec.\n",
      "Iteration 13250 || Loss: 4.8597 || 10iter: 2.2672 sec.\n",
      "Iteration 13260 || Loss: 4.3607 || 10iter: 1.6764 sec.\n",
      "-------------\n",
      "epoch 201 || Epoch_TRAIN_Loss:294.9385 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.5891 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
