{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000009\n",
      "000017\n",
      "000021\n",
      "000023\n",
      "000030\n",
      "000032\n",
      "000035\n",
      "000041\n",
      "000048\n",
      "000050\n",
      "000051\n",
      "000066\n",
      "000073\n",
      "000081\n",
      "000083\n",
      "000089\n",
      "000101\n",
      "000104\n",
      "000110\n",
      "000113\n",
      "000125\n",
      "000129\n",
      "000131\n",
      "000133\n",
      "000138\n",
      "000146\n",
      "000150\n",
      "000159\n",
      "000162\n",
      "000163\n",
      "000164\n",
      "000165\n",
      "000169\n",
      "000170\n",
      "000171\n",
      "000173\n",
      "000174\n",
      "000177\n",
      "000190\n",
      "000192\n",
      "000193\n",
      "000194\n",
      "000200\n",
      "000210\n",
      "000218\n",
      "000220\n",
      "000222\n",
      "000229\n",
      "000232\n",
      "000245\n",
      "000251\n",
      "000257\n",
      "000259\n",
      "000269\n",
      "000275\n",
      "000276\n",
      "000278\n",
      "000282\n",
      "000285\n",
      "000288\n",
      "000298\n",
      "000302\n",
      "000305\n",
      "000308\n",
      "000320\n",
      "000321\n",
      "000322\n",
      "000323\n",
      "000328\n",
      "000331\n",
      "000337\n",
      "000338\n",
      "000352\n",
      "000359\n",
      "000367\n",
      "000372\n",
      "000374\n",
      "000382\n",
      "000394\n",
      "000406\n",
      "000407\n",
      "000411\n",
      "000419\n",
      "000428\n",
      "000433\n",
      "000435\n",
      "000438\n",
      "000443\n",
      "000446\n",
      "000448\n",
      "000463\n",
      "000468\n",
      "000470\n",
      "000476\n",
      "000477\n",
      "000480\n",
      "000482\n",
      "000483\n",
      "000498\n",
      "000499\n",
      "000500\n",
      "000515\n",
      "000516\n",
      "000518\n",
      "000520\n",
      "000523\n",
      "000524\n",
      "000525\n",
      "000526\n",
      "000530\n",
      "000531\n",
      "000535\n",
      "000541\n",
      "000545\n",
      "000554\n",
      "000555\n",
      "000579\n",
      "000583\n",
      "000589\n",
      "000591\n",
      "000597\n",
      "000612\n",
      "000613\n",
      "000625\n",
      "000626\n",
      "000628\n",
      "000633\n",
      "000648\n",
      "000654\n",
      "000677\n",
      "000684\n",
      "000688\n",
      "000690\n",
      "000694\n",
      "000695\n",
      "000702\n",
      "000709\n",
      "000717\n",
      "000726\n",
      "000731\n",
      "000733\n",
      "000739\n",
      "000742\n",
      "000750\n",
      "000752\n",
      "000753\n",
      "000755\n",
      "000760\n",
      "000770\n",
      "000777\n",
      "000782\n",
      "000786\n",
      "000793\n",
      "000797\n",
      "000799\n",
      "000802\n",
      "000805\n",
      "000806\n",
      "000810\n",
      "000812\n",
      "000814\n",
      "000816\n",
      "000828\n",
      "000829\n",
      "000834\n",
      "000842\n",
      "000843\n",
      "000847\n",
      "000848\n",
      "000851\n",
      "000854\n",
      "000855\n",
      "000859\n",
      "000860\n",
      "000862\n",
      "000865\n",
      "000874\n",
      "000878\n",
      "000879\n",
      "000885\n",
      "000892\n",
      "000895\n",
      "000898\n",
      "000902\n",
      "000903\n",
      "000904\n",
      "000906\n",
      "000911\n",
      "000915\n",
      "000918\n",
      "000920\n",
      "000926\n",
      "000937\n",
      "000943\n",
      "000948\n",
      "000949\n",
      "000966\n",
      "000967\n",
      "000971\n",
      "000982\n",
      "000987\n",
      "000991\n",
      "000996\n",
      "000999\n",
      "001001\n",
      "001011\n",
      "001014\n",
      "001017\n",
      "001024\n",
      "001028\n",
      "001036\n",
      "001042\n",
      "001050\n",
      "001057\n",
      "001060\n",
      "001061\n",
      "001064\n",
      "001066\n",
      "001071\n",
      "001072\n",
      "001079\n",
      "001084\n",
      "001091\n",
      "001092\n",
      "001097\n",
      "001101\n",
      "001109\n",
      "001113\n",
      "001125\n",
      "001129\n",
      "001137\n",
      "001140\n",
      "001145\n",
      "001147\n",
      "001151\n",
      "001152\n",
      "001164\n",
      "001168\n",
      "001170\n",
      "001171\n",
      "001175\n",
      "001185\n",
      "001186\n",
      "001201\n",
      "001206\n",
      "001212\n",
      "001221\n",
      "001224\n",
      "001229\n",
      "001234\n",
      "001236\n",
      "001240\n",
      "001241\n",
      "001248\n",
      "001254\n",
      "001259\n",
      "001265\n",
      "001266\n",
      "001269\n",
      "001272\n",
      "001279\n",
      "001281\n",
      "001284\n",
      "001287\n",
      "001292\n",
      "001298\n",
      "001304\n",
      "001309\n",
      "001310\n",
      "001311\n",
      "001315\n",
      "001325\n",
      "001327\n",
      "001330\n",
      "001333\n",
      "001337\n",
      "001346\n",
      "001350\n",
      "001352\n",
      "001362\n",
      "001378\n",
      "001388\n",
      "001390\n",
      "001393\n",
      "001405\n",
      "001406\n",
      "001408\n",
      "001409\n",
      "001414\n",
      "001420\n",
      "001421\n",
      "001426\n",
      "001427\n",
      "001430\n",
      "001434\n",
      "001445\n",
      "001450\n",
      "001451\n",
      "001455\n",
      "001460\n",
      "001463\n",
      "001472\n",
      "001475\n",
      "001479\n",
      "001480\n",
      "001484\n",
      "001485\n",
      "001493\n",
      "001498\n",
      "001499\n",
      "001501\n",
      "001504\n",
      "001509\n",
      "001510\n",
      "001514\n",
      "001521\n",
      "001523\n",
      "001524\n",
      "001526\n",
      "001531\n",
      "001532\n",
      "001536\n",
      "001537\n",
      "001544\n",
      "001548\n",
      "001554\n",
      "001557\n",
      "001561\n",
      "001563\n",
      "001571\n",
      "001577\n",
      "001579\n",
      "001580\n",
      "001586\n",
      "001593\n",
      "001594\n",
      "001603\n",
      "001608\n",
      "001611\n",
      "001617\n",
      "001622\n",
      "001627\n",
      "001628\n",
      "001630\n",
      "001640\n",
      "001649\n",
      "001651\n",
      "001654\n",
      "001678\n",
      "001682\n",
      "001686\n",
      "001689\n",
      "001690\n",
      "001691\n",
      "001711\n",
      "001717\n",
      "001721\n",
      "001723\n",
      "001725\n",
      "001726\n",
      "001727\n",
      "001729\n",
      "001730\n",
      "001749\n",
      "001754\n",
      "001756\n",
      "001758\n",
      "001759\n",
      "001766\n",
      "001772\n",
      "001775\n",
      "001784\n",
      "001785\n",
      "001789\n",
      "001793\n",
      "001795\n",
      "001797\n",
      "001799\n",
      "001800\n",
      "001801\n",
      "001806\n",
      "001807\n",
      "001809\n",
      "001810\n",
      "001828\n",
      "001830\n",
      "001832\n",
      "001833\n",
      "001836\n",
      "001837\n",
      "001841\n",
      "001843\n",
      "001849\n",
      "001854\n",
      "001861\n",
      "001864\n",
      "001881\n",
      "001887\n",
      "001894\n",
      "001899\n",
      "001902\n",
      "001903\n",
      "001904\n",
      "001918\n",
      "001920\n",
      "001927\n",
      "001930\n",
      "001932\n",
      "001938\n",
      "001941\n",
      "001944\n",
      "001945\n",
      "001950\n",
      "001958\n",
      "001963\n",
      "001977\n",
      "001978\n",
      "001980\n",
      "001989\n",
      "001999\n",
      "002000\n",
      "002012\n",
      "002022\n",
      "002024\n",
      "002030\n",
      "002051\n",
      "002055\n",
      "002061\n",
      "002069\n",
      "002082\n",
      "002090\n",
      "002091\n",
      "002096\n",
      "002104\n",
      "002108\n",
      "002114\n",
      "002116\n",
      "002117\n",
      "002120\n",
      "002142\n",
      "002145\n",
      "002156\n",
      "002163\n",
      "002169\n",
      "002170\n",
      "002171\n",
      "002172\n",
      "002174\n",
      "002176\n",
      "002180\n",
      "002183\n",
      "002186\n",
      "002187\n",
      "002190\n",
      "002193\n",
      "002194\n",
      "002213\n",
      "002214\n",
      "002219\n",
      "002220\n",
      "002221\n",
      "002224\n",
      "002226\n",
      "002233\n",
      "002238\n",
      "002241\n",
      "002244\n",
      "002256\n",
      "002261\n",
      "002265\n",
      "002272\n",
      "002273\n",
      "002277\n",
      "002281\n",
      "002285\n",
      "002288\n",
      "002290\n",
      "002293\n",
      "002302\n",
      "002306\n",
      "002307\n",
      "002311\n",
      "002324\n",
      "002329\n",
      "002332\n",
      "002337\n",
      "002342\n",
      "002361\n",
      "002362\n",
      "002367\n",
      "002372\n",
      "002378\n",
      "002382\n",
      "002384\n",
      "002385\n",
      "002387\n",
      "002401\n",
      "002403\n",
      "002405\n",
      "002410\n",
      "002413\n",
      "002419\n",
      "002425\n",
      "002433\n",
      "002443\n",
      "002444\n",
      "002448\n",
      "002450\n",
      "002458\n",
      "002459\n",
      "002460\n",
      "002472\n",
      "002476\n",
      "002477\n",
      "002481\n",
      "002483\n",
      "002491\n",
      "002497\n",
      "002501\n",
      "002504\n",
      "002514\n",
      "002518\n",
      "002520\n",
      "002524\n",
      "002529\n",
      "002533\n",
      "002539\n",
      "002540\n",
      "002542\n",
      "002545\n",
      "002554\n",
      "002555\n",
      "002558\n",
      "002561\n",
      "002563\n",
      "002564\n",
      "002565\n",
      "002566\n",
      "002572\n",
      "002584\n",
      "002585\n",
      "002589\n",
      "002595\n",
      "002611\n",
      "002613\n",
      "002625\n",
      "002632\n",
      "002633\n",
      "002636\n",
      "002637\n",
      "002641\n",
      "002643\n",
      "002649\n",
      "002657\n",
      "002658\n",
      "002662\n",
      "002668\n",
      "002669\n",
      "002675\n",
      "002678\n",
      "002689\n",
      "002691\n",
      "002693\n",
      "002696\n",
      "002710\n",
      "002713\n",
      "002714\n",
      "002715\n",
      "002723\n",
      "002727\n",
      "002730\n",
      "002735\n",
      "002741\n",
      "002744\n",
      "002751\n",
      "002755\n",
      "002759\n",
      "002767\n",
      "002776\n",
      "002786\n",
      "002795\n",
      "002798\n",
      "002803\n",
      "002804\n",
      "002807\n",
      "002815\n",
      "002826\n",
      "002827\n",
      "002834\n",
      "002838\n",
      "002841\n",
      "002842\n",
      "002855\n",
      "002858\n",
      "002859\n",
      "002864\n",
      "002867\n",
      "002870\n",
      "002879\n",
      "002881\n",
      "002884\n",
      "002886\n",
      "002889\n",
      "002899\n",
      "002901\n",
      "002912\n",
      "002913\n",
      "002914\n",
      "002915\n",
      "002916\n",
      "002931\n",
      "002932\n",
      "002933\n",
      "002935\n",
      "002939\n",
      "002940\n",
      "002954\n",
      "002958\n",
      "002960\n",
      "002962\n",
      "002965\n",
      "002975\n",
      "002976\n",
      "002987\n",
      "002988\n",
      "002990\n",
      "002992\n",
      "003002\n",
      "003003\n",
      "003017\n",
      "003023\n",
      "003024\n",
      "003027\n",
      "003031\n",
      "003034\n",
      "003039\n",
      "003042\n",
      "003044\n",
      "003057\n",
      "003058\n",
      "003063\n",
      "003064\n",
      "003065\n",
      "003077\n",
      "003078\n",
      "003086\n",
      "003088\n",
      "003089\n",
      "003100\n",
      "003102\n",
      "003106\n",
      "003107\n",
      "003118\n",
      "003121\n",
      "003122\n",
      "003124\n",
      "003129\n",
      "003134\n",
      "003138\n",
      "003140\n",
      "003145\n",
      "003147\n",
      "003150\n",
      "003159\n",
      "003162\n",
      "003170\n",
      "003175\n",
      "003181\n",
      "003185\n",
      "003188\n",
      "003189\n",
      "003205\n",
      "003210\n",
      "003213\n",
      "003218\n",
      "003228\n",
      "003236\n",
      "003247\n",
      "003253\n",
      "003259\n",
      "003260\n",
      "003261\n",
      "003270\n",
      "003280\n",
      "003282\n",
      "003284\n",
      "003290\n",
      "003292\n",
      "003296\n",
      "003301\n",
      "003303\n",
      "003311\n",
      "003313\n",
      "003320\n",
      "003327\n",
      "003331\n",
      "003335\n",
      "003344\n",
      "003350\n",
      "003355\n",
      "003360\n",
      "003362\n",
      "003367\n",
      "003369\n",
      "003373\n",
      "003376\n",
      "003380\n",
      "003382\n",
      "003386\n",
      "003390\n",
      "003391\n",
      "003395\n",
      "003396\n",
      "003397\n",
      "003403\n",
      "003406\n",
      "003407\n",
      "003412\n",
      "003416\n",
      "003417\n",
      "003422\n",
      "003424\n",
      "003429\n",
      "003433\n",
      "003435\n",
      "003436\n",
      "003450\n",
      "003453\n",
      "003455\n",
      "003462\n",
      "003465\n",
      "003466\n",
      "003468\n",
      "003469\n",
      "003470\n",
      "003484\n",
      "003487\n",
      "003492\n",
      "003496\n",
      "003510\n",
      "003518\n",
      "003521\n",
      "003528\n",
      "003529\n",
      "003530\n",
      "003536\n",
      "003539\n",
      "003548\n",
      "003550\n",
      "003556\n",
      "003566\n",
      "003567\n",
      "003575\n",
      "003576\n",
      "003577\n",
      "003580\n",
      "003585\n",
      "003589\n",
      "003603\n",
      "003611\n",
      "003618\n",
      "003621\n",
      "003625\n",
      "003628\n",
      "003638\n",
      "003644\n",
      "003645\n",
      "003648\n",
      "003651\n",
      "003654\n",
      "003655\n",
      "003662\n",
      "003663\n",
      "003664\n",
      "003669\n",
      "003678\n",
      "003685\n",
      "003688\n",
      "003690\n",
      "003691\n",
      "003694\n",
      "003696\n",
      "003699\n",
      "003700\n",
      "003703\n",
      "003706\n",
      "003708\n",
      "003711\n",
      "003717\n",
      "003732\n",
      "003735\n",
      "003740\n",
      "003749\n",
      "003751\n",
      "003759\n",
      "003760\n",
      "003779\n",
      "003780\n",
      "003781\n",
      "003797\n",
      "003811\n",
      "003817\n",
      "003820\n",
      "003824\n",
      "003826\n",
      "003827\n",
      "003838\n",
      "003847\n",
      "003849\n",
      "003855\n",
      "003856\n",
      "003859\n",
      "003860\n",
      "003863\n",
      "003865\n",
      "003868\n",
      "003869\n",
      "003877\n",
      "003879\n",
      "003889\n",
      "003891\n",
      "003899\n",
      "003913\n",
      "003915\n",
      "003923\n",
      "003932\n",
      "003939\n",
      "003945\n",
      "003946\n",
      "003948\n",
      "003953\n",
      "003960\n",
      "003961\n",
      "003963\n",
      "003966\n",
      "003970\n",
      "003973\n",
      "003974\n",
      "003986\n",
      "003992\n",
      "003994\n",
      "003996\n",
      "004008\n",
      "004010\n",
      "004012\n",
      "004014\n",
      "004016\n",
      "004017\n",
      "004019\n",
      "004020\n",
      "004028\n",
      "004031\n",
      "004033\n",
      "004039\n",
      "004046\n",
      "004047\n",
      "004051\n",
      "004058\n",
      "004060\n",
      "004067\n",
      "004073\n",
      "004075\n",
      "004077\n",
      "004085\n",
      "004089\n",
      "004091\n",
      "004092\n",
      "004110\n",
      "004111\n",
      "004117\n",
      "004121\n",
      "004122\n",
      "004133\n",
      "004136\n",
      "004137\n",
      "004138\n",
      "004141\n",
      "004142\n",
      "004145\n",
      "004158\n",
      "004168\n",
      "004169\n",
      "004170\n",
      "004191\n",
      "004195\n",
      "004201\n",
      "004205\n",
      "004209\n",
      "004223\n",
      "004224\n",
      "004229\n",
      "004230\n",
      "004231\n",
      "004232\n",
      "004237\n",
      "004244\n",
      "004246\n",
      "004257\n",
      "004258\n",
      "004259\n",
      "004265\n",
      "004269\n",
      "004270\n",
      "004273\n",
      "004275\n",
      "004279\n",
      "004281\n",
      "004287\n",
      "004291\n",
      "004292\n",
      "004296\n",
      "004298\n",
      "004304\n",
      "004321\n",
      "004322\n",
      "004323\n",
      "004327\n",
      "004329\n",
      "004339\n",
      "004346\n",
      "004356\n",
      "004359\n",
      "004360\n",
      "004361\n",
      "004370\n",
      "004372\n",
      "004380\n",
      "004384\n",
      "004387\n",
      "004389\n",
      "004392\n",
      "004397\n",
      "004404\n",
      "004424\n",
      "004433\n",
      "004437\n",
      "004438\n",
      "004457\n",
      "004459\n",
      "004463\n",
      "004464\n",
      "004466\n",
      "004470\n",
      "004471\n",
      "004479\n",
      "004481\n",
      "004487\n",
      "004494\n",
      "004495\n",
      "004496\n",
      "004499\n",
      "004508\n",
      "004509\n",
      "004512\n",
      "004517\n",
      "004518\n",
      "004520\n",
      "004528\n",
      "004530\n",
      "004535\n",
      "004537\n",
      "004542\n",
      "004553\n",
      "004562\n",
      "004563\n",
      "004571\n",
      "004574\n",
      "004588\n",
      "004595\n",
      "004601\n",
      "004604\n",
      "004605\n",
      "004607\n",
      "004609\n",
      "004611\n",
      "004622\n",
      "004630\n",
      "004631\n",
      "004647\n",
      "004648\n",
      "004653\n",
      "004656\n",
      "004660\n",
      "004662\n",
      "004672\n",
      "004673\n",
      "004676\n",
      "004679\n",
      "004685\n",
      "004686\n",
      "004687\n",
      "004692\n",
      "004693\n",
      "004694\n",
      "004699\n",
      "004706\n",
      "004707\n",
      "004715\n",
      "004718\n",
      "004722\n",
      "004723\n",
      "004727\n",
      "004732\n",
      "004742\n",
      "004750\n",
      "004753\n",
      "004760\n",
      "004761\n",
      "004770\n",
      "004779\n",
      "004782\n",
      "004788\n",
      "004789\n",
      "004790\n",
      "004796\n",
      "004797\n",
      "004801\n",
      "004812\n",
      "004815\n",
      "004826\n",
      "004828\n",
      "004830\n",
      "004831\n",
      "004832\n",
      "004834\n",
      "004840\n",
      "004846\n",
      "004852\n",
      "004857\n",
      "004859\n",
      "004868\n",
      "004872\n",
      "004879\n",
      "004882\n",
      "004886\n",
      "004897\n",
      "004898\n",
      "004902\n",
      "004913\n",
      "004916\n",
      "004928\n",
      "004931\n",
      "004935\n",
      "004939\n",
      "004943\n",
      "004946\n",
      "004950\n",
      "004953\n",
      "004954\n",
      "004968\n",
      "004976\n",
      "004977\n",
      "004985\n",
      "004987\n",
      "004990\n",
      "004994\n",
      "004995\n",
      "005014\n",
      "005024\n",
      "005027\n",
      "005029\n",
      "005036\n",
      "005037\n",
      "005039\n",
      "005042\n",
      "005045\n",
      "005052\n",
      "005054\n",
      "005057\n",
      "005061\n",
      "005063\n",
      "005064\n",
      "005081\n",
      "005086\n",
      "005090\n",
      "005093\n",
      "005097\n",
      "005102\n",
      "005107\n",
      "005111\n",
      "005114\n",
      "005116\n",
      "005122\n",
      "005124\n",
      "005129\n",
      "005134\n",
      "005144\n",
      "005146\n",
      "005156\n",
      "005159\n",
      "005160\n",
      "005161\n",
      "005173\n",
      "005176\n",
      "005189\n",
      "005190\n",
      "005195\n",
      "005208\n",
      "005219\n",
      "005220\n",
      "005222\n",
      "005229\n",
      "005230\n",
      "005231\n",
      "005236\n",
      "005239\n",
      "005242\n",
      "005245\n",
      "005248\n",
      "005257\n",
      "005260\n",
      "005268\n",
      "005269\n",
      "005273\n",
      "005274\n",
      "005278\n",
      "005281\n",
      "005283\n",
      "005297\n",
      "005298\n",
      "005303\n",
      "005304\n",
      "005306\n",
      "005307\n",
      "005310\n",
      "005311\n",
      "005315\n",
      "005326\n",
      "005327\n",
      "005331\n",
      "005336\n",
      "005343\n",
      "005344\n",
      "005346\n",
      "005348\n",
      "005351\n",
      "005368\n",
      "005371\n",
      "005374\n",
      "005383\n",
      "005384\n",
      "005385\n",
      "005396\n",
      "005398\n",
      "005405\n",
      "005410\n",
      "005413\n",
      "005414\n",
      "005417\n",
      "005419\n",
      "005421\n",
      "005424\n",
      "005429\n",
      "005430\n",
      "005439\n",
      "005441\n",
      "005448\n",
      "005450\n",
      "005451\n",
      "005457\n",
      "005461\n",
      "005467\n",
      "005471\n",
      "005475\n",
      "005478\n",
      "005485\n",
      "005487\n",
      "005496\n",
      "005499\n",
      "005507\n",
      "005508\n",
      "005509\n",
      "005511\n",
      "005522\n",
      "005526\n",
      "005527\n",
      "005536\n",
      "005541\n",
      "005542\n",
      "005547\n",
      "005550\n",
      "005552\n",
      "005554\n",
      "005563\n",
      "005573\n",
      "005574\n",
      "005582\n",
      "005583\n",
      "005585\n",
      "005593\n",
      "005599\n",
      "005600\n",
      "005606\n",
      "005608\n",
      "005611\n",
      "005613\n",
      "005615\n",
      "005618\n",
      "005631\n",
      "005636\n",
      "005639\n",
      "005644\n",
      "005645\n",
      "005648\n",
      "005652\n",
      "005653\n",
      "005657\n",
      "005658\n",
      "005660\n",
      "005662\n",
      "005664\n",
      "005669\n",
      "005672\n",
      "005680\n",
      "005682\n",
      "005693\n",
      "005695\n",
      "005700\n",
      "005702\n",
      "005704\n",
      "005714\n",
      "005716\n",
      "005719\n",
      "005728\n",
      "005729\n",
      "005731\n",
      "005732\n",
      "005738\n",
      "005740\n",
      "005741\n",
      "005747\n",
      "005757\n",
      "005761\n",
      "005764\n",
      "005765\n",
      "005768\n",
      "005773\n",
      "005780\n",
      "005784\n",
      "005786\n",
      "005788\n",
      "005790\n",
      "005796\n",
      "005799\n",
      "005811\n",
      "005814\n",
      "005818\n",
      "005824\n",
      "005826\n",
      "005829\n",
      "005836\n",
      "005840\n",
      "005845\n",
      "005850\n",
      "005852\n",
      "005856\n",
      "005859\n",
      "005863\n",
      "005873\n",
      "005881\n",
      "005884\n",
      "005885\n",
      "005889\n",
      "005903\n",
      "005905\n",
      "005906\n",
      "005908\n",
      "005910\n",
      "005919\n",
      "005923\n",
      "005928\n",
      "005930\n",
      "005938\n",
      "005940\n",
      "005948\n",
      "005951\n",
      "005956\n",
      "005961\n",
      "005971\n",
      "005975\n",
      "005979\n",
      "005984\n",
      "005985\n",
      "005988\n",
      "005989\n",
      "005990\n",
      "005991\n",
      "005992\n",
      "006004\n",
      "006005\n",
      "006011\n",
      "006028\n",
      "006029\n",
      "006033\n",
      "006038\n",
      "006041\n",
      "006043\n",
      "006071\n",
      "006073\n",
      "006095\n",
      "006103\n",
      "006104\n",
      "006117\n",
      "006128\n",
      "006130\n",
      "006133\n",
      "006140\n",
      "006148\n",
      "006150\n",
      "006151\n",
      "006156\n",
      "006158\n",
      "006161\n",
      "006163\n",
      "006171\n",
      "006172\n",
      "006175\n",
      "006176\n",
      "006177\n",
      "006179\n",
      "006180\n",
      "006185\n",
      "006187\n",
      "006198\n",
      "006209\n",
      "006210\n",
      "006221\n",
      "006225\n",
      "006229\n",
      "006233\n",
      "006235\n",
      "006236\n",
      "006238\n",
      "006240\n",
      "006243\n",
      "006259\n",
      "006272\n",
      "006276\n",
      "006282\n",
      "006284\n",
      "006285\n",
      "006286\n",
      "006289\n",
      "006296\n",
      "006300\n",
      "006320\n",
      "006339\n",
      "006341\n",
      "006344\n",
      "006346\n",
      "006351\n",
      "006352\n",
      "006355\n",
      "006369\n",
      "006381\n",
      "006387\n",
      "006391\n",
      "006392\n",
      "006395\n",
      "006398\n",
      "006409\n",
      "006427\n",
      "006428\n",
      "006433\n",
      "006440\n",
      "006442\n",
      "006444\n",
      "006445\n",
      "006450\n",
      "006455\n",
      "006459\n",
      "006463\n",
      "006465\n",
      "006475\n",
      "006476\n",
      "006482\n",
      "006483\n",
      "006486\n",
      "006488\n",
      "006492\n",
      "006497\n",
      "006503\n",
      "006506\n",
      "006519\n",
      "006529\n",
      "006532\n",
      "006536\n",
      "006542\n",
      "006550\n",
      "006551\n",
      "006560\n",
      "006572\n",
      "006576\n",
      "006578\n",
      "006583\n",
      "006584\n",
      "006585\n",
      "006587\n",
      "006588\n",
      "006599\n",
      "006602\n",
      "006609\n",
      "006610\n",
      "006611\n",
      "006612\n",
      "006617\n",
      "006618\n",
      "006621\n",
      "006625\n",
      "006626\n",
      "006631\n",
      "006637\n",
      "006645\n",
      "006647\n",
      "006648\n",
      "006658\n",
      "006661\n",
      "006664\n",
      "006666\n",
      "006667\n",
      "006668\n",
      "006670\n",
      "006677\n",
      "006682\n",
      "006690\n",
      "006695\n",
      "006696\n",
      "006697\n",
      "006698\n",
      "006699\n",
      "006703\n",
      "006707\n",
      "006709\n",
      "006714\n",
      "006718\n",
      "006722\n",
      "006727\n",
      "006731\n",
      "006735\n",
      "006738\n",
      "006739\n",
      "006748\n",
      "006753\n",
      "006762\n",
      "006766\n",
      "006768\n",
      "006772\n",
      "006777\n",
      "006782\n",
      "006783\n",
      "006784\n",
      "006786\n",
      "006789\n",
      "006797\n",
      "006800\n",
      "006802\n",
      "006803\n",
      "006808\n",
      "006810\n",
      "006821\n",
      "006824\n",
      "006833\n",
      "006835\n",
      "006836\n",
      "006838\n",
      "006840\n",
      "006841\n",
      "006847\n",
      "006850\n",
      "006855\n",
      "006858\n",
      "006862\n",
      "006864\n",
      "006866\n",
      "006868\n",
      "006874\n",
      "006876\n",
      "006880\n",
      "006887\n",
      "006896\n",
      "006903\n",
      "006908\n",
      "006910\n",
      "006911\n",
      "006912\n",
      "006919\n",
      "006924\n",
      "006932\n",
      "006934\n",
      "006939\n",
      "006943\n",
      "006945\n",
      "006948\n",
      "006950\n",
      "006956\n",
      "006958\n",
      "006959\n",
      "006963\n",
      "006965\n",
      "006968\n",
      "006981\n",
      "006983\n",
      "006987\n",
      "006988\n",
      "006989\n",
      "006990\n",
      "007004\n",
      "007007\n",
      "007016\n",
      "007020\n",
      "007023\n",
      "007031\n",
      "007035\n",
      "007036\n",
      "007040\n",
      "007045\n",
      "007046\n",
      "007048\n",
      "007049\n",
      "007052\n",
      "007054\n",
      "007059\n",
      "007073\n",
      "007079\n",
      "007080\n",
      "007093\n",
      "007108\n",
      "007109\n",
      "007114\n",
      "007117\n",
      "007121\n",
      "007122\n",
      "007128\n",
      "007129\n",
      "007138\n",
      "007139\n",
      "007140\n",
      "007147\n",
      "007148\n",
      "007150\n",
      "007153\n",
      "007154\n",
      "007159\n",
      "007163\n",
      "007167\n",
      "007172\n",
      "007182\n",
      "007184\n",
      "007187\n",
      "007189\n",
      "007200\n",
      "007208\n",
      "007213\n",
      "007214\n",
      "007216\n",
      "007227\n",
      "007234\n",
      "007236\n",
      "007243\n",
      "007244\n",
      "007256\n",
      "007260\n",
      "007261\n",
      "007266\n",
      "007270\n",
      "007271\n",
      "007276\n",
      "007284\n",
      "007292\n",
      "007296\n",
      "007298\n",
      "007305\n",
      "007308\n",
      "007314\n",
      "007318\n",
      "007325\n",
      "007330\n",
      "007336\n",
      "007346\n",
      "007350\n",
      "007359\n",
      "007369\n",
      "007375\n",
      "007381\n",
      "007388\n",
      "007398\n",
      "007408\n",
      "007410\n",
      "007413\n",
      "007414\n",
      "007436\n",
      "007438\n",
      "007445\n",
      "007446\n",
      "007448\n",
      "007449\n",
      "007457\n",
      "007466\n",
      "007475\n",
      "007480\n",
      "007482\n",
      "007483\n",
      "007484\n",
      "007493\n",
      "007497\n",
      "007498\n",
      "007511\n",
      "007526\n",
      "007535\n",
      "007540\n",
      "007543\n",
      "007547\n",
      "007558\n",
      "007559\n",
      "007563\n",
      "007566\n",
      "007568\n",
      "007570\n",
      "007571\n",
      "007572\n",
      "007576\n",
      "007578\n",
      "007579\n",
      "007586\n",
      "007590\n",
      "007592\n",
      "007594\n",
      "007605\n",
      "007606\n",
      "007611\n",
      "007618\n",
      "007619\n",
      "007629\n",
      "007631\n",
      "007639\n",
      "007647\n",
      "007649\n",
      "007653\n",
      "007656\n",
      "007657\n",
      "007668\n",
      "007675\n",
      "007683\n",
      "007687\n",
      "007694\n",
      "007697\n",
      "007699\n",
      "007709\n",
      "007712\n",
      "007718\n",
      "007723\n",
      "007727\n",
      "007735\n",
      "007745\n",
      "007746\n",
      "007748\n",
      "007751\n",
      "007760\n",
      "007763\n",
      "007768\n",
      "007777\n",
      "007786\n",
      "007793\n",
      "007795\n",
      "007799\n",
      "007809\n",
      "007810\n",
      "007813\n",
      "007814\n",
      "007815\n",
      "007821\n",
      "007836\n",
      "007841\n",
      "007856\n",
      "007872\n",
      "007877\n",
      "007878\n",
      "007884\n",
      "007889\n",
      "007897\n",
      "007901\n",
      "007902\n",
      "007910\n",
      "007911\n",
      "007915\n",
      "007919\n",
      "007928\n",
      "007931\n",
      "007932\n",
      "007940\n",
      "007943\n",
      "007946\n",
      "007953\n",
      "007963\n",
      "007964\n",
      "007970\n",
      "007971\n",
      "007979\n",
      "007984\n",
      "007987\n",
      "007996\n",
      "007997\n",
      "007998\n",
      "008002\n",
      "008008\n",
      "008019\n",
      "008023\n",
      "008024\n",
      "008026\n",
      "008033\n",
      "008040\n",
      "008042\n",
      "008043\n",
      "008048\n",
      "008053\n",
      "008069\n",
      "008072\n",
      "008079\n",
      "008082\n",
      "008083\n",
      "008086\n",
      "008091\n",
      "008096\n",
      "008103\n",
      "008105\n",
      "008108\n",
      "008122\n",
      "008125\n",
      "008127\n",
      "008130\n",
      "008132\n",
      "008137\n",
      "008139\n",
      "008142\n",
      "008160\n",
      "008163\n",
      "008164\n",
      "008171\n",
      "008175\n",
      "008177\n",
      "008180\n",
      "008188\n",
      "008190\n",
      "008199\n",
      "008204\n",
      "008209\n",
      "008211\n",
      "008216\n",
      "008218\n",
      "008220\n",
      "008224\n",
      "008225\n",
      "008232\n",
      "008241\n",
      "008248\n",
      "008253\n",
      "008261\n",
      "008263\n",
      "008272\n",
      "008279\n",
      "008281\n",
      "008284\n",
      "008285\n",
      "008293\n",
      "008294\n",
      "008301\n",
      "008302\n",
      "008307\n",
      "008312\n",
      "008313\n",
      "008315\n",
      "008327\n",
      "008332\n",
      "008341\n",
      "008360\n",
      "008368\n",
      "008374\n",
      "008376\n",
      "008381\n",
      "008385\n",
      "008387\n",
      "008390\n",
      "008391\n",
      "008397\n",
      "008398\n",
      "008410\n",
      "008413\n",
      "008430\n",
      "008433\n",
      "008437\n",
      "008444\n",
      "008445\n",
      "008449\n",
      "008452\n",
      "008453\n",
      "008461\n",
      "008467\n",
      "008470\n",
      "008475\n",
      "008482\n",
      "008485\n",
      "008503\n",
      "008509\n",
      "008513\n",
      "008514\n",
      "008526\n",
      "008529\n",
      "008541\n",
      "008550\n",
      "008559\n",
      "008562\n",
      "008573\n",
      "008576\n",
      "008582\n",
      "008585\n",
      "008587\n",
      "008592\n",
      "008596\n",
      "008602\n",
      "008604\n",
      "008612\n",
      "008617\n",
      "008618\n",
      "008628\n",
      "008638\n",
      "008644\n",
      "008647\n",
      "008663\n",
      "008665\n",
      "008676\n",
      "008690\n",
      "008692\n",
      "008698\n",
      "008699\n",
      "008701\n",
      "008702\n",
      "008706\n",
      "008718\n",
      "008728\n",
      "008731\n",
      "008738\n",
      "008739\n",
      "008742\n",
      "008744\n",
      "008748\n",
      "008752\n",
      "008766\n",
      "008768\n",
      "008769\n",
      "008771\n",
      "008775\n",
      "008794\n",
      "008801\n",
      "008805\n",
      "008810\n",
      "008813\n",
      "008817\n",
      "008833\n",
      "008836\n",
      "008838\n",
      "008862\n",
      "008865\n",
      "008867\n",
      "008872\n",
      "008876\n",
      "008879\n",
      "008884\n",
      "008886\n",
      "008891\n",
      "008892\n",
      "008900\n",
      "008911\n",
      "008920\n",
      "008921\n",
      "008931\n",
      "008932\n",
      "008943\n",
      "008944\n",
      "008953\n",
      "008960\n",
      "008961\n",
      "008965\n",
      "008975\n",
      "008979\n",
      "008983\n",
      "009000\n",
      "009005\n",
      "009020\n",
      "009022\n",
      "009024\n",
      "009034\n",
      "009036\n",
      "009045\n",
      "009049\n",
      "009064\n",
      "009068\n",
      "009073\n",
      "009078\n",
      "009079\n",
      "009087\n",
      "009089\n",
      "009091\n",
      "009114\n",
      "009123\n",
      "009128\n",
      "009136\n",
      "009141\n",
      "009153\n",
      "009160\n",
      "009161\n",
      "009174\n",
      "009180\n",
      "009184\n",
      "009193\n",
      "009197\n",
      "009202\n",
      "009208\n",
      "009209\n",
      "009214\n",
      "009224\n",
      "009230\n",
      "009236\n",
      "009238\n",
      "009239\n",
      "009242\n",
      "009251\n",
      "009259\n",
      "009273\n",
      "009283\n",
      "009286\n",
      "009287\n",
      "009288\n",
      "009290\n",
      "009291\n",
      "009295\n",
      "009296\n",
      "009299\n",
      "009303\n",
      "009306\n",
      "009308\n",
      "009312\n",
      "009316\n",
      "009318\n",
      "009327\n",
      "009330\n",
      "009331\n",
      "009336\n",
      "009342\n",
      "009343\n",
      "009349\n",
      "009358\n",
      "009362\n",
      "009368\n",
      "009374\n",
      "009375\n",
      "009377\n",
      "009382\n",
      "009393\n",
      "009394\n",
      "009407\n",
      "009410\n",
      "009411\n",
      "009412\n",
      "009413\n",
      "009414\n",
      "009418\n",
      "009420\n",
      "009421\n",
      "009424\n",
      "009433\n",
      "009434\n",
      "009437\n",
      "009440\n",
      "009448\n",
      "009456\n",
      "009460\n",
      "009468\n",
      "009469\n",
      "009477\n",
      "009479\n",
      "009481\n",
      "009490\n",
      "009494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009496\n",
      "009497\n",
      "009499\n",
      "009500\n",
      "009504\n",
      "009512\n",
      "009517\n",
      "009518\n",
      "009520\n",
      "009524\n",
      "009526\n",
      "009531\n",
      "009532\n",
      "009537\n",
      "009541\n",
      "009542\n",
      "009546\n",
      "009551\n",
      "009557\n",
      "009558\n",
      "009565\n",
      "009567\n",
      "009568\n",
      "009573\n",
      "009577\n",
      "009579\n",
      "009585\n",
      "009591\n",
      "009596\n",
      "009600\n",
      "009603\n",
      "009609\n",
      "009611\n",
      "009613\n",
      "009614\n",
      "009617\n",
      "009618\n",
      "009638\n",
      "009641\n",
      "009647\n",
      "009649\n",
      "009654\n",
      "009655\n",
      "009656\n",
      "009659\n",
      "009666\n",
      "009668\n",
      "009671\n",
      "009676\n",
      "009684\n",
      "009687\n",
      "009691\n",
      "009693\n",
      "009698\n",
      "009703\n",
      "009711\n",
      "009712\n",
      "009713\n",
      "009717\n",
      "009718\n",
      "009721\n",
      "009726\n",
      "009732\n",
      "009734\n",
      "009738\n",
      "009747\n",
      "009754\n",
      "009755\n",
      "009756\n",
      "009762\n",
      "009767\n",
      "009773\n",
      "009776\n",
      "009780\n",
      "009781\n",
      "009789\n",
      "009792\n",
      "009796\n",
      "009800\n",
      "009809\n",
      "009813\n",
      "009822\n",
      "009828\n",
      "009841\n",
      "009845\n",
      "009848\n",
      "009851\n",
      "009859\n",
      "009867\n",
      "009868\n",
      "009869\n",
      "009874\n",
      "009877\n",
      "009878\n",
      "009879\n",
      "009880\n",
      "009881\n",
      "009882\n",
      "009900\n",
      "009902\n",
      "009917\n",
      "009918\n",
      "009926\n",
      "009935\n",
      "009942\n",
      "009944\n",
      "009946\n",
      "009947\n",
      "009949\n",
      "009950\n",
      "009954\n",
      "009958\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 128  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 128])\n",
      "64\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.5851, 0.8360, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazeFace(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (4): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (5): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (7): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (8): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (9): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BlazeFaceExtra(\n",
      "  (features): Sequential(\n",
      "    (0): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.blazeface import SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 128,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16],  # DBOXの大きさを決める\n",
    "    'min_sizes': [16, 32],  # DBOXの大きさを決める\n",
    "    'max_sizes': [32, 64],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-3\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface128_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 5.6041 || 10iter: 13.4629 sec.\n",
      "Iteration 20 || Loss: 5.1559 || 10iter: 10.2217 sec.\n",
      "Iteration 30 || Loss: 5.2245 || 10iter: 7.4033 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:178.0630 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2429 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 40 || Loss: 5.1779 || 10iter: 12.7278 sec.\n",
      "Iteration 50 || Loss: 5.1793 || 10iter: 7.1467 sec.\n",
      "Iteration 60 || Loss: 5.7415 || 10iter: 6.2813 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:178.7961 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1239 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 70 || Loss: 5.5289 || 10iter: 12.2706 sec.\n",
      "Iteration 80 || Loss: 5.1408 || 10iter: 10.1684 sec.\n",
      "Iteration 90 || Loss: 5.1711 || 10iter: 7.1539 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:179.8645 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.2760 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 100 || Loss: 5.6417 || 10iter: 9.4749 sec.\n",
      "Iteration 110 || Loss: 5.4667 || 10iter: 13.6163 sec.\n",
      "Iteration 120 || Loss: 5.5193 || 10iter: 8.2753 sec.\n",
      "Iteration 130 || Loss: 5.2463 || 10iter: 6.3226 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:179.3244 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.2047 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 140 || Loss: 5.6449 || 10iter: 12.6315 sec.\n",
      "Iteration 150 || Loss: 5.8131 || 10iter: 13.6881 sec.\n",
      "Iteration 160 || Loss: 5.3256 || 10iter: 7.3497 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:179.5185 ||Epoch_VAL_Loss:91.6016\n",
      "timer:  46.1510 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 170 || Loss: 6.2491 || 10iter: 12.7644 sec.\n",
      "Iteration 180 || Loss: 5.5706 || 10iter: 13.3781 sec.\n",
      "Iteration 190 || Loss: 5.3469 || 10iter: 7.9849 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:179.3745 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.3690 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 200 || Loss: 5.5230 || 10iter: 6.0775 sec.\n",
      "Iteration 210 || Loss: 5.3422 || 10iter: 9.3451 sec.\n",
      "Iteration 220 || Loss: 4.8998 || 10iter: 5.0695 sec.\n",
      "Iteration 230 || Loss: 5.0907 || 10iter: 5.4383 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:178.5023 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  26.8450 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 240 || Loss: 5.6765 || 10iter: 20.8404 sec.\n",
      "Iteration 250 || Loss: 5.4597 || 10iter: 9.0489 sec.\n",
      "Iteration 260 || Loss: 5.4980 || 10iter: 6.2108 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:179.2326 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.8283 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 270 || Loss: 6.0423 || 10iter: 12.7453 sec.\n",
      "Iteration 280 || Loss: 5.2984 || 10iter: 13.1094 sec.\n",
      "Iteration 290 || Loss: 5.6453 || 10iter: 7.8254 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:178.7843 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.4816 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 300 || Loss: 5.5727 || 10iter: 7.0254 sec.\n",
      "Iteration 310 || Loss: 5.2299 || 10iter: 9.2929 sec.\n",
      "Iteration 320 || Loss: 5.3777 || 10iter: 11.2824 sec.\n",
      "Iteration 330 || Loss: 6.5992 || 10iter: 6.8305 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:179.6269 ||Epoch_VAL_Loss:91.5694\n",
      "timer:  44.6624 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 340 || Loss: 6.0083 || 10iter: 17.4117 sec.\n",
      "Iteration 350 || Loss: 5.0417 || 10iter: 11.1306 sec.\n",
      "Iteration 360 || Loss: 5.5242 || 10iter: 7.0528 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:179.1926 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.5878 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 370 || Loss: 4.9620 || 10iter: 10.1901 sec.\n",
      "Iteration 380 || Loss: 5.5999 || 10iter: 9.0114 sec.\n",
      "Iteration 390 || Loss: 5.5851 || 10iter: 9.4645 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:178.0262 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2539 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 400 || Loss: 5.2147 || 10iter: 10.6615 sec.\n",
      "Iteration 410 || Loss: 5.4349 || 10iter: 7.5244 sec.\n",
      "Iteration 420 || Loss: 5.4144 || 10iter: 6.3008 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:179.8091 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  28.5852 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n",
      "Iteration 430 || Loss: 5.4774 || 10iter: 5.6958 sec.\n",
      "Iteration 440 || Loss: 5.4655 || 10iter: 13.3495 sec.\n",
      "Iteration 450 || Loss: 5.0864 || 10iter: 9.9476 sec.\n",
      "Iteration 460 || Loss: 5.5691 || 10iter: 6.4688 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:178.1254 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.3139 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 470 || Loss: 5.4050 || 10iter: 11.0718 sec.\n",
      "Iteration 480 || Loss: 5.6352 || 10iter: 9.2115 sec.\n",
      "Iteration 490 || Loss: 5.0089 || 10iter: 8.9066 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:180.4185 ||Epoch_VAL_Loss:91.4777\n",
      "timer:  42.3376 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "train\n",
      "Iteration 500 || Loss: 5.3308 || 10iter: 8.9635 sec.\n",
      "Iteration 510 || Loss: 5.5515 || 10iter: 9.2697 sec.\n",
      "Iteration 520 || Loss: 5.9090 || 10iter: 10.8730 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:179.1939 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.9678 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 17/200\n",
      "-------------\n",
      "train\n",
      "Iteration 530 || Loss: 5.7087 || 10iter: 7.4018 sec.\n",
      "Iteration 540 || Loss: 5.6923 || 10iter: 7.6904 sec.\n",
      "Iteration 550 || Loss: 5.4134 || 10iter: 7.1119 sec.\n",
      "Iteration 560 || Loss: 5.3900 || 10iter: 9.0472 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:180.3647 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.0457 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 18/200\n",
      "-------------\n",
      "train\n",
      "Iteration 570 || Loss: 5.3226 || 10iter: 16.4953 sec.\n",
      "Iteration 580 || Loss: 5.1475 || 10iter: 7.2093 sec.\n",
      "Iteration 590 || Loss: 5.5498 || 10iter: 6.4547 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:178.1785 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.1847 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 19/200\n",
      "-------------\n",
      "train\n",
      "Iteration 600 || Loss: 5.4895 || 10iter: 14.6437 sec.\n",
      "Iteration 610 || Loss: 5.3371 || 10iter: 9.4722 sec.\n",
      "Iteration 620 || Loss: 5.2975 || 10iter: 6.3587 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:177.1820 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.1082 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 20/200\n",
      "-------------\n",
      "train\n",
      "Iteration 630 || Loss: 5.8512 || 10iter: 8.3936 sec.\n",
      "Iteration 640 || Loss: 5.2682 || 10iter: 7.0324 sec.\n",
      "Iteration 650 || Loss: 5.9545 || 10iter: 10.6609 sec.\n",
      "Iteration 660 || Loss: 5.9182 || 10iter: 7.4261 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:179.2295 ||Epoch_VAL_Loss:91.1638\n",
      "timer:  44.9615 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 21/200\n",
      "-------------\n",
      "train\n",
      "Iteration 670 || Loss: 5.8857 || 10iter: 14.5773 sec.\n",
      "Iteration 680 || Loss: 5.0630 || 10iter: 11.7402 sec.\n",
      "Iteration 690 || Loss: 4.8917 || 10iter: 7.3180 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:177.2825 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.5528 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 22/200\n",
      "-------------\n",
      "train\n",
      "Iteration 700 || Loss: 5.0517 || 10iter: 10.1707 sec.\n",
      "Iteration 710 || Loss: 5.0597 || 10iter: 7.4871 sec.\n",
      "Iteration 720 || Loss: 5.3263 || 10iter: 8.2486 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:177.4423 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  30.9670 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 23/200\n",
      "-------------\n",
      "train\n",
      "Iteration 730 || Loss: 5.2929 || 10iter: 11.0824 sec.\n",
      "Iteration 740 || Loss: 5.1877 || 10iter: 8.6741 sec.\n",
      "Iteration 750 || Loss: 5.2297 || 10iter: 6.4751 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:178.8460 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8671 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 24/200\n",
      "-------------\n",
      "train\n",
      "Iteration 760 || Loss: 5.3574 || 10iter: 9.4219 sec.\n",
      "Iteration 770 || Loss: 5.3590 || 10iter: 12.1200 sec.\n",
      "Iteration 780 || Loss: 5.4890 || 10iter: 6.7860 sec.\n",
      "Iteration 790 || Loss: 5.0124 || 10iter: 6.5375 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:178.7727 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.2128 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 25/200\n",
      "-------------\n",
      "train\n",
      "Iteration 800 || Loss: 5.9758 || 10iter: 20.3344 sec.\n",
      "Iteration 810 || Loss: 5.4636 || 10iter: 10.0434 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 820 || Loss: 5.3574 || 10iter: 6.4120 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:180.6583 ||Epoch_VAL_Loss:91.2578\n",
      "timer:  49.0977 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 26/200\n",
      "-------------\n",
      "train\n",
      "Iteration 830 || Loss: 5.4789 || 10iter: 8.7715 sec.\n",
      "Iteration 840 || Loss: 5.2433 || 10iter: 12.4867 sec.\n",
      "Iteration 850 || Loss: 5.0040 || 10iter: 8.8925 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:177.9276 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.7462 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 27/200\n",
      "-------------\n",
      "train\n",
      "Iteration 860 || Loss: 5.5082 || 10iter: 5.9661 sec.\n",
      "Iteration 870 || Loss: 5.9313 || 10iter: 8.2595 sec.\n",
      "Iteration 880 || Loss: 5.3088 || 10iter: 7.5498 sec.\n",
      "Iteration 890 || Loss: 4.8905 || 10iter: 9.2868 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:179.4821 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.7952 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 28/200\n",
      "-------------\n",
      "train\n",
      "Iteration 900 || Loss: 5.2306 || 10iter: 16.0498 sec.\n",
      "Iteration 910 || Loss: 5.3216 || 10iter: 6.4786 sec.\n",
      "Iteration 920 || Loss: 5.3524 || 10iter: 6.0437 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:177.4651 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1485 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 29/200\n",
      "-------------\n",
      "train\n",
      "Iteration 930 || Loss: 5.1632 || 10iter: 15.1165 sec.\n",
      "Iteration 940 || Loss: 5.4423 || 10iter: 10.0919 sec.\n",
      "Iteration 950 || Loss: 5.7221 || 10iter: 6.3533 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:179.5037 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.2106 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 30/200\n",
      "-------------\n",
      "train\n",
      "Iteration 960 || Loss: 5.0774 || 10iter: 12.4403 sec.\n",
      "Iteration 970 || Loss: 5.4179 || 10iter: 13.3588 sec.\n",
      "Iteration 980 || Loss: 5.2927 || 10iter: 8.0418 sec.\n",
      "Iteration 990 || Loss: 5.6867 || 10iter: 6.4207 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:179.5355 ||Epoch_VAL_Loss:91.1182\n",
      "timer:  51.2514 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 31/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1000 || Loss: 5.4327 || 10iter: 20.8729 sec.\n",
      "Iteration 1010 || Loss: 5.9654 || 10iter: 7.0369 sec.\n",
      "Iteration 1020 || Loss: 5.8259 || 10iter: 6.4458 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:179.0447 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.2570 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 32/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1030 || Loss: 5.3608 || 10iter: 11.5683 sec.\n",
      "Iteration 1040 || Loss: 5.2137 || 10iter: 7.5663 sec.\n",
      "Iteration 1050 || Loss: 5.2283 || 10iter: 10.5603 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:179.1682 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.8929 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 33/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1060 || Loss: 5.9073 || 10iter: 10.6794 sec.\n",
      "Iteration 1070 || Loss: 5.2466 || 10iter: 7.8227 sec.\n",
      "Iteration 1080 || Loss: 5.2755 || 10iter: 6.7707 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:177.5017 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4920 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 34/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1090 || Loss: 5.5694 || 10iter: 7.8948 sec.\n",
      "Iteration 1100 || Loss: 5.3307 || 10iter: 11.5424 sec.\n",
      "Iteration 1110 || Loss: 5.6147 || 10iter: 6.3344 sec.\n",
      "Iteration 1120 || Loss: 5.2868 || 10iter: 6.1607 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:180.8883 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.4412 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 35/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1130 || Loss: 5.2256 || 10iter: 19.0202 sec.\n",
      "Iteration 1140 || Loss: 4.9871 || 10iter: 10.6508 sec.\n",
      "Iteration 1150 || Loss: 5.1138 || 10iter: 6.6358 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:179.6755 ||Epoch_VAL_Loss:91.2556\n",
      "timer:  50.1007 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 36/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1160 || Loss: 5.0037 || 10iter: 14.5074 sec.\n",
      "Iteration 1170 || Loss: 5.3232 || 10iter: 10.6544 sec.\n",
      "Iteration 1180 || Loss: 5.2384 || 10iter: 6.5679 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:177.9845 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.0187 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 37/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1190 || Loss: 5.3172 || 10iter: 10.4469 sec.\n",
      "Iteration 1200 || Loss: 5.4761 || 10iter: 15.3351 sec.\n",
      "Iteration 1210 || Loss: 5.6078 || 10iter: 9.1105 sec.\n",
      "Iteration 1220 || Loss: 5.3871 || 10iter: 6.2184 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:179.3965 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.0311 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 38/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1230 || Loss: 5.2136 || 10iter: 13.3481 sec.\n",
      "Iteration 1240 || Loss: 5.4550 || 10iter: 6.5666 sec.\n",
      "Iteration 1250 || Loss: 5.7293 || 10iter: 7.0858 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:177.8062 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  30.8615 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 39/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1260 || Loss: 5.8166 || 10iter: 13.9977 sec.\n",
      "Iteration 1270 || Loss: 5.3705 || 10iter: 9.1172 sec.\n",
      "Iteration 1280 || Loss: 5.0590 || 10iter: 6.4354 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:176.6995 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.1586 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 40/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1290 || Loss: 5.1709 || 10iter: 12.8351 sec.\n",
      "Iteration 1300 || Loss: 5.2134 || 10iter: 12.9982 sec.\n",
      "Iteration 1310 || Loss: 5.1120 || 10iter: 7.7426 sec.\n",
      "Iteration 1320 || Loss: 5.1038 || 10iter: 6.2036 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:178.3751 ||Epoch_VAL_Loss:91.1604\n",
      "timer:  52.0710 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 41/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1330 || Loss: 5.5652 || 10iter: 19.3153 sec.\n",
      "Iteration 1340 || Loss: 5.8585 || 10iter: 7.0484 sec.\n",
      "Iteration 1350 || Loss: 5.2208 || 10iter: 6.4392 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:177.8377 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.8526 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 42/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1360 || Loss: 5.7727 || 10iter: 19.2113 sec.\n",
      "Iteration 1370 || Loss: 5.3600 || 10iter: 11.3578 sec.\n",
      "Iteration 1380 || Loss: 5.5219 || 10iter: 6.7878 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:177.8924 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.3428 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 43/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1390 || Loss: 5.3615 || 10iter: 7.8556 sec.\n",
      "Iteration 1400 || Loss: 5.2153 || 10iter: 13.8992 sec.\n",
      "Iteration 1410 || Loss: 5.2787 || 10iter: 8.8807 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:179.7475 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.1122 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 44/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1420 || Loss: 5.3834 || 10iter: 5.4290 sec.\n",
      "Iteration 1430 || Loss: 5.3240 || 10iter: 8.8412 sec.\n",
      "Iteration 1440 || Loss: 5.2458 || 10iter: 7.3045 sec.\n",
      "Iteration 1450 || Loss: 5.7579 || 10iter: 4.1001 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:178.8817 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  26.8781 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 45/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1460 || Loss: 5.6370 || 10iter: 20.1745 sec.\n",
      "Iteration 1470 || Loss: 5.5516 || 10iter: 10.7382 sec.\n",
      "Iteration 1480 || Loss: 4.9010 || 10iter: 6.3476 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:178.9656 ||Epoch_VAL_Loss:91.1509\n",
      "timer:  51.9069 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 46/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1490 || Loss: 5.5328 || 10iter: 13.4285 sec.\n",
      "Iteration 1500 || Loss: 5.2839 || 10iter: 11.2062 sec.\n",
      "Iteration 1510 || Loss: 5.4755 || 10iter: 6.5843 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:178.2863 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.5086 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 47/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1520 || Loss: 5.6187 || 10iter: 8.6255 sec.\n",
      "Iteration 1530 || Loss: 5.1543 || 10iter: 15.2141 sec.\n",
      "Iteration 1540 || Loss: 5.6497 || 10iter: 8.9857 sec.\n",
      "Iteration 1550 || Loss: 5.7774 || 10iter: 6.6810 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:178.6765 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.5298 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 48/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1560 || Loss: 5.1388 || 10iter: 13.7020 sec.\n",
      "Iteration 1570 || Loss: 5.1251 || 10iter: 13.5284 sec.\n",
      "Iteration 1580 || Loss: 5.0917 || 10iter: 6.9134 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:176.2894 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.8401 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 49/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1590 || Loss: 5.9277 || 10iter: 10.0864 sec.\n",
      "Iteration 1600 || Loss: 5.2669 || 10iter: 7.4866 sec.\n",
      "Iteration 1610 || Loss: 6.0232 || 10iter: 9.3016 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:177.8047 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  31.9851 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 50/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1620 || Loss: 5.3551 || 10iter: 9.9699 sec.\n",
      "Iteration 1630 || Loss: 5.5118 || 10iter: 8.5303 sec.\n",
      "Iteration 1640 || Loss: 5.7814 || 10iter: 6.4079 sec.\n",
      "Iteration 1650 || Loss: 5.6199 || 10iter: 5.7744 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:178.7732 ||Epoch_VAL_Loss:91.1637\n",
      "timer:  41.7314 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 51/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1660 || Loss: 5.9790 || 10iter: 21.3653 sec.\n",
      "Iteration 1670 || Loss: 5.2513 || 10iter: 8.1523 sec.\n",
      "Iteration 1680 || Loss: 5.0382 || 10iter: 6.1313 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:175.5281 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.7490 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 52/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1690 || Loss: 5.3843 || 10iter: 17.6204 sec.\n",
      "Iteration 1700 || Loss: 4.9923 || 10iter: 11.5112 sec.\n",
      "Iteration 1710 || Loss: 5.1665 || 10iter: 6.8075 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:177.7090 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.0145 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 53/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1720 || Loss: 5.4515 || 10iter: 8.0359 sec.\n",
      "Iteration 1730 || Loss: 5.7978 || 10iter: 12.8581 sec.\n",
      "Iteration 1740 || Loss: 5.0681 || 10iter: 8.6810 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:179.3004 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.0022 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 54/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1750 || Loss: 5.7908 || 10iter: 5.0278 sec.\n",
      "Iteration 1760 || Loss: 5.3099 || 10iter: 8.4578 sec.\n",
      "Iteration 1770 || Loss: 5.2625 || 10iter: 7.5925 sec.\n",
      "Iteration 1780 || Loss: 5.2001 || 10iter: 9.6043 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:177.0545 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.3063 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 55/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1790 || Loss: 5.5562 || 10iter: 14.2304 sec.\n",
      "Iteration 1800 || Loss: 5.5543 || 10iter: 7.2723 sec.\n",
      "Iteration 1810 || Loss: 5.3187 || 10iter: 6.1261 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:176.0135 ||Epoch_VAL_Loss:91.1668\n",
      "timer:  45.1414 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 56/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1820 || Loss: 5.7031 || 10iter: 10.8162 sec.\n",
      "Iteration 1830 || Loss: 5.2241 || 10iter: 7.9351 sec.\n",
      "Iteration 1840 || Loss: 5.5846 || 10iter: 5.9797 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:178.2047 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  28.5323 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 57/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1850 || Loss: 5.6400 || 10iter: 8.3317 sec.\n",
      "Iteration 1860 || Loss: 5.4281 || 10iter: 14.3220 sec.\n",
      "Iteration 1870 || Loss: 5.0173 || 10iter: 8.4683 sec.\n",
      "Iteration 1880 || Loss: 5.3100 || 10iter: 6.8478 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:177.6500 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.7318 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 58/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1890 || Loss: 5.2403 || 10iter: 13.4832 sec.\n",
      "Iteration 1900 || Loss: 5.1242 || 10iter: 10.9607 sec.\n",
      "Iteration 1910 || Loss: 5.3100 || 10iter: 8.1843 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:177.5934 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  35.7611 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 59/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1920 || Loss: 5.4980 || 10iter: 10.4081 sec.\n",
      "Iteration 1930 || Loss: 5.1705 || 10iter: 7.2498 sec.\n",
      "Iteration 1940 || Loss: 5.3690 || 10iter: 8.5490 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:177.9111 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.1911 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 60/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1950 || Loss: 5.5687 || 10iter: 9.9170 sec.\n",
      "Iteration 1960 || Loss: 5.1961 || 10iter: 8.7233 sec.\n",
      "Iteration 1970 || Loss: 5.6610 || 10iter: 6.6734 sec.\n",
      "Iteration 1980 || Loss: 5.7921 || 10iter: 7.0675 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:178.1460 ||Epoch_VAL_Loss:90.9699\n",
      "timer:  46.5503 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 61/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1990 || Loss: 5.2915 || 10iter: 16.1057 sec.\n",
      "Iteration 2000 || Loss: 5.0524 || 10iter: 6.7585 sec.\n",
      "Iteration 2010 || Loss: 5.2331 || 10iter: 8.7594 sec.\n",
      "-------------\n",
      "epoch 61 || Epoch_TRAIN_Loss:179.5553 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  34.5675 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 62/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2020 || Loss: 5.0555 || 10iter: 14.4599 sec.\n",
      "Iteration 2030 || Loss: 5.2633 || 10iter: 7.9629 sec.\n",
      "Iteration 2040 || Loss: 5.3738 || 10iter: 6.1227 sec.\n",
      "-------------\n",
      "epoch 62 || Epoch_TRAIN_Loss:177.8429 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.5187 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 63/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2050 || Loss: 5.4314 || 10iter: 7.3187 sec.\n",
      "Iteration 2060 || Loss: 4.9469 || 10iter: 8.6670 sec.\n",
      "Iteration 2070 || Loss: 5.1993 || 10iter: 11.0974 sec.\n",
      "-------------\n",
      "epoch 63 || Epoch_TRAIN_Loss:178.1265 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.7557 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 64/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2080 || Loss: 5.6193 || 10iter: 7.1063 sec.\n",
      "Iteration 2090 || Loss: 5.3073 || 10iter: 7.8679 sec.\n",
      "Iteration 2100 || Loss: 5.2322 || 10iter: 6.7230 sec.\n",
      "Iteration 2110 || Loss: 5.6688 || 10iter: 9.0279 sec.\n",
      "-------------\n",
      "epoch 64 || Epoch_TRAIN_Loss:177.0806 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8863 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 65/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2120 || Loss: 5.4218 || 10iter: 15.9578 sec.\n",
      "Iteration 2130 || Loss: 5.3049 || 10iter: 7.5699 sec.\n",
      "Iteration 2140 || Loss: 5.3407 || 10iter: 6.3332 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 65 || Epoch_TRAIN_Loss:177.2075 ||Epoch_VAL_Loss:90.9989\n",
      "timer:  48.3706 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 66/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2150 || Loss: 5.6308 || 10iter: 11.6753 sec.\n",
      "Iteration 2160 || Loss: 5.5172 || 10iter: 7.3971 sec.\n",
      "Iteration 2170 || Loss: 5.4214 || 10iter: 6.3068 sec.\n",
      "-------------\n",
      "epoch 66 || Epoch_TRAIN_Loss:177.5256 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.5738 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 67/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2180 || Loss: 5.8230 || 10iter: 8.3738 sec.\n",
      "Iteration 2190 || Loss: 5.2132 || 10iter: 11.0737 sec.\n",
      "Iteration 2200 || Loss: 5.0553 || 10iter: 6.4611 sec.\n",
      "Iteration 2210 || Loss: 5.3694 || 10iter: 6.5109 sec.\n",
      "-------------\n",
      "epoch 67 || Epoch_TRAIN_Loss:175.5934 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2609 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 68/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2220 || Loss: 5.2757 || 10iter: 20.5764 sec.\n",
      "Iteration 2230 || Loss: 4.9913 || 10iter: 9.1972 sec.\n",
      "Iteration 2240 || Loss: 5.3300 || 10iter: 6.5704 sec.\n",
      "-------------\n",
      "epoch 68 || Epoch_TRAIN_Loss:175.1759 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.1605 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 69/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2250 || Loss: 5.5178 || 10iter: 11.3322 sec.\n",
      "Iteration 2260 || Loss: 5.1765 || 10iter: 7.1202 sec.\n",
      "Iteration 2270 || Loss: 5.3641 || 10iter: 8.8633 sec.\n",
      "-------------\n",
      "epoch 69 || Epoch_TRAIN_Loss:179.2458 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  32.8853 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 70/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2280 || Loss: 4.9132 || 10iter: 9.4839 sec.\n",
      "Iteration 2290 || Loss: 5.4058 || 10iter: 9.3542 sec.\n",
      "Iteration 2300 || Loss: 5.8030 || 10iter: 6.3294 sec.\n",
      "Iteration 2310 || Loss: 5.2879 || 10iter: 7.1388 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 70 || Epoch_TRAIN_Loss:179.4737 ||Epoch_VAL_Loss:91.1338\n",
      "timer:  47.0997 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 71/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2320 || Loss: 5.3998 || 10iter: 15.1337 sec.\n",
      "Iteration 2330 || Loss: 5.3617 || 10iter: 6.1390 sec.\n",
      "Iteration 2340 || Loss: 4.9369 || 10iter: 8.9568 sec.\n",
      "-------------\n",
      "epoch 71 || Epoch_TRAIN_Loss:179.1462 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2476 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 72/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2350 || Loss: 5.7245 || 10iter: 15.4040 sec.\n",
      "Iteration 2360 || Loss: 5.2646 || 10iter: 7.2696 sec.\n",
      "Iteration 2370 || Loss: 4.9326 || 10iter: 6.1983 sec.\n",
      "-------------\n",
      "epoch 72 || Epoch_TRAIN_Loss:177.4196 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  33.2463 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 73/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2380 || Loss: 5.5208 || 10iter: 14.7445 sec.\n",
      "Iteration 2390 || Loss: 5.5423 || 10iter: 11.9659 sec.\n",
      "Iteration 2400 || Loss: 5.2300 || 10iter: 6.3578 sec.\n",
      "-------------\n",
      "epoch 73 || Epoch_TRAIN_Loss:178.5552 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.0842 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 74/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2410 || Loss: 5.4569 || 10iter: 7.4716 sec.\n",
      "Iteration 2420 || Loss: 5.4930 || 10iter: 15.9013 sec.\n",
      "Iteration 2430 || Loss: 5.5266 || 10iter: 9.2243 sec.\n",
      "Iteration 2440 || Loss: 5.7391 || 10iter: 6.4268 sec.\n",
      "-------------\n",
      "epoch 74 || Epoch_TRAIN_Loss:178.6583 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.8862 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 75/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2450 || Loss: 5.4786 || 10iter: 12.7070 sec.\n",
      "Iteration 2460 || Loss: 5.4750 || 10iter: 6.9981 sec.\n",
      "Iteration 2470 || Loss: 5.5104 || 10iter: 5.3650 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 75 || Epoch_TRAIN_Loss:179.1614 ||Epoch_VAL_Loss:91.0334\n",
      "timer:  43.9434 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 76/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2480 || Loss: 5.4879 || 10iter: 9.3700 sec.\n",
      "Iteration 2490 || Loss: 5.3558 || 10iter: 7.4044 sec.\n",
      "Iteration 2500 || Loss: 6.1656 || 10iter: 3.6790 sec.\n",
      "-------------\n",
      "epoch 76 || Epoch_TRAIN_Loss:177.3414 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  23.0439 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 77/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2510 || Loss: 4.9438 || 10iter: 4.6403 sec.\n",
      "Iteration 2520 || Loss: 5.6132 || 10iter: 6.9952 sec.\n",
      "Iteration 2530 || Loss: 5.3060 || 10iter: 3.6729 sec.\n",
      "Iteration 2540 || Loss: 5.4812 || 10iter: 2.7444 sec.\n",
      "-------------\n",
      "epoch 77 || Epoch_TRAIN_Loss:177.0768 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5632 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 78/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2550 || Loss: 5.2411 || 10iter: 9.6839 sec.\n",
      "Iteration 2560 || Loss: 5.3014 || 10iter: 4.8841 sec.\n",
      "Iteration 2570 || Loss: 5.4924 || 10iter: 2.7162 sec.\n",
      "-------------\n",
      "epoch 78 || Epoch_TRAIN_Loss:175.2617 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5666 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 79/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2580 || Loss: 5.6254 || 10iter: 7.6279 sec.\n",
      "Iteration 2590 || Loss: 5.1916 || 10iter: 5.8199 sec.\n",
      "Iteration 2600 || Loss: 5.7688 || 10iter: 3.1920 sec.\n",
      "-------------\n",
      "epoch 79 || Epoch_TRAIN_Loss:177.6160 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7239 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 80/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2610 || Loss: 5.5601 || 10iter: 5.4966 sec.\n",
      "Iteration 2620 || Loss: 6.3147 || 10iter: 6.6697 sec.\n",
      "Iteration 2630 || Loss: 5.5713 || 10iter: 3.7178 sec.\n",
      "Iteration 2640 || Loss: 5.5261 || 10iter: 2.7675 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 80 || Epoch_TRAIN_Loss:177.5574 ||Epoch_VAL_Loss:90.9448\n",
      "timer:  23.8652 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 81/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2650 || Loss: 5.4702 || 10iter: 10.4166 sec.\n",
      "Iteration 2660 || Loss: 5.6389 || 10iter: 5.8086 sec.\n",
      "Iteration 2670 || Loss: 5.7831 || 10iter: 2.7172 sec.\n",
      "-------------\n",
      "epoch 81 || Epoch_TRAIN_Loss:177.1464 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  20.0000 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 82/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2680 || Loss: 5.4048 || 10iter: 9.6004 sec.\n",
      "Iteration 2690 || Loss: 5.1497 || 10iter: 5.4884 sec.\n",
      "Iteration 2700 || Loss: 5.6758 || 10iter: 3.0560 sec.\n",
      "-------------\n",
      "epoch 82 || Epoch_TRAIN_Loss:177.4089 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.9667 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 83/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2710 || Loss: 5.3395 || 10iter: 7.0951 sec.\n",
      "Iteration 2720 || Loss: 5.4091 || 10iter: 5.6144 sec.\n",
      "Iteration 2730 || Loss: 5.2361 || 10iter: 3.6114 sec.\n",
      "-------------\n",
      "epoch 83 || Epoch_TRAIN_Loss:177.3449 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.0259 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 84/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2740 || Loss: 5.5666 || 10iter: 4.6059 sec.\n",
      "Iteration 2750 || Loss: 5.3757 || 10iter: 7.3485 sec.\n",
      "Iteration 2760 || Loss: 5.5802 || 10iter: 4.2744 sec.\n",
      "Iteration 2770 || Loss: 5.2047 || 10iter: 2.6578 sec.\n",
      "-------------\n",
      "epoch 84 || Epoch_TRAIN_Loss:177.9257 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.6523 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 85/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2780 || Loss: 4.8399 || 10iter: 9.1073 sec.\n",
      "Iteration 2790 || Loss: 5.2057 || 10iter: 5.3267 sec.\n",
      "Iteration 2800 || Loss: 6.2856 || 10iter: 2.7391 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 85 || Epoch_TRAIN_Loss:180.1585 ||Epoch_VAL_Loss:91.0138\n",
      "timer:  23.1266 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 86/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2810 || Loss: 5.4031 || 10iter: 6.8912 sec.\n",
      "Iteration 2820 || Loss: 5.1956 || 10iter: 5.6745 sec.\n",
      "Iteration 2830 || Loss: 5.8961 || 10iter: 3.4774 sec.\n",
      "-------------\n",
      "epoch 86 || Epoch_TRAIN_Loss:175.5486 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3889 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 87/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2840 || Loss: 5.5600 || 10iter: 4.9715 sec.\n",
      "Iteration 2850 || Loss: 4.8141 || 10iter: 6.6375 sec.\n",
      "Iteration 2860 || Loss: 5.4956 || 10iter: 3.8687 sec.\n",
      "Iteration 2870 || Loss: 5.1364 || 10iter: 2.7591 sec.\n",
      "-------------\n",
      "epoch 87 || Epoch_TRAIN_Loss:177.7279 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7198 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 88/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2880 || Loss: 5.0752 || 10iter: 9.2984 sec.\n",
      "Iteration 2890 || Loss: 5.6018 || 10iter: 5.0037 sec.\n",
      "Iteration 2900 || Loss: 5.8904 || 10iter: 2.6952 sec.\n",
      "-------------\n",
      "epoch 88 || Epoch_TRAIN_Loss:177.2082 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2978 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 89/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2910 || Loss: 5.2899 || 10iter: 7.6174 sec.\n",
      "Iteration 2920 || Loss: 5.4011 || 10iter: 5.6086 sec.\n",
      "Iteration 2930 || Loss: 5.6681 || 10iter: 3.2424 sec.\n",
      "-------------\n",
      "epoch 89 || Epoch_TRAIN_Loss:179.2598 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5840 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 90/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2940 || Loss: 5.4107 || 10iter: 5.6669 sec.\n",
      "Iteration 2950 || Loss: 5.1358 || 10iter: 6.1498 sec.\n",
      "Iteration 2960 || Loss: 5.3431 || 10iter: 3.7498 sec.\n",
      "Iteration 2970 || Loss: 5.1074 || 10iter: 2.6641 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 90 || Epoch_TRAIN_Loss:177.0622 ||Epoch_VAL_Loss:90.7568\n",
      "timer:  23.0080 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 91/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2980 || Loss: 5.7182 || 10iter: 10.2212 sec.\n",
      "Iteration 2990 || Loss: 5.3674 || 10iter: 4.6860 sec.\n",
      "Iteration 3000 || Loss: 5.4280 || 10iter: 2.6525 sec.\n",
      "-------------\n",
      "epoch 91 || Epoch_TRAIN_Loss:179.3993 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5819 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 92/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3010 || Loss: 5.4001 || 10iter: 7.9464 sec.\n",
      "Iteration 3020 || Loss: 5.3157 || 10iter: 5.6297 sec.\n",
      "Iteration 3030 || Loss: 5.6330 || 10iter: 2.9078 sec.\n",
      "-------------\n",
      "epoch 92 || Epoch_TRAIN_Loss:178.0464 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3426 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 93/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3040 || Loss: 5.2742 || 10iter: 6.6861 sec.\n",
      "Iteration 3050 || Loss: 5.5963 || 10iter: 6.5308 sec.\n",
      "Iteration 3060 || Loss: 5.5689 || 10iter: 3.5117 sec.\n",
      "-------------\n",
      "epoch 93 || Epoch_TRAIN_Loss:177.0245 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.4183 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 94/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3070 || Loss: 5.1824 || 10iter: 3.9621 sec.\n",
      "Iteration 3080 || Loss: 5.0744 || 10iter: 7.0762 sec.\n",
      "Iteration 3090 || Loss: 5.7439 || 10iter: 4.3121 sec.\n",
      "Iteration 3100 || Loss: 5.8675 || 10iter: 2.6545 sec.\n",
      "-------------\n",
      "epoch 94 || Epoch_TRAIN_Loss:178.5627 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7338 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 95/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3110 || Loss: 5.3342 || 10iter: 8.6532 sec.\n",
      "Iteration 3120 || Loss: 5.3148 || 10iter: 5.2884 sec.\n",
      "Iteration 3130 || Loss: 5.1550 || 10iter: 2.6260 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 95 || Epoch_TRAIN_Loss:176.7280 ||Epoch_VAL_Loss:90.9657\n",
      "timer:  22.8792 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 96/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3140 || Loss: 5.2479 || 10iter: 6.9452 sec.\n",
      "Iteration 3150 || Loss: 5.3974 || 10iter: 5.8389 sec.\n",
      "Iteration 3160 || Loss: 5.3499 || 10iter: 3.3143 sec.\n",
      "-------------\n",
      "epoch 96 || Epoch_TRAIN_Loss:177.3054 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4370 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 97/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3170 || Loss: 5.2457 || 10iter: 4.5713 sec.\n",
      "Iteration 3180 || Loss: 5.2316 || 10iter: 6.9270 sec.\n",
      "Iteration 3190 || Loss: 6.1061 || 10iter: 3.8134 sec.\n",
      "Iteration 3200 || Loss: 5.0611 || 10iter: 2.6792 sec.\n",
      "-------------\n",
      "epoch 97 || Epoch_TRAIN_Loss:177.0117 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5003 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 98/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3210 || Loss: 5.5767 || 10iter: 9.1640 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3220 || Loss: 5.1922 || 10iter: 5.2936 sec.\n",
      "Iteration 3230 || Loss: 5.2714 || 10iter: 2.7261 sec.\n",
      "-------------\n",
      "epoch 98 || Epoch_TRAIN_Loss:176.8206 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4160 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 99/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3240 || Loss: 5.6491 || 10iter: 7.6452 sec.\n",
      "Iteration 3250 || Loss: 5.2761 || 10iter: 5.5755 sec.\n",
      "Iteration 3260 || Loss: 5.5672 || 10iter: 3.2349 sec.\n",
      "-------------\n",
      "epoch 99 || Epoch_TRAIN_Loss:177.2025 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5937 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 100/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3270 || Loss: 5.5807 || 10iter: 5.8068 sec.\n",
      "Iteration 3280 || Loss: 5.6111 || 10iter: 5.9501 sec.\n",
      "Iteration 3290 || Loss: 5.4797 || 10iter: 3.7781 sec.\n",
      "Iteration 3300 || Loss: 5.2891 || 10iter: 2.6755 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 100 || Epoch_TRAIN_Loss:178.0156 ||Epoch_VAL_Loss:90.7770\n",
      "timer:  23.0242 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 101/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3310 || Loss: 5.0892 || 10iter: 10.8023 sec.\n",
      "Iteration 3320 || Loss: 5.6809 || 10iter: 4.1677 sec.\n",
      "Iteration 3330 || Loss: 5.3410 || 10iter: 2.7226 sec.\n",
      "-------------\n",
      "epoch 101 || Epoch_TRAIN_Loss:177.3298 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7023 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 102/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3340 || Loss: 5.2011 || 10iter: 8.4939 sec.\n",
      "Iteration 3350 || Loss: 5.8233 || 10iter: 6.0779 sec.\n",
      "Iteration 3360 || Loss: 5.3609 || 10iter: 2.9632 sec.\n",
      "-------------\n",
      "epoch 102 || Epoch_TRAIN_Loss:176.1315 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.3619 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 103/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3370 || Loss: 5.5203 || 10iter: 6.6766 sec.\n",
      "Iteration 3380 || Loss: 5.0175 || 10iter: 5.5930 sec.\n",
      "Iteration 3390 || Loss: 5.1911 || 10iter: 3.6950 sec.\n",
      "-------------\n",
      "epoch 103 || Epoch_TRAIN_Loss:176.0290 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.6388 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 104/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3400 || Loss: 5.1900 || 10iter: 4.4138 sec.\n",
      "Iteration 3410 || Loss: 5.1906 || 10iter: 6.7020 sec.\n",
      "Iteration 3420 || Loss: 5.1846 || 10iter: 4.1392 sec.\n",
      "Iteration 3430 || Loss: 5.4742 || 10iter: 2.6842 sec.\n",
      "-------------\n",
      "epoch 104 || Epoch_TRAIN_Loss:177.2428 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7189 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 105/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3440 || Loss: 5.2790 || 10iter: 8.6173 sec.\n",
      "Iteration 3450 || Loss: 4.8533 || 10iter: 5.9340 sec.\n",
      "Iteration 3460 || Loss: 5.4328 || 10iter: 2.5633 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 105 || Epoch_TRAIN_Loss:176.3977 ||Epoch_VAL_Loss:90.8107\n",
      "timer:  23.1375 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 106/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3470 || Loss: 5.0273 || 10iter: 6.9969 sec.\n",
      "Iteration 3480 || Loss: 5.4861 || 10iter: 5.4557 sec.\n",
      "Iteration 3490 || Loss: 5.7963 || 10iter: 3.4025 sec.\n",
      "-------------\n",
      "epoch 106 || Epoch_TRAIN_Loss:177.7506 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2396 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 107/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3500 || Loss: 5.3809 || 10iter: 4.9244 sec.\n",
      "Iteration 3510 || Loss: 5.5253 || 10iter: 6.0866 sec.\n",
      "Iteration 3520 || Loss: 5.4139 || 10iter: 4.2837 sec.\n",
      "Iteration 3530 || Loss: 5.3655 || 10iter: 2.7041 sec.\n",
      "-------------\n",
      "epoch 107 || Epoch_TRAIN_Loss:175.6460 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5208 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 108/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3540 || Loss: 5.3260 || 10iter: 10.0934 sec.\n",
      "Iteration 3550 || Loss: 5.4603 || 10iter: 4.3232 sec.\n",
      "Iteration 3560 || Loss: 4.9398 || 10iter: 2.6772 sec.\n",
      "-------------\n",
      "epoch 108 || Epoch_TRAIN_Loss:178.8365 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4105 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 109/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3570 || Loss: 5.5186 || 10iter: 7.3894 sec.\n",
      "Iteration 3580 || Loss: 5.1940 || 10iter: 5.8843 sec.\n",
      "Iteration 3590 || Loss: 5.6882 || 10iter: 3.0409 sec.\n",
      "-------------\n",
      "epoch 109 || Epoch_TRAIN_Loss:176.4328 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4005 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 110/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3600 || Loss: 5.0745 || 10iter: 5.4341 sec.\n",
      "Iteration 3610 || Loss: 5.3074 || 10iter: 6.0645 sec.\n",
      "Iteration 3620 || Loss: 5.5378 || 10iter: 4.0048 sec.\n",
      "Iteration 3630 || Loss: 5.3224 || 10iter: 2.6536 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 110 || Epoch_TRAIN_Loss:174.4339 ||Epoch_VAL_Loss:90.7465\n",
      "timer:  22.8841 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 111/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3640 || Loss: 5.5401 || 10iter: 10.3070 sec.\n",
      "Iteration 3650 || Loss: 5.3385 || 10iter: 4.6882 sec.\n",
      "Iteration 3660 || Loss: 5.5532 || 10iter: 2.7242 sec.\n",
      "-------------\n",
      "epoch 111 || Epoch_TRAIN_Loss:178.5080 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7536 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 112/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3670 || Loss: 5.4364 || 10iter: 7.7949 sec.\n",
      "Iteration 3680 || Loss: 5.0632 || 10iter: 5.7524 sec.\n",
      "Iteration 3690 || Loss: 5.0648 || 10iter: 2.9412 sec.\n",
      "-------------\n",
      "epoch 112 || Epoch_TRAIN_Loss:176.8790 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2858 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 113/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3700 || Loss: 5.4365 || 10iter: 6.4160 sec.\n",
      "Iteration 3710 || Loss: 4.9999 || 10iter: 5.7394 sec.\n",
      "Iteration 3720 || Loss: 5.1354 || 10iter: 3.4925 sec.\n",
      "-------------\n",
      "epoch 113 || Epoch_TRAIN_Loss:176.6191 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2912 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 114/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3730 || Loss: 5.2900 || 10iter: 3.8336 sec.\n",
      "Iteration 3740 || Loss: 5.2723 || 10iter: 6.8477 sec.\n",
      "Iteration 3750 || Loss: 5.4977 || 10iter: 4.3170 sec.\n",
      "Iteration 3760 || Loss: 5.1813 || 10iter: 2.6614 sec.\n",
      "-------------\n",
      "epoch 114 || Epoch_TRAIN_Loss:177.2413 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5028 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 115/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3770 || Loss: 5.0362 || 10iter: 8.6212 sec.\n",
      "Iteration 3780 || Loss: 5.6917 || 10iter: 5.6428 sec.\n",
      "Iteration 3790 || Loss: 5.9331 || 10iter: 2.6433 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 115 || Epoch_TRAIN_Loss:178.3636 ||Epoch_VAL_Loss:90.6277\n",
      "timer:  22.9570 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 116/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3800 || Loss: 5.5101 || 10iter: 6.9654 sec.\n",
      "Iteration 3810 || Loss: 5.2364 || 10iter: 5.3115 sec.\n",
      "Iteration 3820 || Loss: 5.1143 || 10iter: 3.5388 sec.\n",
      "-------------\n",
      "epoch 116 || Epoch_TRAIN_Loss:176.6121 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2231 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 117/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3830 || Loss: 5.9836 || 10iter: 5.4215 sec.\n",
      "Iteration 3840 || Loss: 5.3929 || 10iter: 5.9248 sec.\n",
      "Iteration 3850 || Loss: 5.5776 || 10iter: 3.7880 sec.\n",
      "Iteration 3860 || Loss: 5.3970 || 10iter: 2.7322 sec.\n",
      "-------------\n",
      "epoch 117 || Epoch_TRAIN_Loss:178.5050 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3652 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 118/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3870 || Loss: 5.6554 || 10iter: 9.8835 sec.\n",
      "Iteration 3880 || Loss: 5.5861 || 10iter: 4.5801 sec.\n",
      "Iteration 3890 || Loss: 5.5453 || 10iter: 2.6430 sec.\n",
      "-------------\n",
      "epoch 118 || Epoch_TRAIN_Loss:176.9199 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3589 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 119/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3900 || Loss: 5.5228 || 10iter: 7.3077 sec.\n",
      "Iteration 3910 || Loss: 5.1648 || 10iter: 5.6554 sec.\n",
      "Iteration 3920 || Loss: 5.6243 || 10iter: 3.1488 sec.\n",
      "-------------\n",
      "epoch 119 || Epoch_TRAIN_Loss:177.6371 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2039 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 120/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3930 || Loss: 5.5931 || 10iter: 5.6121 sec.\n",
      "Iteration 3940 || Loss: 5.8167 || 10iter: 6.3239 sec.\n",
      "Iteration 3950 || Loss: 5.3182 || 10iter: 3.4758 sec.\n",
      "Iteration 3960 || Loss: 5.4514 || 10iter: 2.6452 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 120 || Epoch_TRAIN_Loss:176.5641 ||Epoch_VAL_Loss:90.5143\n",
      "timer:  23.0150 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 121/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3970 || Loss: 5.2894 || 10iter: 9.8358 sec.\n",
      "Iteration 3980 || Loss: 5.1088 || 10iter: 4.7474 sec.\n",
      "Iteration 3990 || Loss: 5.3623 || 10iter: 2.7096 sec.\n",
      "-------------\n",
      "epoch 121 || Epoch_TRAIN_Loss:176.6734 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3207 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 122/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4000 || Loss: 6.0494 || 10iter: 8.1849 sec.\n",
      "Iteration 4010 || Loss: 5.6459 || 10iter: 5.4878 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4020 || Loss: 5.4939 || 10iter: 3.0155 sec.\n",
      "-------------\n",
      "epoch 122 || Epoch_TRAIN_Loss:176.9544 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5220 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 123/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4030 || Loss: 5.0363 || 10iter: 6.4393 sec.\n",
      "Iteration 4040 || Loss: 5.5407 || 10iter: 5.7666 sec.\n",
      "Iteration 4050 || Loss: 5.3622 || 10iter: 3.5525 sec.\n",
      "-------------\n",
      "epoch 123 || Epoch_TRAIN_Loss:176.0069 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4261 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 124/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4060 || Loss: 5.5153 || 10iter: 3.8196 sec.\n",
      "Iteration 4070 || Loss: 5.5441 || 10iter: 7.1078 sec.\n",
      "Iteration 4080 || Loss: 5.5459 || 10iter: 3.8972 sec.\n",
      "Iteration 4090 || Loss: 5.5274 || 10iter: 2.6410 sec.\n",
      "-------------\n",
      "epoch 124 || Epoch_TRAIN_Loss:177.9515 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2216 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 125/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4100 || Loss: 5.4429 || 10iter: 8.6546 sec.\n",
      "Iteration 4110 || Loss: 5.6398 || 10iter: 5.1546 sec.\n",
      "Iteration 4120 || Loss: 5.3027 || 10iter: 2.7067 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 125 || Epoch_TRAIN_Loss:180.3500 ||Epoch_VAL_Loss:90.5633\n",
      "timer:  22.6842 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 126/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4130 || Loss: 5.2411 || 10iter: 7.2839 sec.\n",
      "Iteration 4140 || Loss: 5.5836 || 10iter: 5.2222 sec.\n",
      "Iteration 4150 || Loss: 5.4936 || 10iter: 3.2926 sec.\n",
      "-------------\n",
      "epoch 126 || Epoch_TRAIN_Loss:176.7142 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1252 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 127/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4160 || Loss: 5.4012 || 10iter: 5.3492 sec.\n",
      "Iteration 4170 || Loss: 5.1025 || 10iter: 6.0360 sec.\n",
      "Iteration 4180 || Loss: 5.0693 || 10iter: 3.7552 sec.\n",
      "Iteration 4190 || Loss: 5.2624 || 10iter: 2.6666 sec.\n",
      "-------------\n",
      "epoch 127 || Epoch_TRAIN_Loss:176.3464 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2677 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 128/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4200 || Loss: 4.9357 || 10iter: 9.2894 sec.\n",
      "Iteration 4210 || Loss: 5.2919 || 10iter: 4.9168 sec.\n",
      "Iteration 4220 || Loss: 5.0424 || 10iter: 2.6016 sec.\n",
      "-------------\n",
      "epoch 128 || Epoch_TRAIN_Loss:178.5776 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.0700 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 129/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4230 || Loss: 5.0533 || 10iter: 7.5258 sec.\n",
      "Iteration 4240 || Loss: 5.7829 || 10iter: 5.3727 sec.\n",
      "Iteration 4250 || Loss: 5.5177 || 10iter: 3.3419 sec.\n",
      "-------------\n",
      "epoch 129 || Epoch_TRAIN_Loss:176.5311 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3797 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 130/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4260 || Loss: 5.2809 || 10iter: 5.8293 sec.\n",
      "Iteration 4270 || Loss: 5.5720 || 10iter: 6.1155 sec.\n",
      "Iteration 4280 || Loss: 5.9143 || 10iter: 4.1103 sec.\n",
      "Iteration 4290 || Loss: 5.1260 || 10iter: 2.6617 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 130 || Epoch_TRAIN_Loss:175.9275 ||Epoch_VAL_Loss:90.4647\n",
      "timer:  23.3535 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 131/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4300 || Loss: 5.9782 || 10iter: 10.0689 sec.\n",
      "Iteration 4310 || Loss: 5.3005 || 10iter: 4.6539 sec.\n",
      "Iteration 4320 || Loss: 5.1683 || 10iter: 2.5739 sec.\n",
      "-------------\n",
      "epoch 131 || Epoch_TRAIN_Loss:177.0118 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3094 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 132/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4330 || Loss: 5.3316 || 10iter: 8.2007 sec.\n",
      "Iteration 4340 || Loss: 5.5411 || 10iter: 5.5296 sec.\n",
      "Iteration 4350 || Loss: 5.3813 || 10iter: 2.9527 sec.\n",
      "-------------\n",
      "epoch 132 || Epoch_TRAIN_Loss:178.3900 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5204 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 133/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4360 || Loss: 5.0325 || 10iter: 6.2971 sec.\n",
      "Iteration 4370 || Loss: 5.0512 || 10iter: 5.8382 sec.\n",
      "Iteration 4380 || Loss: 5.7603 || 10iter: 3.4118 sec.\n",
      "-------------\n",
      "epoch 133 || Epoch_TRAIN_Loss:175.3424 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1542 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 134/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4390 || Loss: 5.1118 || 10iter: 4.1155 sec.\n",
      "Iteration 4400 || Loss: 5.4507 || 10iter: 6.5086 sec.\n",
      "Iteration 4410 || Loss: 4.9478 || 10iter: 4.4807 sec.\n",
      "Iteration 4420 || Loss: 5.4159 || 10iter: 2.6290 sec.\n",
      "-------------\n",
      "epoch 134 || Epoch_TRAIN_Loss:178.6885 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5090 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 135/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4430 || Loss: 5.6371 || 10iter: 8.4420 sec.\n",
      "Iteration 4440 || Loss: 5.1295 || 10iter: 5.4210 sec.\n",
      "Iteration 4450 || Loss: 5.3368 || 10iter: 2.7180 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 135 || Epoch_TRAIN_Loss:175.5919 ||Epoch_VAL_Loss:90.4083\n",
      "timer:  22.6798 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 136/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4460 || Loss: 5.2385 || 10iter: 7.1312 sec.\n",
      "Iteration 4470 || Loss: 5.3428 || 10iter: 5.4610 sec.\n",
      "Iteration 4480 || Loss: 5.2309 || 10iter: 3.4272 sec.\n",
      "-------------\n",
      "epoch 136 || Epoch_TRAIN_Loss:176.4757 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3207 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 137/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4490 || Loss: 5.4033 || 10iter: 5.2571 sec.\n",
      "Iteration 4500 || Loss: 5.2777 || 10iter: 5.4689 sec.\n",
      "Iteration 4510 || Loss: 5.7016 || 10iter: 4.0683 sec.\n",
      "Iteration 4520 || Loss: 5.4078 || 10iter: 2.6744 sec.\n",
      "-------------\n",
      "epoch 137 || Epoch_TRAIN_Loss:176.1747 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.0248 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 138/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4530 || Loss: 5.2818 || 10iter: 10.2128 sec.\n",
      "Iteration 4540 || Loss: 5.3096 || 10iter: 4.1807 sec.\n",
      "Iteration 4550 || Loss: 4.7843 || 10iter: 2.6593 sec.\n",
      "-------------\n",
      "epoch 138 || Epoch_TRAIN_Loss:176.4613 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3524 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 139/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4560 || Loss: 5.5002 || 10iter: 7.1743 sec.\n",
      "Iteration 4570 || Loss: 5.7190 || 10iter: 5.7768 sec.\n",
      "Iteration 4580 || Loss: 4.8277 || 10iter: 3.2348 sec.\n",
      "-------------\n",
      "epoch 139 || Epoch_TRAIN_Loss:177.3734 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3288 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 140/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4590 || Loss: 5.0847 || 10iter: 5.8635 sec.\n",
      "Iteration 4600 || Loss: 4.9824 || 10iter: 6.0671 sec.\n",
      "Iteration 4610 || Loss: 5.3181 || 10iter: 4.1515 sec.\n",
      "Iteration 4620 || Loss: 5.3488 || 10iter: 2.6099 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 140 || Epoch_TRAIN_Loss:177.2763 ||Epoch_VAL_Loss:90.6931\n",
      "timer:  23.3867 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 141/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4630 || Loss: 5.2606 || 10iter: 10.5206 sec.\n",
      "Iteration 4640 || Loss: 4.9922 || 10iter: 4.3024 sec.\n",
      "Iteration 4650 || Loss: 5.2870 || 10iter: 2.6095 sec.\n",
      "-------------\n",
      "epoch 141 || Epoch_TRAIN_Loss:177.0002 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4665 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 142/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4660 || Loss: 5.2869 || 10iter: 8.1075 sec.\n",
      "Iteration 4670 || Loss: 5.1942 || 10iter: 5.3807 sec.\n",
      "Iteration 4680 || Loss: 5.1159 || 10iter: 3.0692 sec.\n",
      "-------------\n",
      "epoch 142 || Epoch_TRAIN_Loss:176.4289 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3495 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 143/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4690 || Loss: 5.1499 || 10iter: 6.1390 sec.\n",
      "Iteration 4700 || Loss: 5.3276 || 10iter: 5.9236 sec.\n",
      "Iteration 4710 || Loss: 5.0584 || 10iter: 3.4901 sec.\n",
      "-------------\n",
      "epoch 143 || Epoch_TRAIN_Loss:177.0626 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2130 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 144/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4720 || Loss: 5.3368 || 10iter: 4.3603 sec.\n",
      "Iteration 4730 || Loss: 5.5732 || 10iter: 6.4121 sec.\n",
      "Iteration 4740 || Loss: 5.3178 || 10iter: 4.1596 sec.\n",
      "Iteration 4750 || Loss: 5.3024 || 10iter: 2.6358 sec.\n",
      "-------------\n",
      "epoch 144 || Epoch_TRAIN_Loss:176.9182 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3328 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 145/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4760 || Loss: 6.1827 || 10iter: 8.7754 sec.\n",
      "Iteration 4770 || Loss: 5.5326 || 10iter: 5.4969 sec.\n",
      "Iteration 4780 || Loss: 5.2399 || 10iter: 2.7231 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 145 || Epoch_TRAIN_Loss:178.1640 ||Epoch_VAL_Loss:90.5481\n",
      "timer:  23.1170 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 146/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4790 || Loss: 5.3423 || 10iter: 7.2138 sec.\n",
      "Iteration 4800 || Loss: 5.4989 || 10iter: 5.4678 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4810 || Loss: 5.5521 || 10iter: 3.5166 sec.\n",
      "-------------\n",
      "epoch 146 || Epoch_TRAIN_Loss:177.3148 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5824 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 147/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4820 || Loss: 6.0957 || 10iter: 5.4086 sec.\n",
      "Iteration 4830 || Loss: 5.5008 || 10iter: 5.6003 sec.\n",
      "Iteration 4840 || Loss: 5.3099 || 10iter: 4.0849 sec.\n",
      "Iteration 4850 || Loss: 5.2468 || 10iter: 2.7200 sec.\n",
      "-------------\n",
      "epoch 147 || Epoch_TRAIN_Loss:176.4210 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3624 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 148/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4860 || Loss: 5.3880 || 10iter: 9.8233 sec.\n",
      "Iteration 4870 || Loss: 5.0581 || 10iter: 4.7309 sec.\n",
      "Iteration 4880 || Loss: 5.8605 || 10iter: 2.6536 sec.\n",
      "-------------\n",
      "epoch 148 || Epoch_TRAIN_Loss:176.4308 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5012 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 149/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4890 || Loss: 5.8185 || 10iter: 8.2775 sec.\n",
      "Iteration 4900 || Loss: 5.9017 || 10iter: 5.9606 sec.\n",
      "Iteration 4910 || Loss: 5.3147 || 10iter: 3.0339 sec.\n",
      "-------------\n",
      "epoch 149 || Epoch_TRAIN_Loss:177.3748 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.3639 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 150/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4920 || Loss: 5.3788 || 10iter: 5.7203 sec.\n",
      "Iteration 4930 || Loss: 5.4081 || 10iter: 5.8679 sec.\n",
      "Iteration 4940 || Loss: 5.3692 || 10iter: 3.9201 sec.\n",
      "Iteration 4950 || Loss: 4.9739 || 10iter: 2.6593 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 150 || Epoch_TRAIN_Loss:177.5217 ||Epoch_VAL_Loss:90.3977\n",
      "timer:  22.8626 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 151/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4960 || Loss: 5.5566 || 10iter: 10.2288 sec.\n",
      "Iteration 4970 || Loss: 4.9652 || 10iter: 4.4909 sec.\n",
      "Iteration 4980 || Loss: 5.3310 || 10iter: 2.6807 sec.\n",
      "-------------\n",
      "epoch 151 || Epoch_TRAIN_Loss:176.4220 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4425 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 152/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4990 || Loss: 6.0747 || 10iter: 7.8782 sec.\n",
      "Iteration 5000 || Loss: 5.6253 || 10iter: 5.3937 sec.\n",
      "Iteration 5010 || Loss: 5.1578 || 10iter: 2.9066 sec.\n",
      "-------------\n",
      "epoch 152 || Epoch_TRAIN_Loss:178.3400 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.0389 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 153/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5020 || Loss: 5.1683 || 10iter: 6.4175 sec.\n",
      "Iteration 5030 || Loss: 5.2759 || 10iter: 5.8712 sec.\n",
      "Iteration 5040 || Loss: 5.5119 || 10iter: 3.5014 sec.\n",
      "-------------\n",
      "epoch 153 || Epoch_TRAIN_Loss:177.0919 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4438 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 154/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5050 || Loss: 5.1997 || 10iter: 3.7848 sec.\n",
      "Iteration 5060 || Loss: 5.5495 || 10iter: 6.8015 sec.\n",
      "Iteration 5070 || Loss: 5.2058 || 10iter: 4.3049 sec.\n",
      "Iteration 5080 || Loss: 5.3483 || 10iter: 2.6938 sec.\n",
      "-------------\n",
      "epoch 154 || Epoch_TRAIN_Loss:175.9279 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3929 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 155/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5090 || Loss: 5.4160 || 10iter: 8.2688 sec.\n",
      "Iteration 5100 || Loss: 5.8083 || 10iter: 5.7254 sec.\n",
      "Iteration 5110 || Loss: 5.4041 || 10iter: 2.7300 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 155 || Epoch_TRAIN_Loss:174.7309 ||Epoch_VAL_Loss:90.4343\n",
      "timer:  22.8039 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 156/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5120 || Loss: 5.3703 || 10iter: 7.1005 sec.\n",
      "Iteration 5130 || Loss: 5.4685 || 10iter: 5.6213 sec.\n",
      "Iteration 5140 || Loss: 5.0449 || 10iter: 3.7227 sec.\n",
      "-------------\n",
      "epoch 156 || Epoch_TRAIN_Loss:176.6498 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7982 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 157/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5150 || Loss: 5.2081 || 10iter: 4.6706 sec.\n",
      "Iteration 5160 || Loss: 5.4771 || 10iter: 6.4931 sec.\n",
      "Iteration 5170 || Loss: 5.9494 || 10iter: 3.9578 sec.\n",
      "Iteration 5180 || Loss: 4.8078 || 10iter: 2.7160 sec.\n",
      "-------------\n",
      "epoch 157 || Epoch_TRAIN_Loss:178.1629 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3071 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 158/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5190 || Loss: 5.4974 || 10iter: 10.0447 sec.\n",
      "Iteration 5200 || Loss: 6.0152 || 10iter: 5.7565 sec.\n",
      "Iteration 5210 || Loss: 4.9147 || 10iter: 2.5377 sec.\n",
      "-------------\n",
      "epoch 158 || Epoch_TRAIN_Loss:176.7103 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.5968 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 159/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5220 || Loss: 4.8883 || 10iter: 7.4211 sec.\n",
      "Iteration 5230 || Loss: 5.8552 || 10iter: 5.7038 sec.\n",
      "Iteration 5240 || Loss: 5.3457 || 10iter: 3.1760 sec.\n",
      "-------------\n",
      "epoch 159 || Epoch_TRAIN_Loss:177.7934 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3431 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 160/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5250 || Loss: 5.0893 || 10iter: 5.8378 sec.\n",
      "Iteration 5260 || Loss: 5.2000 || 10iter: 6.0224 sec.\n",
      "Iteration 5270 || Loss: 5.4055 || 10iter: 3.6202 sec.\n",
      "Iteration 5280 || Loss: 5.4852 || 10iter: 2.6736 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 160 || Epoch_TRAIN_Loss:176.7404 ||Epoch_VAL_Loss:90.4569\n",
      "timer:  22.9349 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 161/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5290 || Loss: 5.4969 || 10iter: 10.3937 sec.\n",
      "Iteration 5300 || Loss: 5.4053 || 10iter: 4.1490 sec.\n",
      "Iteration 5310 || Loss: 5.2732 || 10iter: 2.6320 sec.\n",
      "-------------\n",
      "epoch 161 || Epoch_TRAIN_Loss:176.6400 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1851 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 162/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5320 || Loss: 5.7952 || 10iter: 8.1291 sec.\n",
      "Iteration 5330 || Loss: 5.5124 || 10iter: 5.2726 sec.\n",
      "Iteration 5340 || Loss: 4.9102 || 10iter: 2.9170 sec.\n",
      "-------------\n",
      "epoch 162 || Epoch_TRAIN_Loss:177.6297 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1560 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 163/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5350 || Loss: 5.6511 || 10iter: 6.4463 sec.\n",
      "Iteration 5360 || Loss: 5.1753 || 10iter: 5.7751 sec.\n",
      "Iteration 5370 || Loss: 5.6919 || 10iter: 3.4776 sec.\n",
      "-------------\n",
      "epoch 163 || Epoch_TRAIN_Loss:178.3237 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3926 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 164/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5380 || Loss: 5.3483 || 10iter: 3.9305 sec.\n",
      "Iteration 5390 || Loss: 5.0346 || 10iter: 7.1128 sec.\n",
      "Iteration 5400 || Loss: 5.5014 || 10iter: 3.8405 sec.\n",
      "Iteration 5410 || Loss: 5.3868 || 10iter: 2.7271 sec.\n",
      "-------------\n",
      "epoch 164 || Epoch_TRAIN_Loss:177.7654 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3552 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 165/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5420 || Loss: 4.9073 || 10iter: 8.5768 sec.\n",
      "Iteration 5430 || Loss: 5.3584 || 10iter: 5.4179 sec.\n",
      "Iteration 5440 || Loss: 5.3118 || 10iter: 2.7701 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 165 || Epoch_TRAIN_Loss:175.3335 ||Epoch_VAL_Loss:90.4672\n",
      "timer:  22.7910 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 166/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5450 || Loss: 5.5560 || 10iter: 7.0803 sec.\n",
      "Iteration 5460 || Loss: 5.0510 || 10iter: 5.5394 sec.\n",
      "Iteration 5470 || Loss: 5.2866 || 10iter: 3.5736 sec.\n",
      "-------------\n",
      "epoch 166 || Epoch_TRAIN_Loss:174.2393 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5409 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 167/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5480 || Loss: 5.5083 || 10iter: 4.6500 sec.\n",
      "Iteration 5490 || Loss: 5.0746 || 10iter: 6.7751 sec.\n",
      "Iteration 5500 || Loss: 4.9593 || 10iter: 4.4604 sec.\n",
      "Iteration 5510 || Loss: 5.5077 || 10iter: 2.7202 sec.\n",
      "-------------\n",
      "epoch 167 || Epoch_TRAIN_Loss:175.5291 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.0779 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 168/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5520 || Loss: 6.3414 || 10iter: 10.1696 sec.\n",
      "Iteration 5530 || Loss: 5.2474 || 10iter: 4.2415 sec.\n",
      "Iteration 5540 || Loss: 5.2587 || 10iter: 2.6535 sec.\n",
      "-------------\n",
      "epoch 168 || Epoch_TRAIN_Loss:177.6247 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3372 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 169/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5550 || Loss: 5.5609 || 10iter: 7.6183 sec.\n",
      "Iteration 5560 || Loss: 5.1417 || 10iter: 5.5123 sec.\n",
      "Iteration 5570 || Loss: 5.1561 || 10iter: 3.3511 sec.\n",
      "-------------\n",
      "epoch 169 || Epoch_TRAIN_Loss:175.0834 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5846 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 170/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5580 || Loss: 5.6588 || 10iter: 5.3231 sec.\n",
      "Iteration 5590 || Loss: 5.7339 || 10iter: 6.4899 sec.\n",
      "Iteration 5600 || Loss: 5.3944 || 10iter: 3.8340 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5610 || Loss: 5.1039 || 10iter: 2.6859 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 170 || Epoch_TRAIN_Loss:176.2907 ||Epoch_VAL_Loss:90.4128\n",
      "timer:  23.0759 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 171/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5620 || Loss: 5.3225 || 10iter: 10.2095 sec.\n",
      "Iteration 5630 || Loss: 5.5309 || 10iter: 4.5481 sec.\n",
      "Iteration 5640 || Loss: 4.9989 || 10iter: 2.6340 sec.\n",
      "-------------\n",
      "epoch 171 || Epoch_TRAIN_Loss:179.5880 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4328 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 172/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5650 || Loss: 5.2309 || 10iter: 8.1992 sec.\n",
      "Iteration 5660 || Loss: 5.4572 || 10iter: 5.3228 sec.\n",
      "Iteration 5670 || Loss: 5.6440 || 10iter: 2.9831 sec.\n",
      "-------------\n",
      "epoch 172 || Epoch_TRAIN_Loss:174.2918 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2854 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 173/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5680 || Loss: 5.4925 || 10iter: 6.1745 sec.\n",
      "Iteration 5690 || Loss: 5.2717 || 10iter: 5.9287 sec.\n",
      "Iteration 5700 || Loss: 4.8258 || 10iter: 3.8130 sec.\n",
      "-------------\n",
      "epoch 173 || Epoch_TRAIN_Loss:176.4555 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5783 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 174/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5710 || Loss: 5.1150 || 10iter: 4.2451 sec.\n",
      "Iteration 5720 || Loss: 5.2241 || 10iter: 6.6575 sec.\n",
      "Iteration 5730 || Loss: 5.0618 || 10iter: 4.0454 sec.\n",
      "Iteration 5740 || Loss: 5.2806 || 10iter: 2.7493 sec.\n",
      "-------------\n",
      "epoch 174 || Epoch_TRAIN_Loss:174.8706 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4378 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 175/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5750 || Loss: 5.8931 || 10iter: 8.9402 sec.\n",
      "Iteration 5760 || Loss: 5.3039 || 10iter: 5.5041 sec.\n",
      "Iteration 5770 || Loss: 5.2071 || 10iter: 2.6132 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 175 || Epoch_TRAIN_Loss:177.2418 ||Epoch_VAL_Loss:90.4160\n",
      "timer:  23.1099 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 176/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5780 || Loss: 5.4251 || 10iter: 7.0915 sec.\n",
      "Iteration 5790 || Loss: 5.3000 || 10iter: 5.6775 sec.\n",
      "Iteration 5800 || Loss: 5.6526 || 10iter: 3.6333 sec.\n",
      "-------------\n",
      "epoch 176 || Epoch_TRAIN_Loss:176.2910 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.8096 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 177/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5810 || Loss: 5.2885 || 10iter: 4.1498 sec.\n",
      "Iteration 5820 || Loss: 5.2741 || 10iter: 7.2495 sec.\n",
      "Iteration 5830 || Loss: 5.4000 || 10iter: 4.1209 sec.\n",
      "Iteration 5840 || Loss: 5.3225 || 10iter: 2.7009 sec.\n",
      "-------------\n",
      "epoch 177 || Epoch_TRAIN_Loss:177.5012 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.7149 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 178/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5850 || Loss: 5.6427 || 10iter: 9.6078 sec.\n",
      "Iteration 5860 || Loss: 5.3147 || 10iter: 4.7951 sec.\n",
      "Iteration 5870 || Loss: 4.9951 || 10iter: 2.6454 sec.\n",
      "-------------\n",
      "epoch 178 || Epoch_TRAIN_Loss:176.8758 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3322 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 179/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5880 || Loss: 5.2614 || 10iter: 7.1290 sec.\n",
      "Iteration 5890 || Loss: 5.0233 || 10iter: 5.6541 sec.\n",
      "Iteration 5900 || Loss: 5.4578 || 10iter: 3.7522 sec.\n",
      "-------------\n",
      "epoch 179 || Epoch_TRAIN_Loss:176.0485 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5146 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 180/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5910 || Loss: 5.5640 || 10iter: 5.5233 sec.\n",
      "Iteration 5920 || Loss: 5.3622 || 10iter: 6.0458 sec.\n",
      "Iteration 5930 || Loss: 5.6848 || 10iter: 3.7802 sec.\n",
      "Iteration 5940 || Loss: 5.3737 || 10iter: 2.6577 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 180 || Epoch_TRAIN_Loss:177.3198 ||Epoch_VAL_Loss:90.4684\n",
      "timer:  22.9574 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 181/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5950 || Loss: 4.8776 || 10iter: 10.2438 sec.\n",
      "Iteration 5960 || Loss: 5.7280 || 10iter: 4.1587 sec.\n",
      "Iteration 5970 || Loss: 5.3513 || 10iter: 2.6397 sec.\n",
      "-------------\n",
      "epoch 181 || Epoch_TRAIN_Loss:176.5309 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.0657 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 182/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5980 || Loss: 5.3247 || 10iter: 7.7566 sec.\n",
      "Iteration 5990 || Loss: 5.1142 || 10iter: 5.3876 sec.\n",
      "Iteration 6000 || Loss: 5.3791 || 10iter: 2.9751 sec.\n",
      "-------------\n",
      "epoch 182 || Epoch_TRAIN_Loss:173.9501 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.9588 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 183/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6010 || Loss: 5.2330 || 10iter: 6.1983 sec.\n",
      "Iteration 6020 || Loss: 5.2660 || 10iter: 5.9206 sec.\n",
      "Iteration 6030 || Loss: 5.4694 || 10iter: 3.6684 sec.\n",
      "-------------\n",
      "epoch 183 || Epoch_TRAIN_Loss:175.0877 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4293 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 184/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6040 || Loss: 4.9624 || 10iter: 4.1588 sec.\n",
      "Iteration 6050 || Loss: 5.1761 || 10iter: 6.7596 sec.\n",
      "Iteration 6060 || Loss: 5.6222 || 10iter: 3.8987 sec.\n",
      "Iteration 6070 || Loss: 5.0565 || 10iter: 2.6853 sec.\n",
      "-------------\n",
      "epoch 184 || Epoch_TRAIN_Loss:175.3282 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3536 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 185/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6080 || Loss: 5.4291 || 10iter: 8.4721 sec.\n",
      "Iteration 6090 || Loss: 5.4735 || 10iter: 5.7263 sec.\n",
      "Iteration 6100 || Loss: 5.5322 || 10iter: 2.5767 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 185 || Epoch_TRAIN_Loss:175.1378 ||Epoch_VAL_Loss:90.3388\n",
      "timer:  22.8628 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 186/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6110 || Loss: 5.3361 || 10iter: 7.4360 sec.\n",
      "Iteration 6120 || Loss: 5.1292 || 10iter: 6.1332 sec.\n",
      "Iteration 6130 || Loss: 5.3132 || 10iter: 3.4741 sec.\n",
      "-------------\n",
      "epoch 186 || Epoch_TRAIN_Loss:175.5118 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  19.3951 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 187/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6140 || Loss: 5.7893 || 10iter: 5.4162 sec.\n",
      "Iteration 6150 || Loss: 5.2954 || 10iter: 5.8118 sec.\n",
      "Iteration 6160 || Loss: 5.2838 || 10iter: 4.0434 sec.\n",
      "Iteration 6170 || Loss: 5.4988 || 10iter: 2.7074 sec.\n",
      "-------------\n",
      "epoch 187 || Epoch_TRAIN_Loss:178.3104 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4874 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 188/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6180 || Loss: 5.1540 || 10iter: 10.1146 sec.\n",
      "Iteration 6190 || Loss: 5.3164 || 10iter: 4.4961 sec.\n",
      "Iteration 6200 || Loss: 5.3955 || 10iter: 2.6125 sec.\n",
      "-------------\n",
      "epoch 188 || Epoch_TRAIN_Loss:177.1651 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5444 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 189/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6210 || Loss: 5.2996 || 10iter: 7.8245 sec.\n",
      "Iteration 6220 || Loss: 5.2198 || 10iter: 5.3434 sec.\n",
      "Iteration 6230 || Loss: 5.2046 || 10iter: 3.2246 sec.\n",
      "-------------\n",
      "epoch 189 || Epoch_TRAIN_Loss:176.1683 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3975 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 190/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6240 || Loss: 5.1955 || 10iter: 5.5318 sec.\n",
      "Iteration 6250 || Loss: 5.6561 || 10iter: 6.0420 sec.\n",
      "Iteration 6260 || Loss: 5.4389 || 10iter: 3.6039 sec.\n",
      "Iteration 6270 || Loss: 5.1197 || 10iter: 2.6690 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 190 || Epoch_TRAIN_Loss:176.2755 ||Epoch_VAL_Loss:90.4509\n",
      "timer:  22.6577 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 191/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6280 || Loss: 6.0038 || 10iter: 10.5394 sec.\n",
      "Iteration 6290 || Loss: 5.3396 || 10iter: 4.0772 sec.\n",
      "Iteration 6300 || Loss: 5.5246 || 10iter: 2.6918 sec.\n",
      "-------------\n",
      "epoch 191 || Epoch_TRAIN_Loss:176.9252 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3190 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 192/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6310 || Loss: 5.6322 || 10iter: 7.7099 sec.\n",
      "Iteration 6320 || Loss: 5.1861 || 10iter: 5.8109 sec.\n",
      "Iteration 6330 || Loss: 5.2819 || 10iter: 2.8844 sec.\n",
      "-------------\n",
      "epoch 192 || Epoch_TRAIN_Loss:175.8185 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.2646 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 193/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6340 || Loss: 5.4001 || 10iter: 6.3326 sec.\n",
      "Iteration 6350 || Loss: 5.1322 || 10iter: 5.8292 sec.\n",
      "Iteration 6360 || Loss: 5.4617 || 10iter: 3.6714 sec.\n",
      "-------------\n",
      "epoch 193 || Epoch_TRAIN_Loss:176.6277 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.4683 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 194/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6370 || Loss: 5.8018 || 10iter: 4.4083 sec.\n",
      "Iteration 6380 || Loss: 5.2354 || 10iter: 6.3764 sec.\n",
      "Iteration 6390 || Loss: 5.5184 || 10iter: 4.1297 sec.\n",
      "Iteration 6400 || Loss: 5.8737 || 10iter: 2.6506 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "epoch 194 || Epoch_TRAIN_Loss:178.3666 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3391 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 195/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6410 || Loss: 5.5067 || 10iter: 8.7283 sec.\n",
      "Iteration 6420 || Loss: 5.1139 || 10iter: 6.0315 sec.\n",
      "Iteration 6430 || Loss: 4.9493 || 10iter: 2.7104 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 195 || Epoch_TRAIN_Loss:175.8441 ||Epoch_VAL_Loss:90.3797\n",
      "timer:  23.4829 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 196/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6440 || Loss: 5.4066 || 10iter: 6.9434 sec.\n",
      "Iteration 6450 || Loss: 5.2994 || 10iter: 5.5812 sec.\n",
      "Iteration 6460 || Loss: 5.5270 || 10iter: 3.2248 sec.\n",
      "-------------\n",
      "epoch 196 || Epoch_TRAIN_Loss:177.1734 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.1119 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 197/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6470 || Loss: 5.2945 || 10iter: 4.9341 sec.\n",
      "Iteration 6480 || Loss: 5.6017 || 10iter: 6.0553 sec.\n",
      "Iteration 6490 || Loss: 5.4667 || 10iter: 3.8061 sec.\n",
      "Iteration 6500 || Loss: 5.3356 || 10iter: 2.6665 sec.\n",
      "-------------\n",
      "epoch 197 || Epoch_TRAIN_Loss:177.0781 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  17.9722 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 198/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6510 || Loss: 5.5699 || 10iter: 10.0397 sec.\n",
      "Iteration 6520 || Loss: 5.2307 || 10iter: 4.4050 sec.\n",
      "Iteration 6530 || Loss: 5.2076 || 10iter: 2.6788 sec.\n",
      "-------------\n",
      "epoch 198 || Epoch_TRAIN_Loss:177.2985 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3984 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 199/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6540 || Loss: 5.6420 || 10iter: 7.6609 sec.\n",
      "Iteration 6550 || Loss: 5.7748 || 10iter: 5.4066 sec.\n",
      "Iteration 6560 || Loss: 5.4914 || 10iter: 3.1651 sec.\n",
      "-------------\n",
      "epoch 199 || Epoch_TRAIN_Loss:178.2220 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.3395 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 200/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6570 || Loss: 5.7566 || 10iter: 5.6655 sec.\n",
      "Iteration 6580 || Loss: 5.5160 || 10iter: 6.2400 sec.\n",
      "Iteration 6590 || Loss: 5.3869 || 10iter: 3.5747 sec.\n",
      "Iteration 6600 || Loss: 5.8786 || 10iter: 2.6617 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 200 || Epoch_TRAIN_Loss:176.4431 ||Epoch_VAL_Loss:90.5344\n",
      "timer:  22.9990 sec.\n",
      "lr is: 1e-05\n",
      "-------------\n",
      "Epoch 201/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6610 || Loss: 5.0231 || 10iter: 10.6176 sec.\n",
      "Iteration 6620 || Loss: 5.7733 || 10iter: 4.1916 sec.\n",
      "Iteration 6630 || Loss: 4.8963 || 10iter: 2.6768 sec.\n",
      "-------------\n",
      "epoch 201 || Epoch_TRAIN_Loss:178.3179 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  18.5415 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
