{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "os.makedirs(\"log\", exist_ok=True)\n",
    "input_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val not found\n",
      "trainlist:  6469\n",
      "vallist:  2097\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# extend with VOC2012\n",
    "vocpath = \"../VOCdevkit/VOC2012\"\n",
    "train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath, cls=\"person\", VOC2012=True)\n",
    "\n",
    "train_img_list.extend(train_img_list2)\n",
    "train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "\n",
    "print(\"trainlist: \", len(train_img_list))\n",
    "print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ..., -66.0397, -60.8152, -67.4163],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ..., -24.9051, -10.3171, -15.8597],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,  25.9560,  32.9662,  30.9724],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]]),\n",
       " array([[0.77328647, 0.48931624, 0.91212654, 0.79487179, 0.        ],\n",
       "        [0.96309315, 0.61111111, 1.        , 0.89102564, 0.        ]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "32\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0573, 0.0000, 1.0000, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.blazeface import SSD256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 256,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16],  # DBOXの大きさを決める\n",
    "    'min_sizes': [16, 32],  # DBOXの大きさを決める\n",
    "    'max_sizes': [32, 100],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD256(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD256(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra2): BlazeFaceExtra2(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-4\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log/log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface256_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 65.4531 || 10iter: 7.2785 sec.\n",
      "Iteration 20 || Loss: 49.4294 || 10iter: 3.5289 sec.\n",
      "Iteration 30 || Loss: 64.4441 || 10iter: 3.9851 sec.\n",
      "Iteration 40 || Loss: 37.6062 || 10iter: 4.2480 sec.\n",
      "Iteration 50 || Loss: 56.9393 || 10iter: 4.0935 sec.\n",
      "Iteration 60 || Loss: 50.8827 || 10iter: 4.1093 sec.\n",
      "Iteration 70 || Loss: 60.1804 || 10iter: 4.0369 sec.\n",
      "Iteration 80 || Loss: 43.3787 || 10iter: 3.9698 sec.\n",
      "Iteration 90 || Loss: 48.7653 || 10iter: 3.9149 sec.\n",
      "Iteration 100 || Loss: 49.1596 || 10iter: 4.0868 sec.\n",
      "Iteration 110 || Loss: 39.0327 || 10iter: 4.1437 sec.\n",
      "Iteration 120 || Loss: 46.1667 || 10iter: 4.0960 sec.\n",
      "Iteration 130 || Loss: 58.0489 || 10iter: 4.1082 sec.\n",
      "Iteration 140 || Loss: 49.2085 || 10iter: 3.9622 sec.\n",
      "Iteration 150 || Loss: 48.5656 || 10iter: 4.1017 sec.\n",
      "Iteration 160 || Loss: 39.4221 || 10iter: 3.7092 sec.\n",
      "Iteration 170 || Loss: 47.2192 || 10iter: 3.7537 sec.\n",
      "Iteration 180 || Loss: 42.5922 || 10iter: 3.7061 sec.\n",
      "Iteration 190 || Loss: 40.4525 || 10iter: 3.5844 sec.\n",
      "Iteration 200 || Loss: 38.3090 || 10iter: 3.3327 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:10714.4430 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  82.9053 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 210 || Loss: 48.8379 || 10iter: 5.7393 sec.\n",
      "Iteration 220 || Loss: 35.9089 || 10iter: 3.6852 sec.\n",
      "Iteration 230 || Loss: 37.3149 || 10iter: 3.7270 sec.\n",
      "Iteration 240 || Loss: 40.6810 || 10iter: 3.7706 sec.\n",
      "Iteration 250 || Loss: 43.4674 || 10iter: 3.6401 sec.\n",
      "Iteration 260 || Loss: 47.3659 || 10iter: 3.7846 sec.\n",
      "Iteration 270 || Loss: 36.9680 || 10iter: 3.7297 sec.\n",
      "Iteration 280 || Loss: 30.0206 || 10iter: 3.6035 sec.\n",
      "Iteration 290 || Loss: 26.0999 || 10iter: 3.6699 sec.\n",
      "Iteration 300 || Loss: 32.3027 || 10iter: 3.6979 sec.\n",
      "Iteration 310 || Loss: 36.4805 || 10iter: 3.6716 sec.\n",
      "Iteration 320 || Loss: 30.9623 || 10iter: 3.6738 sec.\n",
      "Iteration 330 || Loss: 36.0357 || 10iter: 3.6935 sec.\n",
      "Iteration 340 || Loss: 32.1889 || 10iter: 3.6056 sec.\n",
      "Iteration 350 || Loss: 38.8257 || 10iter: 3.5936 sec.\n",
      "Iteration 360 || Loss: 33.5170 || 10iter: 3.6954 sec.\n",
      "Iteration 370 || Loss: 34.8093 || 10iter: 3.7186 sec.\n",
      "Iteration 380 || Loss: 30.3271 || 10iter: 4.0283 sec.\n",
      "Iteration 390 || Loss: 39.2620 || 10iter: 3.6747 sec.\n",
      "Iteration 400 || Loss: 32.0571 || 10iter: 3.3872 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:7498.0603 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6139 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 410 || Loss: 27.7411 || 10iter: 4.3920 sec.\n",
      "Iteration 420 || Loss: 27.7113 || 10iter: 4.0320 sec.\n",
      "Iteration 430 || Loss: 32.8583 || 10iter: 3.6873 sec.\n",
      "Iteration 440 || Loss: 30.6177 || 10iter: 3.7061 sec.\n",
      "Iteration 450 || Loss: 27.9105 || 10iter: 3.8089 sec.\n",
      "Iteration 460 || Loss: 33.8062 || 10iter: 3.6799 sec.\n",
      "Iteration 470 || Loss: 30.1208 || 10iter: 3.7576 sec.\n",
      "Iteration 480 || Loss: 25.6990 || 10iter: 3.6587 sec.\n",
      "Iteration 490 || Loss: 26.9587 || 10iter: 3.6759 sec.\n",
      "Iteration 500 || Loss: 28.8140 || 10iter: 3.6906 sec.\n",
      "Iteration 510 || Loss: 26.1486 || 10iter: 3.7936 sec.\n",
      "Iteration 520 || Loss: 32.1701 || 10iter: 3.6670 sec.\n",
      "Iteration 530 || Loss: 31.0261 || 10iter: 3.8067 sec.\n",
      "Iteration 540 || Loss: 27.1796 || 10iter: 3.7112 sec.\n",
      "Iteration 550 || Loss: 24.2448 || 10iter: 3.7638 sec.\n",
      "Iteration 560 || Loss: 27.7339 || 10iter: 3.7543 sec.\n",
      "Iteration 570 || Loss: 23.3498 || 10iter: 3.6529 sec.\n",
      "Iteration 580 || Loss: 32.3050 || 10iter: 3.7352 sec.\n",
      "Iteration 590 || Loss: 27.8575 || 10iter: 3.7011 sec.\n",
      "Iteration 600 || Loss: 21.6059 || 10iter: 3.5214 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:5652.9033 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.0294 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 610 || Loss: 23.8166 || 10iter: 2.9462 sec.\n",
      "Iteration 620 || Loss: 24.8189 || 10iter: 4.4583 sec.\n",
      "Iteration 630 || Loss: 26.4088 || 10iter: 3.6203 sec.\n",
      "Iteration 640 || Loss: 27.3839 || 10iter: 3.7240 sec.\n",
      "Iteration 650 || Loss: 22.4067 || 10iter: 3.7042 sec.\n",
      "Iteration 660 || Loss: 28.5184 || 10iter: 3.7153 sec.\n",
      "Iteration 670 || Loss: 29.9719 || 10iter: 3.6892 sec.\n",
      "Iteration 680 || Loss: 17.7375 || 10iter: 3.6219 sec.\n",
      "Iteration 690 || Loss: 28.9028 || 10iter: 3.7485 sec.\n",
      "Iteration 700 || Loss: 23.0245 || 10iter: 3.7214 sec.\n",
      "Iteration 710 || Loss: 22.2653 || 10iter: 3.7364 sec.\n",
      "Iteration 720 || Loss: 32.3164 || 10iter: 3.6569 sec.\n",
      "Iteration 730 || Loss: 30.7008 || 10iter: 3.7129 sec.\n",
      "Iteration 740 || Loss: 34.7682 || 10iter: 3.6934 sec.\n",
      "Iteration 750 || Loss: 27.6331 || 10iter: 3.6920 sec.\n",
      "Iteration 760 || Loss: 26.6333 || 10iter: 3.6563 sec.\n",
      "Iteration 770 || Loss: 28.2584 || 10iter: 3.7518 sec.\n",
      "Iteration 780 || Loss: 22.3539 || 10iter: 3.7785 sec.\n",
      "Iteration 790 || Loss: 22.2334 || 10iter: 3.6418 sec.\n",
      "Iteration 800 || Loss: 22.9183 || 10iter: 3.5588 sec.\n",
      "Iteration 810 || Loss: 30.4287 || 10iter: 3.3993 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:5398.4665 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6978 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 820 || Loss: 21.7738 || 10iter: 6.1706 sec.\n",
      "Iteration 830 || Loss: 24.7067 || 10iter: 3.7465 sec.\n",
      "Iteration 840 || Loss: 23.0346 || 10iter: 3.9047 sec.\n",
      "Iteration 850 || Loss: 25.1425 || 10iter: 3.8610 sec.\n",
      "Iteration 860 || Loss: 22.2893 || 10iter: 3.7681 sec.\n",
      "Iteration 870 || Loss: 24.6029 || 10iter: 3.7965 sec.\n",
      "Iteration 880 || Loss: 28.9262 || 10iter: 3.5842 sec.\n",
      "Iteration 890 || Loss: 22.1613 || 10iter: 3.6501 sec.\n",
      "Iteration 900 || Loss: 19.9273 || 10iter: 3.6386 sec.\n",
      "Iteration 910 || Loss: 23.3924 || 10iter: 3.6513 sec.\n",
      "Iteration 920 || Loss: 31.6503 || 10iter: 3.7264 sec.\n",
      "Iteration 930 || Loss: 30.3737 || 10iter: 3.6577 sec.\n",
      "Iteration 940 || Loss: 28.3220 || 10iter: 3.6876 sec.\n",
      "Iteration 950 || Loss: 29.8245 || 10iter: 3.6231 sec.\n",
      "Iteration 960 || Loss: 22.3884 || 10iter: 3.6718 sec.\n",
      "Iteration 970 || Loss: 25.8995 || 10iter: 3.7057 sec.\n",
      "Iteration 980 || Loss: 29.6855 || 10iter: 3.7506 sec.\n",
      "Iteration 990 || Loss: 21.9468 || 10iter: 3.6755 sec.\n",
      "Iteration 1000 || Loss: 23.9600 || 10iter: 3.6474 sec.\n",
      "Iteration 1010 || Loss: 25.7840 || 10iter: 3.3764 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:5264.1710 ||Epoch_VAL_Loss:1450.5376\n",
      "timer:  89.4748 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1020 || Loss: 22.5001 || 10iter: 5.0447 sec.\n",
      "Iteration 1030 || Loss: 25.0749 || 10iter: 3.7316 sec.\n",
      "Iteration 1040 || Loss: 29.6452 || 10iter: 3.6837 sec.\n",
      "Iteration 1050 || Loss: 25.7257 || 10iter: 3.6741 sec.\n",
      "Iteration 1060 || Loss: 27.7900 || 10iter: 3.7240 sec.\n",
      "Iteration 1070 || Loss: 26.8757 || 10iter: 3.6883 sec.\n",
      "Iteration 1080 || Loss: 20.8379 || 10iter: 3.6849 sec.\n",
      "Iteration 1090 || Loss: 23.5449 || 10iter: 3.6817 sec.\n",
      "Iteration 1100 || Loss: 23.2037 || 10iter: 3.7220 sec.\n",
      "Iteration 1110 || Loss: 17.9627 || 10iter: 3.7103 sec.\n",
      "Iteration 1120 || Loss: 28.4751 || 10iter: 3.6421 sec.\n",
      "Iteration 1130 || Loss: 25.7973 || 10iter: 3.6217 sec.\n",
      "Iteration 1140 || Loss: 27.6130 || 10iter: 3.6806 sec.\n",
      "Iteration 1150 || Loss: 29.4225 || 10iter: 3.7231 sec.\n",
      "Iteration 1160 || Loss: 30.1665 || 10iter: 3.6930 sec.\n",
      "Iteration 1170 || Loss: 24.2821 || 10iter: 3.6794 sec.\n",
      "Iteration 1180 || Loss: 25.2625 || 10iter: 3.7185 sec.\n",
      "Iteration 1190 || Loss: 29.1724 || 10iter: 3.6529 sec.\n",
      "Iteration 1200 || Loss: 24.4612 || 10iter: 3.6940 sec.\n",
      "Iteration 1210 || Loss: 27.7585 || 10iter: 3.4037 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:5155.2018 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.3692 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1220 || Loss: 22.8126 || 10iter: 3.5321 sec.\n",
      "Iteration 1230 || Loss: 25.3862 || 10iter: 4.2789 sec.\n",
      "Iteration 1240 || Loss: 22.2603 || 10iter: 3.6760 sec.\n",
      "Iteration 1250 || Loss: 21.1276 || 10iter: 3.6515 sec.\n",
      "Iteration 1260 || Loss: 30.5461 || 10iter: 3.6758 sec.\n",
      "Iteration 1270 || Loss: 26.6565 || 10iter: 3.8240 sec.\n",
      "Iteration 1280 || Loss: 23.4265 || 10iter: 3.8353 sec.\n",
      "Iteration 1290 || Loss: 26.0472 || 10iter: 3.7313 sec.\n",
      "Iteration 1300 || Loss: 24.8107 || 10iter: 3.7191 sec.\n",
      "Iteration 1310 || Loss: 21.1861 || 10iter: 3.6871 sec.\n",
      "Iteration 1320 || Loss: 21.8057 || 10iter: 3.7782 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1330 || Loss: 28.5187 || 10iter: 3.6563 sec.\n",
      "Iteration 1340 || Loss: 23.2645 || 10iter: 3.6676 sec.\n",
      "Iteration 1350 || Loss: 25.6298 || 10iter: 3.7454 sec.\n",
      "Iteration 1360 || Loss: 27.1195 || 10iter: 3.6629 sec.\n",
      "Iteration 1370 || Loss: 32.8886 || 10iter: 3.7117 sec.\n",
      "Iteration 1380 || Loss: 25.4674 || 10iter: 3.6986 sec.\n",
      "Iteration 1390 || Loss: 27.7631 || 10iter: 3.7003 sec.\n",
      "Iteration 1400 || Loss: 25.8313 || 10iter: 3.7335 sec.\n",
      "Iteration 1410 || Loss: 23.4439 || 10iter: 3.5650 sec.\n",
      "Iteration 1420 || Loss: 25.9289 || 10iter: 3.3516 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:5163.7777 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.0187 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1430 || Loss: 21.4215 || 10iter: 6.7832 sec.\n",
      "Iteration 1440 || Loss: 22.4837 || 10iter: 3.6024 sec.\n",
      "Iteration 1450 || Loss: 30.6683 || 10iter: 3.6801 sec.\n",
      "Iteration 1460 || Loss: 22.0837 || 10iter: 3.7614 sec.\n",
      "Iteration 1470 || Loss: 26.2949 || 10iter: 3.6932 sec.\n",
      "Iteration 1480 || Loss: 29.4908 || 10iter: 3.7358 sec.\n",
      "Iteration 1490 || Loss: 23.0888 || 10iter: 3.6125 sec.\n",
      "Iteration 1500 || Loss: 20.9762 || 10iter: 3.6863 sec.\n",
      "Iteration 1510 || Loss: 22.4559 || 10iter: 3.7302 sec.\n",
      "Iteration 1520 || Loss: 23.2148 || 10iter: 3.6451 sec.\n",
      "Iteration 1530 || Loss: 27.5932 || 10iter: 3.7190 sec.\n",
      "Iteration 1540 || Loss: 25.2592 || 10iter: 3.7160 sec.\n",
      "Iteration 1550 || Loss: 23.2583 || 10iter: 3.6548 sec.\n",
      "Iteration 1560 || Loss: 22.0883 || 10iter: 3.6188 sec.\n",
      "Iteration 1570 || Loss: 25.7065 || 10iter: 3.6401 sec.\n",
      "Iteration 1580 || Loss: 17.4471 || 10iter: 3.6808 sec.\n",
      "Iteration 1590 || Loss: 26.9308 || 10iter: 3.7541 sec.\n",
      "Iteration 1600 || Loss: 24.5667 || 10iter: 3.6414 sec.\n",
      "Iteration 1610 || Loss: 22.7765 || 10iter: 3.5932 sec.\n",
      "Iteration 1620 || Loss: 32.1626 || 10iter: 3.3442 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:5032.3847 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.4275 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1630 || Loss: 20.0303 || 10iter: 5.8541 sec.\n",
      "Iteration 1640 || Loss: 25.8997 || 10iter: 3.6615 sec.\n",
      "Iteration 1650 || Loss: 17.9249 || 10iter: 3.7276 sec.\n",
      "Iteration 1660 || Loss: 19.2004 || 10iter: 3.6736 sec.\n",
      "Iteration 1670 || Loss: 23.1763 || 10iter: 3.7060 sec.\n",
      "Iteration 1680 || Loss: 19.1241 || 10iter: 3.6890 sec.\n",
      "Iteration 1690 || Loss: 33.7223 || 10iter: 3.7120 sec.\n",
      "Iteration 1700 || Loss: 22.8570 || 10iter: 3.7179 sec.\n",
      "Iteration 1710 || Loss: 24.0776 || 10iter: 3.6872 sec.\n",
      "Iteration 1720 || Loss: 22.0664 || 10iter: 3.7217 sec.\n",
      "Iteration 1730 || Loss: 30.8658 || 10iter: 3.7063 sec.\n",
      "Iteration 1740 || Loss: 30.8230 || 10iter: 3.7114 sec.\n",
      "Iteration 1750 || Loss: 22.4893 || 10iter: 3.8059 sec.\n",
      "Iteration 1760 || Loss: 14.9505 || 10iter: 3.9992 sec.\n",
      "Iteration 1770 || Loss: 27.8247 || 10iter: 3.7100 sec.\n",
      "Iteration 1780 || Loss: 22.2590 || 10iter: 3.6329 sec.\n",
      "Iteration 1790 || Loss: 25.2418 || 10iter: 3.6191 sec.\n",
      "Iteration 1800 || Loss: 23.0491 || 10iter: 3.7147 sec.\n",
      "Iteration 1810 || Loss: 23.2150 || 10iter: 3.6926 sec.\n",
      "Iteration 1820 || Loss: 22.6256 || 10iter: 3.3828 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:4820.7784 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.2602 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1830 || Loss: 21.1915 || 10iter: 4.2230 sec.\n",
      "Iteration 1840 || Loss: 24.9077 || 10iter: 4.0354 sec.\n",
      "Iteration 1850 || Loss: 21.8300 || 10iter: 3.7500 sec.\n",
      "Iteration 1860 || Loss: 26.6762 || 10iter: 3.6630 sec.\n",
      "Iteration 1870 || Loss: 24.2347 || 10iter: 3.7518 sec.\n",
      "Iteration 1880 || Loss: 20.0194 || 10iter: 3.8045 sec.\n",
      "Iteration 1890 || Loss: 24.1309 || 10iter: 3.6840 sec.\n",
      "Iteration 1900 || Loss: 22.0805 || 10iter: 3.6382 sec.\n",
      "Iteration 1910 || Loss: 17.0979 || 10iter: 3.6841 sec.\n",
      "Iteration 1920 || Loss: 22.5074 || 10iter: 3.6709 sec.\n",
      "Iteration 1930 || Loss: 23.5825 || 10iter: 3.6759 sec.\n",
      "Iteration 1940 || Loss: 28.9306 || 10iter: 3.7009 sec.\n",
      "Iteration 1950 || Loss: 25.7364 || 10iter: 3.6990 sec.\n",
      "Iteration 1960 || Loss: 26.4771 || 10iter: 3.6236 sec.\n",
      "Iteration 1970 || Loss: 22.3818 || 10iter: 3.6405 sec.\n",
      "Iteration 1980 || Loss: 19.3138 || 10iter: 3.7096 sec.\n",
      "Iteration 1990 || Loss: 26.6447 || 10iter: 3.6747 sec.\n",
      "Iteration 2000 || Loss: 25.3314 || 10iter: 3.7215 sec.\n",
      "Iteration 2010 || Loss: 20.6941 || 10iter: 3.7367 sec.\n",
      "Iteration 2020 || Loss: 20.3540 || 10iter: 3.4998 sec.\n",
      "Iteration 2030 || Loss: 13.4167 || 10iter: 3.1120 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:4859.0321 ||Epoch_VAL_Loss:1334.2667\n",
      "timer:  89.3187 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2040 || Loss: 35.5565 || 10iter: 6.9785 sec.\n",
      "Iteration 2050 || Loss: 23.2241 || 10iter: 3.6428 sec.\n",
      "Iteration 2060 || Loss: 21.4945 || 10iter: 3.7773 sec.\n",
      "Iteration 2070 || Loss: 22.6796 || 10iter: 3.6405 sec.\n",
      "Iteration 2080 || Loss: 20.3774 || 10iter: 3.6741 sec.\n",
      "Iteration 2090 || Loss: 20.4615 || 10iter: 3.6551 sec.\n",
      "Iteration 2100 || Loss: 29.1061 || 10iter: 3.5909 sec.\n",
      "Iteration 2110 || Loss: 19.6948 || 10iter: 3.7343 sec.\n",
      "Iteration 2120 || Loss: 20.8861 || 10iter: 3.6436 sec.\n",
      "Iteration 2130 || Loss: 21.3947 || 10iter: 3.6940 sec.\n",
      "Iteration 2140 || Loss: 25.0073 || 10iter: 3.7006 sec.\n",
      "Iteration 2150 || Loss: 20.1161 || 10iter: 3.7799 sec.\n",
      "Iteration 2160 || Loss: 17.7037 || 10iter: 3.7307 sec.\n",
      "Iteration 2170 || Loss: 19.3516 || 10iter: 3.7935 sec.\n",
      "Iteration 2180 || Loss: 20.5858 || 10iter: 3.6824 sec.\n",
      "Iteration 2190 || Loss: 17.2807 || 10iter: 3.8197 sec.\n",
      "Iteration 2200 || Loss: 23.8712 || 10iter: 3.9470 sec.\n",
      "Iteration 2210 || Loss: 20.1798 || 10iter: 3.6647 sec.\n",
      "Iteration 2220 || Loss: 16.5211 || 10iter: 3.6035 sec.\n",
      "Iteration 2230 || Loss: 23.8825 || 10iter: 3.3772 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:4727.6791 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9426 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2240 || Loss: 17.9434 || 10iter: 6.0617 sec.\n",
      "Iteration 2250 || Loss: 38.5586 || 10iter: 3.7344 sec.\n",
      "Iteration 2260 || Loss: 21.5749 || 10iter: 3.7418 sec.\n",
      "Iteration 2270 || Loss: 27.0228 || 10iter: 3.6844 sec.\n",
      "Iteration 2280 || Loss: 28.6167 || 10iter: 3.8826 sec.\n",
      "Iteration 2290 || Loss: 20.4628 || 10iter: 4.0572 sec.\n",
      "Iteration 2300 || Loss: 22.0910 || 10iter: 3.8860 sec.\n",
      "Iteration 2310 || Loss: 20.4261 || 10iter: 3.9248 sec.\n",
      "Iteration 2320 || Loss: 20.7438 || 10iter: 4.0261 sec.\n",
      "Iteration 2330 || Loss: 24.8951 || 10iter: 4.0293 sec.\n",
      "Iteration 2340 || Loss: 21.9170 || 10iter: 4.0595 sec.\n",
      "Iteration 2350 || Loss: 27.1855 || 10iter: 3.9099 sec.\n",
      "Iteration 2360 || Loss: 26.6639 || 10iter: 4.2136 sec.\n",
      "Iteration 2370 || Loss: 38.0220 || 10iter: 4.0964 sec.\n",
      "Iteration 2380 || Loss: 25.3532 || 10iter: 4.0853 sec.\n",
      "Iteration 2390 || Loss: 22.2265 || 10iter: 4.0183 sec.\n",
      "Iteration 2400 || Loss: 18.1313 || 10iter: 4.0595 sec.\n",
      "Iteration 2410 || Loss: 20.4206 || 10iter: 3.8310 sec.\n",
      "Iteration 2420 || Loss: 22.5807 || 10iter: 3.7506 sec.\n",
      "Iteration 2430 || Loss: 21.6747 || 10iter: 3.3817 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:4744.7352 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  82.2586 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2440 || Loss: 23.4880 || 10iter: 4.8166 sec.\n",
      "Iteration 2450 || Loss: 23.7166 || 10iter: 3.9857 sec.\n",
      "Iteration 2460 || Loss: 15.7485 || 10iter: 3.6732 sec.\n",
      "Iteration 2470 || Loss: 25.6732 || 10iter: 3.7332 sec.\n",
      "Iteration 2480 || Loss: 27.3834 || 10iter: 3.7167 sec.\n",
      "Iteration 2490 || Loss: 25.7106 || 10iter: 3.7048 sec.\n",
      "Iteration 2500 || Loss: 21.4528 || 10iter: 3.7022 sec.\n",
      "Iteration 2510 || Loss: 17.1887 || 10iter: 3.6417 sec.\n",
      "Iteration 2520 || Loss: 18.5004 || 10iter: 3.6388 sec.\n",
      "Iteration 2530 || Loss: 23.5138 || 10iter: 3.6561 sec.\n",
      "Iteration 2540 || Loss: 22.1606 || 10iter: 3.7952 sec.\n",
      "Iteration 2550 || Loss: 25.5941 || 10iter: 3.7338 sec.\n",
      "Iteration 2560 || Loss: 22.4780 || 10iter: 3.7225 sec.\n",
      "Iteration 2570 || Loss: 20.9228 || 10iter: 3.6951 sec.\n",
      "Iteration 2580 || Loss: 23.8102 || 10iter: 3.7151 sec.\n",
      "Iteration 2590 || Loss: 16.5733 || 10iter: 3.6687 sec.\n",
      "Iteration 2600 || Loss: 21.2769 || 10iter: 3.6397 sec.\n",
      "Iteration 2610 || Loss: 24.0121 || 10iter: 3.7064 sec.\n",
      "Iteration 2620 || Loss: 18.4522 || 10iter: 3.7080 sec.\n",
      "Iteration 2630 || Loss: 21.1605 || 10iter: 3.5100 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:4814.2753 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9688 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2640 || Loss: 23.5733 || 10iter: 2.7122 sec.\n",
      "Iteration 2650 || Loss: 21.6336 || 10iter: 5.1560 sec.\n",
      "Iteration 2660 || Loss: 22.8657 || 10iter: 3.7301 sec.\n",
      "Iteration 2670 || Loss: 30.8782 || 10iter: 3.6904 sec.\n",
      "Iteration 2680 || Loss: 23.3613 || 10iter: 3.6443 sec.\n",
      "Iteration 2690 || Loss: 22.7834 || 10iter: 3.7788 sec.\n",
      "Iteration 2700 || Loss: 29.5346 || 10iter: 3.6708 sec.\n",
      "Iteration 2710 || Loss: 19.5737 || 10iter: 3.6891 sec.\n",
      "Iteration 2720 || Loss: 21.3496 || 10iter: 3.7199 sec.\n",
      "Iteration 2730 || Loss: 26.8876 || 10iter: 3.7551 sec.\n",
      "Iteration 2740 || Loss: 18.1088 || 10iter: 3.7981 sec.\n",
      "Iteration 2750 || Loss: 28.0163 || 10iter: 3.6853 sec.\n",
      "Iteration 2760 || Loss: 23.5762 || 10iter: 3.7476 sec.\n",
      "Iteration 2770 || Loss: 21.2818 || 10iter: 3.6769 sec.\n",
      "Iteration 2780 || Loss: 19.8663 || 10iter: 3.7176 sec.\n",
      "Iteration 2790 || Loss: 21.9565 || 10iter: 3.7084 sec.\n",
      "Iteration 2800 || Loss: 21.4363 || 10iter: 3.6855 sec.\n",
      "Iteration 2810 || Loss: 27.1589 || 10iter: 3.7573 sec.\n",
      "Iteration 2820 || Loss: 24.3021 || 10iter: 3.7151 sec.\n",
      "Iteration 2830 || Loss: 21.4088 || 10iter: 3.5556 sec.\n",
      "Iteration 2840 || Loss: 24.2427 || 10iter: 3.3727 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:4733.4277 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.4444 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2850 || Loss: 22.4431 || 10iter: 6.3750 sec.\n",
      "Iteration 2860 || Loss: 22.3235 || 10iter: 3.7342 sec.\n",
      "Iteration 2870 || Loss: 21.1942 || 10iter: 3.7666 sec.\n",
      "Iteration 2880 || Loss: 21.8648 || 10iter: 3.6985 sec.\n",
      "Iteration 2890 || Loss: 35.1060 || 10iter: 3.7407 sec.\n",
      "Iteration 2900 || Loss: 19.6494 || 10iter: 3.6606 sec.\n",
      "Iteration 2910 || Loss: 19.6564 || 10iter: 3.6756 sec.\n",
      "Iteration 2920 || Loss: 20.5134 || 10iter: 3.7187 sec.\n",
      "Iteration 2930 || Loss: 25.6407 || 10iter: 3.5764 sec.\n",
      "Iteration 2940 || Loss: 24.2078 || 10iter: 3.6899 sec.\n",
      "Iteration 2950 || Loss: 25.9006 || 10iter: 3.6577 sec.\n",
      "Iteration 2960 || Loss: 20.9072 || 10iter: 3.6641 sec.\n",
      "Iteration 2970 || Loss: 23.3106 || 10iter: 3.7638 sec.\n",
      "Iteration 2980 || Loss: 29.3364 || 10iter: 3.7474 sec.\n",
      "Iteration 2990 || Loss: 24.2408 || 10iter: 3.7232 sec.\n",
      "Iteration 3000 || Loss: 22.1854 || 10iter: 3.6503 sec.\n",
      "Iteration 3010 || Loss: 17.4786 || 10iter: 3.7158 sec.\n",
      "Iteration 3020 || Loss: 23.7014 || 10iter: 3.6653 sec.\n",
      "Iteration 3030 || Loss: 22.5346 || 10iter: 3.6743 sec.\n",
      "Iteration 3040 || Loss: 28.3685 || 10iter: 3.3311 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:4695.4797 ||Epoch_VAL_Loss:1247.9201\n",
      "timer:  89.2301 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3050 || Loss: 18.1779 || 10iter: 5.0548 sec.\n",
      "Iteration 3060 || Loss: 22.4620 || 10iter: 3.8563 sec.\n",
      "Iteration 3070 || Loss: 15.3544 || 10iter: 3.6809 sec.\n",
      "Iteration 3080 || Loss: 29.5844 || 10iter: 3.7475 sec.\n",
      "Iteration 3090 || Loss: 26.4334 || 10iter: 4.0097 sec.\n",
      "Iteration 3100 || Loss: 26.1751 || 10iter: 3.7503 sec.\n",
      "Iteration 3110 || Loss: 25.4795 || 10iter: 3.6278 sec.\n",
      "Iteration 3120 || Loss: 24.5729 || 10iter: 3.6786 sec.\n",
      "Iteration 3130 || Loss: 21.5070 || 10iter: 3.6418 sec.\n",
      "Iteration 3140 || Loss: 20.8747 || 10iter: 3.7371 sec.\n",
      "Iteration 3150 || Loss: 18.3083 || 10iter: 3.6767 sec.\n",
      "Iteration 3160 || Loss: 33.8030 || 10iter: 3.7695 sec.\n",
      "Iteration 3170 || Loss: 24.1604 || 10iter: 3.6926 sec.\n",
      "Iteration 3180 || Loss: 31.7239 || 10iter: 3.7182 sec.\n",
      "Iteration 3190 || Loss: 21.9366 || 10iter: 3.6756 sec.\n",
      "Iteration 3200 || Loss: 19.0238 || 10iter: 3.6371 sec.\n",
      "Iteration 3210 || Loss: 21.9479 || 10iter: 3.7759 sec.\n",
      "Iteration 3220 || Loss: 37.6101 || 10iter: 3.6827 sec.\n",
      "Iteration 3230 || Loss: 21.0550 || 10iter: 3.6586 sec.\n",
      "Iteration 3240 || Loss: 16.7743 || 10iter: 3.4593 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:4615.4781 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.0260 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 17/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3250 || Loss: 29.5656 || 10iter: 3.4902 sec.\n",
      "Iteration 3260 || Loss: 18.3919 || 10iter: 4.5768 sec.\n",
      "Iteration 3270 || Loss: 25.9952 || 10iter: 3.6120 sec.\n",
      "Iteration 3280 || Loss: 18.4183 || 10iter: 3.7290 sec.\n",
      "Iteration 3290 || Loss: 18.8492 || 10iter: 3.6682 sec.\n",
      "Iteration 3300 || Loss: 22.2415 || 10iter: 3.7301 sec.\n",
      "Iteration 3310 || Loss: 21.6760 || 10iter: 3.7187 sec.\n",
      "Iteration 3320 || Loss: 23.3504 || 10iter: 3.6826 sec.\n",
      "Iteration 3330 || Loss: 21.4873 || 10iter: 3.6403 sec.\n",
      "Iteration 3340 || Loss: 26.4896 || 10iter: 3.7266 sec.\n",
      "Iteration 3350 || Loss: 22.3603 || 10iter: 3.7471 sec.\n",
      "Iteration 3360 || Loss: 23.2289 || 10iter: 3.6237 sec.\n",
      "Iteration 3370 || Loss: 20.4855 || 10iter: 3.7105 sec.\n",
      "Iteration 3380 || Loss: 27.5524 || 10iter: 3.7305 sec.\n",
      "Iteration 3390 || Loss: 22.3881 || 10iter: 3.7226 sec.\n",
      "Iteration 3400 || Loss: 21.9949 || 10iter: 3.7215 sec.\n",
      "Iteration 3410 || Loss: 26.6316 || 10iter: 3.7152 sec.\n",
      "Iteration 3420 || Loss: 16.0891 || 10iter: 3.7685 sec.\n",
      "Iteration 3430 || Loss: 22.2278 || 10iter: 3.6710 sec.\n",
      "Iteration 3440 || Loss: 25.9678 || 10iter: 3.5323 sec.\n",
      "Iteration 3450 || Loss: 26.9528 || 10iter: 3.3978 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:4598.8903 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.0525 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 18/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3460 || Loss: 20.8791 || 10iter: 7.0534 sec.\n",
      "Iteration 3470 || Loss: 18.6835 || 10iter: 3.6065 sec.\n",
      "Iteration 3480 || Loss: 23.5321 || 10iter: 3.7498 sec.\n",
      "Iteration 3490 || Loss: 17.9958 || 10iter: 3.6173 sec.\n",
      "Iteration 3500 || Loss: 21.8170 || 10iter: 3.7009 sec.\n",
      "Iteration 3510 || Loss: 22.9887 || 10iter: 3.6816 sec.\n",
      "Iteration 3520 || Loss: 22.7331 || 10iter: 3.6993 sec.\n",
      "Iteration 3530 || Loss: 12.5974 || 10iter: 3.7057 sec.\n",
      "Iteration 3540 || Loss: 15.8768 || 10iter: 3.6857 sec.\n",
      "Iteration 3550 || Loss: 17.4406 || 10iter: 3.7974 sec.\n",
      "Iteration 3560 || Loss: 17.4231 || 10iter: 3.9883 sec.\n",
      "Iteration 3570 || Loss: 18.6556 || 10iter: 3.6900 sec.\n",
      "Iteration 3580 || Loss: 20.1463 || 10iter: 3.7030 sec.\n",
      "Iteration 3590 || Loss: 19.8282 || 10iter: 3.7286 sec.\n",
      "Iteration 3600 || Loss: 19.8106 || 10iter: 3.7145 sec.\n",
      "Iteration 3610 || Loss: 18.3390 || 10iter: 3.7719 sec.\n",
      "Iteration 3620 || Loss: 26.1223 || 10iter: 3.7914 sec.\n",
      "Iteration 3630 || Loss: 23.3476 || 10iter: 3.6898 sec.\n",
      "Iteration 3640 || Loss: 22.6033 || 10iter: 3.5747 sec.\n",
      "Iteration 3650 || Loss: 26.5591 || 10iter: 3.3954 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:4610.9711 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.4922 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 19/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3660 || Loss: 19.3161 || 10iter: 5.3576 sec.\n",
      "Iteration 3670 || Loss: 23.2340 || 10iter: 3.8389 sec.\n",
      "Iteration 3680 || Loss: 22.9289 || 10iter: 3.7443 sec.\n",
      "Iteration 3690 || Loss: 25.5714 || 10iter: 3.6489 sec.\n",
      "Iteration 3700 || Loss: 23.1466 || 10iter: 3.6793 sec.\n",
      "Iteration 3710 || Loss: 26.9391 || 10iter: 3.7246 sec.\n",
      "Iteration 3720 || Loss: 19.1387 || 10iter: 3.6026 sec.\n",
      "Iteration 3730 || Loss: 23.7307 || 10iter: 3.7164 sec.\n",
      "Iteration 3740 || Loss: 21.7454 || 10iter: 3.7782 sec.\n",
      "Iteration 3750 || Loss: 24.6009 || 10iter: 3.7477 sec.\n",
      "Iteration 3760 || Loss: 21.7629 || 10iter: 3.7174 sec.\n",
      "Iteration 3770 || Loss: 24.0447 || 10iter: 3.6244 sec.\n",
      "Iteration 3780 || Loss: 22.5442 || 10iter: 3.6744 sec.\n",
      "Iteration 3790 || Loss: 24.8572 || 10iter: 3.7009 sec.\n",
      "Iteration 3800 || Loss: 25.5384 || 10iter: 3.6859 sec.\n",
      "Iteration 3810 || Loss: 23.4582 || 10iter: 3.6789 sec.\n",
      "Iteration 3820 || Loss: 16.4577 || 10iter: 3.6802 sec.\n",
      "Iteration 3830 || Loss: 28.0719 || 10iter: 3.6796 sec.\n",
      "Iteration 3840 || Loss: 32.5571 || 10iter: 3.7227 sec.\n",
      "Iteration 3850 || Loss: 20.0190 || 10iter: 3.3978 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:4537.4315 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.5363 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 20/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3860 || Loss: 25.2388 || 10iter: 3.6498 sec.\n",
      "Iteration 3870 || Loss: 25.1281 || 10iter: 4.3631 sec.\n",
      "Iteration 3880 || Loss: 16.3101 || 10iter: 3.5858 sec.\n",
      "Iteration 3890 || Loss: 22.1863 || 10iter: 3.6745 sec.\n",
      "Iteration 3900 || Loss: 24.3973 || 10iter: 3.6462 sec.\n",
      "Iteration 3910 || Loss: 28.7428 || 10iter: 3.6952 sec.\n",
      "Iteration 3920 || Loss: 29.1962 || 10iter: 3.6898 sec.\n",
      "Iteration 3930 || Loss: 16.9205 || 10iter: 3.6495 sec.\n",
      "Iteration 3940 || Loss: 22.9995 || 10iter: 3.7055 sec.\n",
      "Iteration 3950 || Loss: 16.0637 || 10iter: 3.6670 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3960 || Loss: 16.7632 || 10iter: 3.7216 sec.\n",
      "Iteration 3970 || Loss: 19.1432 || 10iter: 3.7196 sec.\n",
      "Iteration 3980 || Loss: 20.0888 || 10iter: 3.7248 sec.\n",
      "Iteration 3990 || Loss: 16.3594 || 10iter: 3.7372 sec.\n",
      "Iteration 4000 || Loss: 15.7826 || 10iter: 3.7239 sec.\n",
      "Iteration 4010 || Loss: 19.5723 || 10iter: 3.7168 sec.\n",
      "Iteration 4020 || Loss: 34.8089 || 10iter: 3.6296 sec.\n",
      "Iteration 4030 || Loss: 21.8820 || 10iter: 3.9800 sec.\n",
      "Iteration 4040 || Loss: 19.1816 || 10iter: 3.8548 sec.\n",
      "Iteration 4050 || Loss: 28.2533 || 10iter: 3.4571 sec.\n",
      "Iteration 4060 || Loss: 14.2667 || 10iter: 3.0954 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:4473.8668 ||Epoch_VAL_Loss:1208.6176\n",
      "timer:  89.2991 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 21/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4070 || Loss: 23.6523 || 10iter: 7.2052 sec.\n",
      "Iteration 4080 || Loss: 22.0044 || 10iter: 3.6781 sec.\n",
      "Iteration 4090 || Loss: 22.4187 || 10iter: 3.7297 sec.\n",
      "Iteration 4100 || Loss: 20.8925 || 10iter: 3.7202 sec.\n",
      "Iteration 4110 || Loss: 25.4070 || 10iter: 3.7184 sec.\n",
      "Iteration 4120 || Loss: 24.6221 || 10iter: 3.7396 sec.\n",
      "Iteration 4130 || Loss: 21.1195 || 10iter: 3.6684 sec.\n",
      "Iteration 4140 || Loss: 18.5056 || 10iter: 3.6109 sec.\n",
      "Iteration 4150 || Loss: 30.4000 || 10iter: 3.7595 sec.\n",
      "Iteration 4160 || Loss: 19.7066 || 10iter: 3.7462 sec.\n",
      "Iteration 4170 || Loss: 29.9714 || 10iter: 3.6538 sec.\n",
      "Iteration 4180 || Loss: 23.5352 || 10iter: 3.6375 sec.\n",
      "Iteration 4190 || Loss: 27.7274 || 10iter: 3.7126 sec.\n",
      "Iteration 4200 || Loss: 19.1377 || 10iter: 3.7479 sec.\n",
      "Iteration 4210 || Loss: 22.2755 || 10iter: 3.6784 sec.\n",
      "Iteration 4220 || Loss: 19.8092 || 10iter: 3.7477 sec.\n",
      "Iteration 4230 || Loss: 19.6835 || 10iter: 3.6985 sec.\n",
      "Iteration 4240 || Loss: 22.3614 || 10iter: 3.8020 sec.\n",
      "Iteration 4250 || Loss: 21.6969 || 10iter: 3.5715 sec.\n",
      "Iteration 4260 || Loss: 25.2252 || 10iter: 3.3331 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:4384.2987 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9610 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 22/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4270 || Loss: 18.5571 || 10iter: 6.0254 sec.\n",
      "Iteration 4280 || Loss: 16.6791 || 10iter: 3.7635 sec.\n",
      "Iteration 4290 || Loss: 18.8507 || 10iter: 3.6585 sec.\n",
      "Iteration 4300 || Loss: 20.4433 || 10iter: 3.7058 sec.\n",
      "Iteration 4310 || Loss: 23.2075 || 10iter: 3.7131 sec.\n",
      "Iteration 4320 || Loss: 22.7664 || 10iter: 3.7541 sec.\n",
      "Iteration 4330 || Loss: 24.4094 || 10iter: 3.7090 sec.\n",
      "Iteration 4340 || Loss: 18.6274 || 10iter: 3.6837 sec.\n",
      "Iteration 4350 || Loss: 13.9462 || 10iter: 3.6716 sec.\n",
      "Iteration 4360 || Loss: 12.4525 || 10iter: 3.7614 sec.\n",
      "Iteration 4370 || Loss: 22.3527 || 10iter: 3.6515 sec.\n",
      "Iteration 4380 || Loss: 20.8920 || 10iter: 3.6750 sec.\n",
      "Iteration 4390 || Loss: 16.0322 || 10iter: 3.7304 sec.\n",
      "Iteration 4400 || Loss: 24.1612 || 10iter: 3.5823 sec.\n",
      "Iteration 4410 || Loss: 18.3602 || 10iter: 3.7269 sec.\n",
      "Iteration 4420 || Loss: 23.6309 || 10iter: 3.7013 sec.\n",
      "Iteration 4430 || Loss: 21.1708 || 10iter: 3.7291 sec.\n",
      "Iteration 4440 || Loss: 30.3476 || 10iter: 3.7038 sec.\n",
      "Iteration 4450 || Loss: 18.0624 || 10iter: 3.7328 sec.\n",
      "Iteration 4460 || Loss: 22.7077 || 10iter: 3.3298 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:4346.7387 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8686 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 23/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4470 || Loss: 21.5679 || 10iter: 4.7055 sec.\n",
      "Iteration 4480 || Loss: 23.2006 || 10iter: 4.0481 sec.\n",
      "Iteration 4490 || Loss: 21.1214 || 10iter: 3.6850 sec.\n",
      "Iteration 4500 || Loss: 16.8295 || 10iter: 3.6862 sec.\n",
      "Iteration 4510 || Loss: 24.3343 || 10iter: 3.7106 sec.\n",
      "Iteration 4520 || Loss: 24.5337 || 10iter: 3.7150 sec.\n",
      "Iteration 4530 || Loss: 19.8049 || 10iter: 3.6647 sec.\n",
      "Iteration 4540 || Loss: 16.3964 || 10iter: 3.7282 sec.\n",
      "Iteration 4550 || Loss: 17.8533 || 10iter: 3.7034 sec.\n",
      "Iteration 4560 || Loss: 21.6353 || 10iter: 3.6972 sec.\n",
      "Iteration 4570 || Loss: 17.1999 || 10iter: 3.7639 sec.\n",
      "Iteration 4580 || Loss: 18.9646 || 10iter: 3.6330 sec.\n",
      "Iteration 4590 || Loss: 15.9870 || 10iter: 3.6615 sec.\n",
      "Iteration 4600 || Loss: 15.9021 || 10iter: 3.7055 sec.\n",
      "Iteration 4610 || Loss: 18.9986 || 10iter: 3.6304 sec.\n",
      "Iteration 4620 || Loss: 19.7841 || 10iter: 3.6472 sec.\n",
      "Iteration 4630 || Loss: 19.8577 || 10iter: 3.7426 sec.\n",
      "Iteration 4640 || Loss: 33.1686 || 10iter: 3.6418 sec.\n",
      "Iteration 4650 || Loss: 19.6882 || 10iter: 3.6630 sec.\n",
      "Iteration 4660 || Loss: 28.5063 || 10iter: 3.4443 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:4345.0426 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6955 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 24/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4670 || Loss: 19.8318 || 10iter: 2.6863 sec.\n",
      "Iteration 4680 || Loss: 18.9939 || 10iter: 4.6076 sec.\n",
      "Iteration 4690 || Loss: 20.1013 || 10iter: 3.7199 sec.\n",
      "Iteration 4700 || Loss: 17.3816 || 10iter: 3.6699 sec.\n",
      "Iteration 4710 || Loss: 19.3484 || 10iter: 3.7116 sec.\n",
      "Iteration 4720 || Loss: 25.6062 || 10iter: 3.7225 sec.\n",
      "Iteration 4730 || Loss: 16.3640 || 10iter: 3.6757 sec.\n",
      "Iteration 4740 || Loss: 23.4832 || 10iter: 3.6890 sec.\n",
      "Iteration 4750 || Loss: 16.8038 || 10iter: 3.7621 sec.\n",
      "Iteration 4760 || Loss: 28.6550 || 10iter: 3.7108 sec.\n",
      "Iteration 4770 || Loss: 20.7451 || 10iter: 3.6919 sec.\n",
      "Iteration 4780 || Loss: 14.4852 || 10iter: 3.6913 sec.\n",
      "Iteration 4790 || Loss: 24.5911 || 10iter: 3.6562 sec.\n",
      "Iteration 4800 || Loss: 20.3698 || 10iter: 3.7458 sec.\n",
      "Iteration 4810 || Loss: 18.9023 || 10iter: 3.6232 sec.\n",
      "Iteration 4820 || Loss: 21.9265 || 10iter: 3.7411 sec.\n",
      "Iteration 4830 || Loss: 17.3839 || 10iter: 3.6813 sec.\n",
      "Iteration 4840 || Loss: 23.2021 || 10iter: 3.6968 sec.\n",
      "Iteration 4850 || Loss: 20.0120 || 10iter: 3.7326 sec.\n",
      "Iteration 4860 || Loss: 22.5390 || 10iter: 3.5552 sec.\n",
      "Iteration 4870 || Loss: 22.3745 || 10iter: 3.3627 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:4207.3933 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.5991 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 25/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4880 || Loss: 23.0605 || 10iter: 6.6087 sec.\n",
      "Iteration 4890 || Loss: 13.2878 || 10iter: 3.7310 sec.\n",
      "Iteration 4900 || Loss: 18.8267 || 10iter: 3.6796 sec.\n",
      "Iteration 4910 || Loss: 24.4791 || 10iter: 3.7068 sec.\n",
      "Iteration 4920 || Loss: 20.7153 || 10iter: 3.7528 sec.\n",
      "Iteration 4930 || Loss: 18.5260 || 10iter: 3.9648 sec.\n",
      "Iteration 4940 || Loss: 23.7974 || 10iter: 3.7324 sec.\n",
      "Iteration 4950 || Loss: 17.0222 || 10iter: 3.6769 sec.\n",
      "Iteration 4960 || Loss: 16.1535 || 10iter: 3.6639 sec.\n",
      "Iteration 4970 || Loss: 22.1264 || 10iter: 3.7521 sec.\n",
      "Iteration 4980 || Loss: 23.2383 || 10iter: 3.6880 sec.\n",
      "Iteration 4990 || Loss: 20.2897 || 10iter: 3.6507 sec.\n",
      "Iteration 5000 || Loss: 27.1838 || 10iter: 3.7045 sec.\n",
      "Iteration 5010 || Loss: 18.1371 || 10iter: 3.6802 sec.\n",
      "Iteration 5020 || Loss: 25.7025 || 10iter: 3.5755 sec.\n",
      "Iteration 5030 || Loss: 15.0905 || 10iter: 3.7049 sec.\n",
      "Iteration 5040 || Loss: 16.9144 || 10iter: 3.7660 sec.\n",
      "Iteration 5050 || Loss: 18.4400 || 10iter: 3.7113 sec.\n",
      "Iteration 5060 || Loss: 20.2296 || 10iter: 3.6788 sec.\n",
      "Iteration 5070 || Loss: 22.2041 || 10iter: 3.3774 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:4110.0687 ||Epoch_VAL_Loss:1179.4455\n",
      "timer:  89.8454 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 26/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5080 || Loss: 18.9435 || 10iter: 4.7882 sec.\n",
      "Iteration 5090 || Loss: 18.8011 || 10iter: 4.1325 sec.\n",
      "Iteration 5100 || Loss: 17.4007 || 10iter: 3.6301 sec.\n",
      "Iteration 5110 || Loss: 15.8072 || 10iter: 3.7388 sec.\n",
      "Iteration 5120 || Loss: 16.3552 || 10iter: 3.7331 sec.\n",
      "Iteration 5130 || Loss: 27.0822 || 10iter: 3.7469 sec.\n",
      "Iteration 5140 || Loss: 17.8159 || 10iter: 3.6375 sec.\n",
      "Iteration 5150 || Loss: 18.5416 || 10iter: 3.6748 sec.\n",
      "Iteration 5160 || Loss: 15.5977 || 10iter: 3.6532 sec.\n",
      "Iteration 5170 || Loss: 21.4516 || 10iter: 3.7751 sec.\n",
      "Iteration 5180 || Loss: 25.6516 || 10iter: 3.7062 sec.\n",
      "Iteration 5190 || Loss: 24.7103 || 10iter: 3.6293 sec.\n",
      "Iteration 5200 || Loss: 24.4577 || 10iter: 3.6833 sec.\n",
      "Iteration 5210 || Loss: 16.6778 || 10iter: 3.6916 sec.\n",
      "Iteration 5220 || Loss: 17.7946 || 10iter: 3.7055 sec.\n",
      "Iteration 5230 || Loss: 20.1701 || 10iter: 3.6869 sec.\n",
      "Iteration 5240 || Loss: 20.9878 || 10iter: 3.6875 sec.\n",
      "Iteration 5250 || Loss: 26.3523 || 10iter: 3.7110 sec.\n",
      "Iteration 5260 || Loss: 17.8264 || 10iter: 3.7638 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5270 || Loss: 25.1058 || 10iter: 3.4244 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:4096.9632 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6829 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 27/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5280 || Loss: 20.7962 || 10iter: 3.7265 sec.\n",
      "Iteration 5290 || Loss: 18.6235 || 10iter: 4.3867 sec.\n",
      "Iteration 5300 || Loss: 17.9558 || 10iter: 3.6300 sec.\n",
      "Iteration 5310 || Loss: 15.5754 || 10iter: 3.7107 sec.\n",
      "Iteration 5320 || Loss: 16.9064 || 10iter: 3.6808 sec.\n",
      "Iteration 5330 || Loss: 15.4374 || 10iter: 3.6695 sec.\n",
      "Iteration 5340 || Loss: 14.8704 || 10iter: 3.6597 sec.\n",
      "Iteration 5350 || Loss: 18.9577 || 10iter: 3.6573 sec.\n",
      "Iteration 5360 || Loss: 13.3798 || 10iter: 3.6443 sec.\n",
      "Iteration 5370 || Loss: 16.7852 || 10iter: 3.8316 sec.\n",
      "Iteration 5380 || Loss: 21.1659 || 10iter: 3.8952 sec.\n",
      "Iteration 5390 || Loss: 18.2064 || 10iter: 3.6599 sec.\n",
      "Iteration 5400 || Loss: 25.2633 || 10iter: 3.7423 sec.\n",
      "Iteration 5410 || Loss: 20.3947 || 10iter: 3.7587 sec.\n",
      "Iteration 5420 || Loss: 21.2316 || 10iter: 3.6969 sec.\n",
      "Iteration 5430 || Loss: 16.6711 || 10iter: 3.7335 sec.\n",
      "Iteration 5440 || Loss: 14.5690 || 10iter: 3.6869 sec.\n",
      "Iteration 5450 || Loss: 22.2555 || 10iter: 3.6798 sec.\n",
      "Iteration 5460 || Loss: 22.3300 || 10iter: 3.7034 sec.\n",
      "Iteration 5470 || Loss: 25.0085 || 10iter: 3.5743 sec.\n",
      "Iteration 5480 || Loss: 20.1797 || 10iter: 3.3843 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:3965.0622 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.2474 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 28/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5490 || Loss: 18.7977 || 10iter: 6.8521 sec.\n",
      "Iteration 5500 || Loss: 19.9212 || 10iter: 3.6963 sec.\n",
      "Iteration 5510 || Loss: 18.8407 || 10iter: 3.7186 sec.\n",
      "Iteration 5520 || Loss: 21.4516 || 10iter: 3.7415 sec.\n",
      "Iteration 5530 || Loss: 24.2744 || 10iter: 3.7173 sec.\n",
      "Iteration 5540 || Loss: 18.6611 || 10iter: 3.6809 sec.\n",
      "Iteration 5550 || Loss: 25.1444 || 10iter: 3.7010 sec.\n",
      "Iteration 5560 || Loss: 20.0912 || 10iter: 3.6029 sec.\n",
      "Iteration 5570 || Loss: 16.1575 || 10iter: 3.5971 sec.\n",
      "Iteration 5580 || Loss: 21.2545 || 10iter: 3.7122 sec.\n",
      "Iteration 5590 || Loss: 26.6448 || 10iter: 3.6540 sec.\n",
      "Iteration 5600 || Loss: 14.4785 || 10iter: 3.6604 sec.\n",
      "Iteration 5610 || Loss: 23.9390 || 10iter: 3.5938 sec.\n",
      "Iteration 5620 || Loss: 18.0541 || 10iter: 3.6492 sec.\n",
      "Iteration 5630 || Loss: 18.3312 || 10iter: 3.6950 sec.\n",
      "Iteration 5640 || Loss: 18.7216 || 10iter: 3.7412 sec.\n",
      "Iteration 5650 || Loss: 17.0161 || 10iter: 3.6657 sec.\n",
      "Iteration 5660 || Loss: 19.6019 || 10iter: 3.6533 sec.\n",
      "Iteration 5670 || Loss: 19.2619 || 10iter: 3.6924 sec.\n",
      "Iteration 5680 || Loss: 21.1572 || 10iter: 3.3712 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:3980.2786 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.5396 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 29/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5690 || Loss: 17.0678 || 10iter: 5.2529 sec.\n",
      "Iteration 5700 || Loss: 25.8478 || 10iter: 3.8452 sec.\n",
      "Iteration 5710 || Loss: 15.1815 || 10iter: 3.6350 sec.\n",
      "Iteration 5720 || Loss: 16.3445 || 10iter: 3.6114 sec.\n",
      "Iteration 5730 || Loss: 13.2086 || 10iter: 3.6554 sec.\n",
      "Iteration 5740 || Loss: 20.7138 || 10iter: 3.7155 sec.\n",
      "Iteration 5750 || Loss: 28.4028 || 10iter: 3.6184 sec.\n",
      "Iteration 5760 || Loss: 17.6037 || 10iter: 3.7317 sec.\n",
      "Iteration 5770 || Loss: 25.9731 || 10iter: 3.6645 sec.\n",
      "Iteration 5780 || Loss: 20.5611 || 10iter: 3.7323 sec.\n",
      "Iteration 5790 || Loss: 17.0317 || 10iter: 3.7280 sec.\n",
      "Iteration 5800 || Loss: 16.7783 || 10iter: 3.6646 sec.\n",
      "Iteration 5810 || Loss: 19.0967 || 10iter: 3.6429 sec.\n",
      "Iteration 5820 || Loss: 14.2216 || 10iter: 3.6862 sec.\n",
      "Iteration 5830 || Loss: 29.5443 || 10iter: 3.6508 sec.\n",
      "Iteration 5840 || Loss: 18.5548 || 10iter: 3.7057 sec.\n",
      "Iteration 5850 || Loss: 12.7583 || 10iter: 4.0044 sec.\n",
      "Iteration 5860 || Loss: 24.4496 || 10iter: 3.7435 sec.\n",
      "Iteration 5870 || Loss: 19.0880 || 10iter: 3.6869 sec.\n",
      "Iteration 5880 || Loss: 18.2631 || 10iter: 3.3952 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:3998.7312 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.5228 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 30/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5890 || Loss: 14.0813 || 10iter: 4.0151 sec.\n",
      "Iteration 5900 || Loss: 25.5449 || 10iter: 4.3750 sec.\n",
      "Iteration 5910 || Loss: 19.5226 || 10iter: 3.6821 sec.\n",
      "Iteration 5920 || Loss: 17.8804 || 10iter: 3.7334 sec.\n",
      "Iteration 5930 || Loss: 21.9463 || 10iter: 3.6488 sec.\n",
      "Iteration 5940 || Loss: 21.8475 || 10iter: 3.7313 sec.\n",
      "Iteration 5950 || Loss: 17.3655 || 10iter: 3.6334 sec.\n",
      "Iteration 5960 || Loss: 16.0165 || 10iter: 3.6973 sec.\n",
      "Iteration 5970 || Loss: 19.6323 || 10iter: 3.7989 sec.\n",
      "Iteration 5980 || Loss: 19.8950 || 10iter: 3.8128 sec.\n",
      "Iteration 5990 || Loss: 15.7151 || 10iter: 3.6790 sec.\n",
      "Iteration 6000 || Loss: 17.2214 || 10iter: 3.7089 sec.\n",
      "Iteration 6010 || Loss: 19.3791 || 10iter: 3.6221 sec.\n",
      "Iteration 6020 || Loss: 16.1198 || 10iter: 3.7851 sec.\n",
      "Iteration 6030 || Loss: 17.9464 || 10iter: 3.6638 sec.\n",
      "Iteration 6040 || Loss: 20.7432 || 10iter: 3.7721 sec.\n",
      "Iteration 6050 || Loss: 18.7089 || 10iter: 3.7664 sec.\n",
      "Iteration 6060 || Loss: 18.8583 || 10iter: 3.7510 sec.\n",
      "Iteration 6070 || Loss: 16.6718 || 10iter: 3.6962 sec.\n",
      "Iteration 6080 || Loss: 14.4777 || 10iter: 3.4893 sec.\n",
      "Iteration 6090 || Loss: 26.4776 || 10iter: 3.0939 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:3863.9074 ||Epoch_VAL_Loss:1145.9636\n",
      "timer:  89.6522 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 31/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6100 || Loss: 13.1569 || 10iter: 7.1249 sec.\n",
      "Iteration 6110 || Loss: 19.8142 || 10iter: 3.6144 sec.\n",
      "Iteration 6120 || Loss: 19.0702 || 10iter: 3.7520 sec.\n",
      "Iteration 6130 || Loss: 17.4154 || 10iter: 3.7130 sec.\n",
      "Iteration 6140 || Loss: 16.2929 || 10iter: 3.7327 sec.\n",
      "Iteration 6150 || Loss: 18.9134 || 10iter: 3.7334 sec.\n",
      "Iteration 6160 || Loss: 20.5193 || 10iter: 3.6050 sec.\n",
      "Iteration 6170 || Loss: 17.6826 || 10iter: 3.6686 sec.\n",
      "Iteration 6180 || Loss: 15.1105 || 10iter: 3.7399 sec.\n",
      "Iteration 6190 || Loss: 18.5900 || 10iter: 3.7180 sec.\n",
      "Iteration 6200 || Loss: 16.2059 || 10iter: 3.6852 sec.\n",
      "Iteration 6210 || Loss: 28.0675 || 10iter: 3.6754 sec.\n",
      "Iteration 6220 || Loss: 23.4197 || 10iter: 3.6826 sec.\n",
      "Iteration 6230 || Loss: 21.3902 || 10iter: 3.5987 sec.\n",
      "Iteration 6240 || Loss: 16.8830 || 10iter: 3.6654 sec.\n",
      "Iteration 6250 || Loss: 17.9935 || 10iter: 3.7113 sec.\n",
      "Iteration 6260 || Loss: 20.4875 || 10iter: 3.6809 sec.\n",
      "Iteration 6270 || Loss: 18.8571 || 10iter: 3.6662 sec.\n",
      "Iteration 6280 || Loss: 28.9315 || 10iter: 3.5765 sec.\n",
      "Iteration 6290 || Loss: 19.1318 || 10iter: 3.3646 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:3852.3779 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.5118 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 32/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6300 || Loss: 18.0947 || 10iter: 6.0320 sec.\n",
      "Iteration 6310 || Loss: 19.9334 || 10iter: 3.8864 sec.\n",
      "Iteration 6320 || Loss: 18.9378 || 10iter: 3.6917 sec.\n",
      "Iteration 6330 || Loss: 14.9710 || 10iter: 3.8144 sec.\n",
      "Iteration 6340 || Loss: 13.6132 || 10iter: 3.6414 sec.\n",
      "Iteration 6350 || Loss: 23.3319 || 10iter: 3.7373 sec.\n",
      "Iteration 6360 || Loss: 17.3359 || 10iter: 3.6393 sec.\n",
      "Iteration 6370 || Loss: 19.0988 || 10iter: 3.7258 sec.\n",
      "Iteration 6380 || Loss: 13.9592 || 10iter: 3.6650 sec.\n",
      "Iteration 6390 || Loss: 14.3039 || 10iter: 3.6748 sec.\n",
      "Iteration 6400 || Loss: 17.9093 || 10iter: 3.6936 sec.\n",
      "Iteration 6410 || Loss: 18.0616 || 10iter: 3.6203 sec.\n",
      "Iteration 6420 || Loss: 17.4638 || 10iter: 3.6927 sec.\n",
      "Iteration 6430 || Loss: 15.0240 || 10iter: 3.7273 sec.\n",
      "Iteration 6440 || Loss: 17.8731 || 10iter: 3.7013 sec.\n",
      "Iteration 6450 || Loss: 16.4709 || 10iter: 3.7204 sec.\n",
      "Iteration 6460 || Loss: 14.9806 || 10iter: 3.7797 sec.\n",
      "Iteration 6470 || Loss: 16.2855 || 10iter: 3.7418 sec.\n",
      "Iteration 6480 || Loss: 27.3290 || 10iter: 3.6904 sec.\n",
      "Iteration 6490 || Loss: 12.3088 || 10iter: 3.3443 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:3810.6539 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.0314 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 33/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6500 || Loss: 20.1389 || 10iter: 4.4736 sec.\n",
      "Iteration 6510 || Loss: 17.8616 || 10iter: 4.0485 sec.\n",
      "Iteration 6520 || Loss: 17.4001 || 10iter: 3.6996 sec.\n",
      "Iteration 6530 || Loss: 17.3875 || 10iter: 3.7223 sec.\n",
      "Iteration 6540 || Loss: 28.0324 || 10iter: 3.7300 sec.\n",
      "Iteration 6550 || Loss: 21.2210 || 10iter: 3.7056 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6560 || Loss: 16.8238 || 10iter: 3.7327 sec.\n",
      "Iteration 6570 || Loss: 16.9673 || 10iter: 3.7372 sec.\n",
      "Iteration 6580 || Loss: 14.3727 || 10iter: 3.6690 sec.\n",
      "Iteration 6590 || Loss: 19.3253 || 10iter: 3.7383 sec.\n",
      "Iteration 6600 || Loss: 20.7637 || 10iter: 3.6662 sec.\n",
      "Iteration 6610 || Loss: 20.9996 || 10iter: 3.7265 sec.\n",
      "Iteration 6620 || Loss: 29.5664 || 10iter: 3.6675 sec.\n",
      "Iteration 6630 || Loss: 16.4456 || 10iter: 3.6129 sec.\n",
      "Iteration 6640 || Loss: 11.2212 || 10iter: 3.6585 sec.\n",
      "Iteration 6650 || Loss: 14.7575 || 10iter: 3.6366 sec.\n",
      "Iteration 6660 || Loss: 15.4217 || 10iter: 3.7411 sec.\n",
      "Iteration 6670 || Loss: 17.8295 || 10iter: 3.7646 sec.\n",
      "Iteration 6680 || Loss: 20.5394 || 10iter: 3.7568 sec.\n",
      "Iteration 6690 || Loss: 18.8835 || 10iter: 3.4688 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:3821.9962 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.7964 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 34/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6700 || Loss: 20.6415 || 10iter: 2.7417 sec.\n",
      "Iteration 6710 || Loss: 15.3449 || 10iter: 4.8381 sec.\n",
      "Iteration 6720 || Loss: 20.8461 || 10iter: 3.7224 sec.\n",
      "Iteration 6730 || Loss: 19.0471 || 10iter: 3.7458 sec.\n",
      "Iteration 6740 || Loss: 14.1711 || 10iter: 3.6519 sec.\n",
      "Iteration 6750 || Loss: 19.6701 || 10iter: 3.9882 sec.\n",
      "Iteration 6760 || Loss: 18.8125 || 10iter: 3.7235 sec.\n",
      "Iteration 6770 || Loss: 16.4717 || 10iter: 3.6902 sec.\n",
      "Iteration 6780 || Loss: 19.5752 || 10iter: 3.7688 sec.\n",
      "Iteration 6790 || Loss: 17.7259 || 10iter: 3.6348 sec.\n",
      "Iteration 6800 || Loss: 19.3680 || 10iter: 3.7328 sec.\n",
      "Iteration 6810 || Loss: 23.9954 || 10iter: 3.6468 sec.\n",
      "Iteration 6820 || Loss: 18.8622 || 10iter: 3.6589 sec.\n",
      "Iteration 6830 || Loss: 16.7812 || 10iter: 3.6794 sec.\n",
      "Iteration 6840 || Loss: 23.2152 || 10iter: 3.7425 sec.\n",
      "Iteration 6850 || Loss: 18.9949 || 10iter: 3.6286 sec.\n",
      "Iteration 6860 || Loss: 16.1160 || 10iter: 3.7370 sec.\n",
      "Iteration 6870 || Loss: 17.5815 || 10iter: 3.7268 sec.\n",
      "Iteration 6880 || Loss: 17.7761 || 10iter: 3.7916 sec.\n",
      "Iteration 6890 || Loss: 24.3763 || 10iter: 3.6284 sec.\n",
      "Iteration 6900 || Loss: 17.4450 || 10iter: 3.3510 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:3686.2647 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.2926 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 35/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6910 || Loss: 17.4438 || 10iter: 6.3867 sec.\n",
      "Iteration 6920 || Loss: 15.4970 || 10iter: 3.6591 sec.\n",
      "Iteration 6930 || Loss: 11.6926 || 10iter: 3.6962 sec.\n",
      "Iteration 6940 || Loss: 17.0708 || 10iter: 3.6962 sec.\n",
      "Iteration 6950 || Loss: 21.9043 || 10iter: 3.7624 sec.\n",
      "Iteration 6960 || Loss: 17.8525 || 10iter: 3.7288 sec.\n",
      "Iteration 6970 || Loss: 21.1594 || 10iter: 3.6682 sec.\n",
      "Iteration 6980 || Loss: 16.7003 || 10iter: 3.7101 sec.\n",
      "Iteration 6990 || Loss: 24.3764 || 10iter: 3.7256 sec.\n",
      "Iteration 7000 || Loss: 15.6597 || 10iter: 3.7416 sec.\n",
      "Iteration 7010 || Loss: 16.1165 || 10iter: 3.6671 sec.\n",
      "Iteration 7020 || Loss: 15.4406 || 10iter: 3.7225 sec.\n",
      "Iteration 7030 || Loss: 15.0787 || 10iter: 3.6479 sec.\n",
      "Iteration 7040 || Loss: 18.2031 || 10iter: 3.7233 sec.\n",
      "Iteration 7050 || Loss: 21.5940 || 10iter: 3.7758 sec.\n",
      "Iteration 7060 || Loss: 18.6649 || 10iter: 3.7115 sec.\n",
      "Iteration 7070 || Loss: 14.3583 || 10iter: 3.7229 sec.\n",
      "Iteration 7080 || Loss: 14.8623 || 10iter: 3.6599 sec.\n",
      "Iteration 7090 || Loss: 22.7746 || 10iter: 3.7600 sec.\n",
      "Iteration 7100 || Loss: 23.9102 || 10iter: 3.3396 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:3795.1830 ||Epoch_VAL_Loss:1134.5023\n",
      "timer:  89.4398 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 36/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7110 || Loss: 21.7079 || 10iter: 4.6716 sec.\n",
      "Iteration 7120 || Loss: 15.8763 || 10iter: 4.0927 sec.\n",
      "Iteration 7130 || Loss: 19.3538 || 10iter: 3.7212 sec.\n",
      "Iteration 7140 || Loss: 18.0890 || 10iter: 3.7658 sec.\n",
      "Iteration 7150 || Loss: 20.0443 || 10iter: 3.7425 sec.\n",
      "Iteration 7160 || Loss: 22.9197 || 10iter: 3.7280 sec.\n",
      "Iteration 7170 || Loss: 16.9908 || 10iter: 3.6677 sec.\n",
      "Iteration 7180 || Loss: 18.4560 || 10iter: 3.6713 sec.\n",
      "Iteration 7190 || Loss: 14.3520 || 10iter: 3.9141 sec.\n",
      "Iteration 7200 || Loss: 16.1480 || 10iter: 3.7228 sec.\n",
      "Iteration 7210 || Loss: 14.6138 || 10iter: 3.7154 sec.\n",
      "Iteration 7220 || Loss: 20.1033 || 10iter: 3.7452 sec.\n",
      "Iteration 7230 || Loss: 15.4513 || 10iter: 3.7028 sec.\n",
      "Iteration 7240 || Loss: 15.5554 || 10iter: 3.7200 sec.\n",
      "Iteration 7250 || Loss: 16.5246 || 10iter: 3.6910 sec.\n",
      "Iteration 7260 || Loss: 21.9086 || 10iter: 3.6948 sec.\n",
      "Iteration 7270 || Loss: 15.8404 || 10iter: 3.7248 sec.\n",
      "Iteration 7280 || Loss: 21.5219 || 10iter: 3.6800 sec.\n",
      "Iteration 7290 || Loss: 17.7139 || 10iter: 3.6924 sec.\n",
      "Iteration 7300 || Loss: 16.2538 || 10iter: 3.4339 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:3706.7884 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9585 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 37/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7310 || Loss: 17.1007 || 10iter: 3.7657 sec.\n",
      "Iteration 7330 || Loss: 18.1559 || 10iter: 3.6473 sec.\n",
      "Iteration 7340 || Loss: 16.8884 || 10iter: 3.7302 sec.\n",
      "Iteration 7350 || Loss: 13.8303 || 10iter: 3.7262 sec.\n",
      "Iteration 7360 || Loss: 12.3392 || 10iter: 3.7262 sec.\n",
      "Iteration 7370 || Loss: 17.7759 || 10iter: 3.6447 sec.\n",
      "Iteration 7380 || Loss: 17.6164 || 10iter: 3.7170 sec.\n",
      "Iteration 7390 || Loss: 13.7857 || 10iter: 3.6262 sec.\n",
      "Iteration 7400 || Loss: 22.4560 || 10iter: 3.7026 sec.\n",
      "Iteration 7410 || Loss: 17.9682 || 10iter: 3.7120 sec.\n",
      "Iteration 7420 || Loss: 17.7277 || 10iter: 3.6549 sec.\n",
      "Iteration 7430 || Loss: 17.7997 || 10iter: 3.7427 sec.\n",
      "Iteration 7440 || Loss: 22.3097 || 10iter: 3.7030 sec.\n",
      "Iteration 7450 || Loss: 15.7334 || 10iter: 3.7077 sec.\n",
      "Iteration 7460 || Loss: 24.7838 || 10iter: 3.6494 sec.\n",
      "Iteration 7470 || Loss: 20.6279 || 10iter: 3.7508 sec.\n",
      "Iteration 7480 || Loss: 18.4462 || 10iter: 3.7797 sec.\n",
      "Iteration 7490 || Loss: 19.6423 || 10iter: 3.7012 sec.\n",
      "Iteration 7500 || Loss: 17.4314 || 10iter: 3.5201 sec.\n",
      "Iteration 7510 || Loss: 17.8302 || 10iter: 3.3418 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:3646.9857 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8697 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 38/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7520 || Loss: 16.0618 || 10iter: 6.6001 sec.\n",
      "Iteration 7530 || Loss: 11.7477 || 10iter: 3.6539 sec.\n",
      "Iteration 7540 || Loss: 21.4683 || 10iter: 3.7122 sec.\n",
      "Iteration 7550 || Loss: 14.9571 || 10iter: 3.6582 sec.\n",
      "Iteration 7560 || Loss: 16.8548 || 10iter: 3.7119 sec.\n",
      "Iteration 7570 || Loss: 19.5863 || 10iter: 3.6386 sec.\n",
      "Iteration 7580 || Loss: 17.3563 || 10iter: 3.7104 sec.\n",
      "Iteration 7590 || Loss: 14.5164 || 10iter: 3.7398 sec.\n",
      "Iteration 7600 || Loss: 20.3784 || 10iter: 3.6286 sec.\n",
      "Iteration 7610 || Loss: 19.2532 || 10iter: 3.7357 sec.\n",
      "Iteration 7620 || Loss: 18.1144 || 10iter: 3.6333 sec.\n",
      "Iteration 7630 || Loss: 17.8785 || 10iter: 3.7014 sec.\n",
      "Iteration 7640 || Loss: 21.9115 || 10iter: 3.6939 sec.\n",
      "Iteration 7650 || Loss: 16.7221 || 10iter: 3.6961 sec.\n",
      "Iteration 7660 || Loss: 14.8025 || 10iter: 3.8298 sec.\n",
      "Iteration 7670 || Loss: 16.4326 || 10iter: 3.7913 sec.\n",
      "Iteration 7680 || Loss: 22.5817 || 10iter: 3.7262 sec.\n",
      "Iteration 7690 || Loss: 18.2217 || 10iter: 3.6890 sec.\n",
      "Iteration 7700 || Loss: 18.7011 || 10iter: 3.6191 sec.\n",
      "Iteration 7710 || Loss: 18.8721 || 10iter: 3.3682 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:3588.3375 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6758 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 39/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7720 || Loss: 15.3279 || 10iter: 5.5186 sec.\n",
      "Iteration 7730 || Loss: 15.3513 || 10iter: 3.8186 sec.\n",
      "Iteration 7740 || Loss: 13.3227 || 10iter: 3.6302 sec.\n",
      "Iteration 7750 || Loss: 18.3292 || 10iter: 3.7424 sec.\n",
      "Iteration 7760 || Loss: 18.2226 || 10iter: 3.7167 sec.\n",
      "Iteration 7770 || Loss: 15.2083 || 10iter: 3.7106 sec.\n",
      "Iteration 7780 || Loss: 17.3652 || 10iter: 3.6643 sec.\n",
      "Iteration 7790 || Loss: 16.5233 || 10iter: 3.6412 sec.\n",
      "Iteration 7800 || Loss: 21.0755 || 10iter: 3.6383 sec.\n",
      "Iteration 7810 || Loss: 19.4844 || 10iter: 3.7597 sec.\n",
      "Iteration 7820 || Loss: 19.2238 || 10iter: 3.6757 sec.\n",
      "Iteration 7830 || Loss: 19.5475 || 10iter: 3.6450 sec.\n",
      "Iteration 7840 || Loss: 18.5279 || 10iter: 3.6831 sec.\n",
      "Iteration 7850 || Loss: 16.9463 || 10iter: 3.7992 sec.\n",
      "Iteration 7860 || Loss: 16.6702 || 10iter: 3.6590 sec.\n",
      "Iteration 7870 || Loss: 13.4489 || 10iter: 3.7321 sec.\n",
      "Iteration 7880 || Loss: 16.0394 || 10iter: 3.6868 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7890 || Loss: 10.2626 || 10iter: 3.7720 sec.\n",
      "Iteration 7900 || Loss: 20.4448 || 10iter: 3.6607 sec.\n",
      "Iteration 7910 || Loss: 18.5823 || 10iter: 3.4080 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:3547.8779 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.7022 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 40/200\n",
      "-------------\n",
      "train\n",
      "Iteration 7920 || Loss: 14.6962 || 10iter: 3.7900 sec.\n",
      "Iteration 7930 || Loss: 16.0847 || 10iter: 4.3577 sec.\n",
      "Iteration 7940 || Loss: 18.1994 || 10iter: 3.6313 sec.\n",
      "Iteration 7950 || Loss: 13.2733 || 10iter: 3.6488 sec.\n",
      "Iteration 7960 || Loss: 20.0387 || 10iter: 3.7032 sec.\n",
      "Iteration 7970 || Loss: 17.3104 || 10iter: 3.7290 sec.\n",
      "Iteration 7980 || Loss: 20.9605 || 10iter: 3.6292 sec.\n",
      "Iteration 7990 || Loss: 26.6363 || 10iter: 3.6916 sec.\n",
      "Iteration 8000 || Loss: 22.2831 || 10iter: 3.6844 sec.\n",
      "Iteration 8010 || Loss: 19.4677 || 10iter: 3.6767 sec.\n",
      "Iteration 8020 || Loss: 17.8550 || 10iter: 3.7372 sec.\n",
      "Iteration 8030 || Loss: 16.6206 || 10iter: 3.6819 sec.\n",
      "Iteration 8040 || Loss: 18.4863 || 10iter: 3.7081 sec.\n",
      "Iteration 8050 || Loss: 15.2069 || 10iter: 3.7541 sec.\n",
      "Iteration 8060 || Loss: 22.5332 || 10iter: 3.7129 sec.\n",
      "Iteration 8070 || Loss: 15.4524 || 10iter: 3.6526 sec.\n",
      "Iteration 8080 || Loss: 18.4718 || 10iter: 3.7554 sec.\n",
      "Iteration 8090 || Loss: 14.6538 || 10iter: 3.6602 sec.\n",
      "Iteration 8100 || Loss: 16.8831 || 10iter: 3.6695 sec.\n",
      "Iteration 8110 || Loss: 15.3687 || 10iter: 3.4960 sec.\n",
      "Iteration 8120 || Loss: 12.6618 || 10iter: 3.0867 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:3499.1149 ||Epoch_VAL_Loss:1116.4750\n",
      "timer:  89.6251 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 41/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8130 || Loss: 17.7501 || 10iter: 7.1475 sec.\n",
      "Iteration 8140 || Loss: 14.3815 || 10iter: 3.5841 sec.\n",
      "Iteration 8150 || Loss: 15.2651 || 10iter: 3.7663 sec.\n",
      "Iteration 8160 || Loss: 17.7869 || 10iter: 3.5867 sec.\n",
      "Iteration 8170 || Loss: 22.0239 || 10iter: 3.7630 sec.\n",
      "Iteration 8180 || Loss: 24.2093 || 10iter: 3.7732 sec.\n",
      "Iteration 8190 || Loss: 19.0583 || 10iter: 3.5961 sec.\n",
      "Iteration 8200 || Loss: 18.0920 || 10iter: 3.7367 sec.\n",
      "Iteration 8210 || Loss: 20.9327 || 10iter: 3.6318 sec.\n",
      "Iteration 8220 || Loss: 17.3042 || 10iter: 3.7649 sec.\n",
      "Iteration 8230 || Loss: 17.1157 || 10iter: 3.5957 sec.\n",
      "Iteration 8240 || Loss: 15.8441 || 10iter: 3.6643 sec.\n",
      "Iteration 8250 || Loss: 14.4126 || 10iter: 3.7065 sec.\n",
      "Iteration 8260 || Loss: 16.4825 || 10iter: 3.7111 sec.\n",
      "Iteration 8270 || Loss: 11.8000 || 10iter: 3.6539 sec.\n",
      "Iteration 8280 || Loss: 20.0131 || 10iter: 3.6669 sec.\n",
      "Iteration 8290 || Loss: 21.4390 || 10iter: 3.7462 sec.\n",
      "Iteration 8300 || Loss: 14.2207 || 10iter: 3.6580 sec.\n",
      "Iteration 8310 || Loss: 16.8455 || 10iter: 3.5587 sec.\n",
      "Iteration 8320 || Loss: 19.8586 || 10iter: 3.3648 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:3533.7874 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.4803 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 42/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8330 || Loss: 19.3516 || 10iter: 6.0090 sec.\n",
      "Iteration 8340 || Loss: 25.4895 || 10iter: 3.7419 sec.\n",
      "Iteration 8350 || Loss: 16.4648 || 10iter: 3.7336 sec.\n",
      "Iteration 8360 || Loss: 25.3314 || 10iter: 3.8326 sec.\n",
      "Iteration 8370 || Loss: 15.6216 || 10iter: 3.7642 sec.\n",
      "Iteration 8380 || Loss: 20.8754 || 10iter: 3.7546 sec.\n",
      "Iteration 8390 || Loss: 16.1353 || 10iter: 3.6390 sec.\n",
      "Iteration 8400 || Loss: 21.4321 || 10iter: 3.6960 sec.\n",
      "Iteration 8410 || Loss: 15.3271 || 10iter: 3.6693 sec.\n",
      "Iteration 8420 || Loss: 16.4173 || 10iter: 3.7243 sec.\n",
      "Iteration 8430 || Loss: 16.3137 || 10iter: 3.7557 sec.\n",
      "Iteration 8440 || Loss: 14.2112 || 10iter: 3.6836 sec.\n",
      "Iteration 8450 || Loss: 19.2934 || 10iter: 3.6349 sec.\n",
      "Iteration 8460 || Loss: 12.2192 || 10iter: 3.6533 sec.\n",
      "Iteration 8470 || Loss: 16.8270 || 10iter: 3.6499 sec.\n",
      "Iteration 8480 || Loss: 12.5051 || 10iter: 3.6991 sec.\n",
      "Iteration 8490 || Loss: 20.5614 || 10iter: 3.7211 sec.\n",
      "Iteration 8500 || Loss: 22.5748 || 10iter: 3.7197 sec.\n",
      "Iteration 8510 || Loss: 20.6354 || 10iter: 3.7469 sec.\n",
      "Iteration 8520 || Loss: 16.0333 || 10iter: 3.3554 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:3615.6198 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9982 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 43/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8530 || Loss: 20.6214 || 10iter: 4.4449 sec.\n",
      "Iteration 8540 || Loss: 15.1710 || 10iter: 3.9629 sec.\n",
      "Iteration 8550 || Loss: 14.9636 || 10iter: 3.7079 sec.\n",
      "Iteration 8560 || Loss: 22.2094 || 10iter: 3.8515 sec.\n",
      "Iteration 8570 || Loss: 15.0463 || 10iter: 3.9444 sec.\n",
      "Iteration 8580 || Loss: 17.4295 || 10iter: 3.6711 sec.\n",
      "Iteration 8590 || Loss: 19.5823 || 10iter: 3.6818 sec.\n",
      "Iteration 8600 || Loss: 13.5345 || 10iter: 3.6699 sec.\n",
      "Iteration 8610 || Loss: 15.6055 || 10iter: 3.6851 sec.\n",
      "Iteration 8620 || Loss: 17.4622 || 10iter: 3.7637 sec.\n",
      "Iteration 8630 || Loss: 16.0846 || 10iter: 3.7958 sec.\n",
      "Iteration 8640 || Loss: 18.9823 || 10iter: 3.6112 sec.\n",
      "Iteration 8650 || Loss: 23.0186 || 10iter: 3.6636 sec.\n",
      "Iteration 8660 || Loss: 16.0439 || 10iter: 3.7374 sec.\n",
      "Iteration 8670 || Loss: 16.7031 || 10iter: 3.7171 sec.\n",
      "Iteration 8680 || Loss: 15.2601 || 10iter: 3.7452 sec.\n",
      "Iteration 8690 || Loss: 17.9676 || 10iter: 3.7567 sec.\n",
      "Iteration 8700 || Loss: 15.4575 || 10iter: 3.6996 sec.\n",
      "Iteration 8710 || Loss: 13.2594 || 10iter: 3.8040 sec.\n",
      "Iteration 8720 || Loss: 15.8014 || 10iter: 3.4449 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:3509.6353 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.1551 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 44/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8730 || Loss: 16.7468 || 10iter: 3.2270 sec.\n",
      "Iteration 8740 || Loss: 17.4436 || 10iter: 4.3795 sec.\n",
      "Iteration 8750 || Loss: 14.5129 || 10iter: 3.7016 sec.\n",
      "Iteration 8760 || Loss: 14.6576 || 10iter: 3.6428 sec.\n",
      "Iteration 8770 || Loss: 13.1119 || 10iter: 3.6476 sec.\n",
      "Iteration 8780 || Loss: 20.4074 || 10iter: 3.7454 sec.\n",
      "Iteration 8790 || Loss: 13.5159 || 10iter: 3.7356 sec.\n",
      "Iteration 8800 || Loss: 24.3036 || 10iter: 3.6604 sec.\n",
      "Iteration 8810 || Loss: 15.2255 || 10iter: 3.6647 sec.\n",
      "Iteration 8820 || Loss: 18.5202 || 10iter: 3.5987 sec.\n",
      "Iteration 8830 || Loss: 14.5639 || 10iter: 3.6778 sec.\n",
      "Iteration 8840 || Loss: 14.2293 || 10iter: 3.7130 sec.\n",
      "Iteration 8850 || Loss: 18.5041 || 10iter: 3.6309 sec.\n",
      "Iteration 8860 || Loss: 17.9483 || 10iter: 3.6601 sec.\n",
      "Iteration 8870 || Loss: 14.7407 || 10iter: 3.6885 sec.\n",
      "Iteration 8880 || Loss: 14.6566 || 10iter: 3.7221 sec.\n",
      "Iteration 8890 || Loss: 9.5392 || 10iter: 3.6863 sec.\n",
      "Iteration 8900 || Loss: 24.0755 || 10iter: 3.7143 sec.\n",
      "Iteration 8910 || Loss: 22.7925 || 10iter: 3.6925 sec.\n",
      "Iteration 8920 || Loss: 20.1654 || 10iter: 3.5593 sec.\n",
      "Iteration 8930 || Loss: 16.4104 || 10iter: 3.3666 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:3464.9303 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6006 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 45/200\n",
      "-------------\n",
      "train\n",
      "Iteration 8940 || Loss: 14.6133 || 10iter: 6.2388 sec.\n",
      "Iteration 8950 || Loss: 14.9397 || 10iter: 3.7571 sec.\n",
      "Iteration 8960 || Loss: 15.5844 || 10iter: 3.6700 sec.\n",
      "Iteration 8970 || Loss: 16.0084 || 10iter: 3.6685 sec.\n",
      "Iteration 8980 || Loss: 18.5277 || 10iter: 3.7244 sec.\n",
      "Iteration 8990 || Loss: 19.1851 || 10iter: 3.7492 sec.\n",
      "Iteration 9000 || Loss: 15.7781 || 10iter: 3.6289 sec.\n",
      "Iteration 9010 || Loss: 23.6415 || 10iter: 3.6680 sec.\n",
      "Iteration 9020 || Loss: 17.9943 || 10iter: 3.5908 sec.\n",
      "Iteration 9030 || Loss: 14.6599 || 10iter: 3.6769 sec.\n",
      "Iteration 9040 || Loss: 21.0897 || 10iter: 4.0091 sec.\n",
      "Iteration 9050 || Loss: 17.4123 || 10iter: 3.7213 sec.\n",
      "Iteration 9060 || Loss: 16.9129 || 10iter: 3.7067 sec.\n",
      "Iteration 9070 || Loss: 11.1637 || 10iter: 3.6350 sec.\n",
      "Iteration 9080 || Loss: 19.0602 || 10iter: 3.7232 sec.\n",
      "Iteration 9090 || Loss: 15.9608 || 10iter: 3.6622 sec.\n",
      "Iteration 9100 || Loss: 15.5186 || 10iter: 3.7191 sec.\n",
      "Iteration 9110 || Loss: 12.6406 || 10iter: 3.7096 sec.\n",
      "Iteration 9120 || Loss: 18.5079 || 10iter: 3.7192 sec.\n",
      "Iteration 9130 || Loss: 19.3369 || 10iter: 3.3367 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:3444.1975 ||Epoch_VAL_Loss:1103.0179\n",
      "timer:  89.5441 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 46/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9140 || Loss: 20.7158 || 10iter: 5.0012 sec.\n",
      "Iteration 9150 || Loss: 22.8923 || 10iter: 3.9831 sec.\n",
      "Iteration 9160 || Loss: 13.1174 || 10iter: 3.7584 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9170 || Loss: 12.2147 || 10iter: 3.6213 sec.\n",
      "Iteration 9180 || Loss: 24.2948 || 10iter: 3.7246 sec.\n",
      "Iteration 9190 || Loss: 14.9573 || 10iter: 3.7672 sec.\n",
      "Iteration 9200 || Loss: 17.8833 || 10iter: 3.6662 sec.\n",
      "Iteration 9210 || Loss: 17.9376 || 10iter: 3.6182 sec.\n",
      "Iteration 9220 || Loss: 18.9377 || 10iter: 3.7235 sec.\n",
      "Iteration 9230 || Loss: 17.3420 || 10iter: 3.6764 sec.\n",
      "Iteration 9240 || Loss: 12.1125 || 10iter: 3.7763 sec.\n",
      "Iteration 9250 || Loss: 15.5098 || 10iter: 3.7415 sec.\n",
      "Iteration 9260 || Loss: 14.1703 || 10iter: 3.7315 sec.\n",
      "Iteration 9270 || Loss: 24.3206 || 10iter: 3.6601 sec.\n",
      "Iteration 9280 || Loss: 13.8033 || 10iter: 3.7178 sec.\n",
      "Iteration 9290 || Loss: 20.5112 || 10iter: 3.7322 sec.\n",
      "Iteration 9300 || Loss: 15.5769 || 10iter: 3.7399 sec.\n",
      "Iteration 9310 || Loss: 12.4265 || 10iter: 3.7030 sec.\n",
      "Iteration 9320 || Loss: 17.3238 || 10iter: 3.7286 sec.\n",
      "Iteration 9330 || Loss: 20.0848 || 10iter: 3.4005 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:3475.7597 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9369 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 47/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9340 || Loss: 19.5985 || 10iter: 3.7334 sec.\n",
      "Iteration 9350 || Loss: 18.8199 || 10iter: 4.1961 sec.\n",
      "Iteration 9360 || Loss: 14.9440 || 10iter: 3.6920 sec.\n",
      "Iteration 9370 || Loss: 14.7015 || 10iter: 3.7916 sec.\n",
      "Iteration 9380 || Loss: 17.6118 || 10iter: 3.8094 sec.\n",
      "Iteration 9390 || Loss: 15.4755 || 10iter: 3.8130 sec.\n",
      "Iteration 9400 || Loss: 13.9503 || 10iter: 3.7353 sec.\n",
      "Iteration 9410 || Loss: 22.6861 || 10iter: 3.6741 sec.\n",
      "Iteration 9420 || Loss: 15.2720 || 10iter: 3.6956 sec.\n",
      "Iteration 9430 || Loss: 19.6253 || 10iter: 3.7035 sec.\n",
      "Iteration 9440 || Loss: 18.3958 || 10iter: 3.7409 sec.\n",
      "Iteration 9450 || Loss: 23.8513 || 10iter: 3.7474 sec.\n",
      "Iteration 9460 || Loss: 17.9334 || 10iter: 3.6631 sec.\n",
      "Iteration 9470 || Loss: 21.6347 || 10iter: 3.7283 sec.\n",
      "Iteration 9480 || Loss: 19.4555 || 10iter: 3.9686 sec.\n",
      "Iteration 9490 || Loss: 16.0622 || 10iter: 3.6856 sec.\n",
      "Iteration 9500 || Loss: 20.8050 || 10iter: 3.7513 sec.\n",
      "Iteration 9510 || Loss: 13.2208 || 10iter: 3.7279 sec.\n",
      "Iteration 9520 || Loss: 13.5048 || 10iter: 3.6959 sec.\n",
      "Iteration 9530 || Loss: 13.1157 || 10iter: 3.5050 sec.\n",
      "Iteration 9540 || Loss: 15.1416 || 10iter: 3.3421 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:3566.4985 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.5545 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 48/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9550 || Loss: 17.1624 || 10iter: 6.7010 sec.\n",
      "Iteration 9560 || Loss: 16.8679 || 10iter: 3.7010 sec.\n",
      "Iteration 9570 || Loss: 12.9324 || 10iter: 3.7484 sec.\n",
      "Iteration 9580 || Loss: 16.4702 || 10iter: 3.6749 sec.\n",
      "Iteration 9590 || Loss: 22.5200 || 10iter: 3.7063 sec.\n",
      "Iteration 9600 || Loss: 18.8721 || 10iter: 3.6886 sec.\n",
      "Iteration 9610 || Loss: 18.5267 || 10iter: 3.6314 sec.\n",
      "Iteration 9620 || Loss: 15.1464 || 10iter: 3.6131 sec.\n",
      "Iteration 9630 || Loss: 12.7808 || 10iter: 3.7129 sec.\n",
      "Iteration 9640 || Loss: 14.0743 || 10iter: 3.6509 sec.\n",
      "Iteration 9650 || Loss: 16.9766 || 10iter: 3.7054 sec.\n",
      "Iteration 9660 || Loss: 22.3954 || 10iter: 3.7561 sec.\n",
      "Iteration 9670 || Loss: 13.8760 || 10iter: 3.6816 sec.\n",
      "Iteration 9680 || Loss: 20.4413 || 10iter: 3.6286 sec.\n",
      "Iteration 9690 || Loss: 20.0410 || 10iter: 3.7581 sec.\n",
      "Iteration 9700 || Loss: 18.4632 || 10iter: 3.6522 sec.\n",
      "Iteration 9710 || Loss: 25.2068 || 10iter: 3.7000 sec.\n",
      "Iteration 9720 || Loss: 18.9200 || 10iter: 3.7326 sec.\n",
      "Iteration 9730 || Loss: 19.1542 || 10iter: 3.6288 sec.\n",
      "Iteration 9740 || Loss: 17.3282 || 10iter: 3.3358 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:3536.3665 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.5560 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 49/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9750 || Loss: 19.3664 || 10iter: 5.6168 sec.\n",
      "Iteration 9760 || Loss: 24.8980 || 10iter: 3.7498 sec.\n",
      "Iteration 9770 || Loss: 13.8780 || 10iter: 3.7807 sec.\n",
      "Iteration 9780 || Loss: 18.8272 || 10iter: 3.6235 sec.\n",
      "Iteration 9790 || Loss: 13.8403 || 10iter: 3.6655 sec.\n",
      "Iteration 9800 || Loss: 17.0628 || 10iter: 3.7451 sec.\n",
      "Iteration 9810 || Loss: 13.7469 || 10iter: 3.6854 sec.\n",
      "Iteration 9820 || Loss: 16.0271 || 10iter: 3.6880 sec.\n",
      "Iteration 9830 || Loss: 24.6141 || 10iter: 3.7209 sec.\n",
      "Iteration 9840 || Loss: 15.3582 || 10iter: 3.7347 sec.\n",
      "Iteration 9850 || Loss: 18.9340 || 10iter: 3.6837 sec.\n",
      "Iteration 9860 || Loss: 19.3622 || 10iter: 3.7247 sec.\n",
      "Iteration 9870 || Loss: 17.7110 || 10iter: 3.6521 sec.\n",
      "Iteration 9880 || Loss: 22.0835 || 10iter: 3.6608 sec.\n",
      "Iteration 9890 || Loss: 18.0782 || 10iter: 3.7086 sec.\n",
      "Iteration 9900 || Loss: 15.5091 || 10iter: 3.6178 sec.\n",
      "Iteration 9910 || Loss: 13.7517 || 10iter: 3.7836 sec.\n",
      "Iteration 9920 || Loss: 20.2041 || 10iter: 3.7034 sec.\n",
      "Iteration 9930 || Loss: 18.7005 || 10iter: 3.6986 sec.\n",
      "Iteration 9940 || Loss: 17.0422 || 10iter: 3.4004 sec.\n",
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:3519.1023 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8512 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 50/200\n",
      "-------------\n",
      "train\n",
      "Iteration 9950 || Loss: 13.4404 || 10iter: 4.1207 sec.\n",
      "Iteration 9960 || Loss: 15.3724 || 10iter: 4.2969 sec.\n",
      "Iteration 9970 || Loss: 15.8433 || 10iter: 3.6341 sec.\n",
      "Iteration 9980 || Loss: 13.3042 || 10iter: 3.6928 sec.\n",
      "Iteration 9990 || Loss: 20.4526 || 10iter: 3.7468 sec.\n",
      "Iteration 10000 || Loss: 20.5689 || 10iter: 3.7102 sec.\n",
      "Iteration 10010 || Loss: 16.4825 || 10iter: 3.6469 sec.\n",
      "Iteration 10020 || Loss: 16.9416 || 10iter: 3.6408 sec.\n",
      "Iteration 10030 || Loss: 16.7505 || 10iter: 3.6090 sec.\n",
      "Iteration 10040 || Loss: 12.8618 || 10iter: 3.6821 sec.\n",
      "Iteration 10050 || Loss: 25.3246 || 10iter: 3.7148 sec.\n",
      "Iteration 10060 || Loss: 16.1525 || 10iter: 3.7064 sec.\n",
      "Iteration 10070 || Loss: 12.0091 || 10iter: 3.6919 sec.\n",
      "Iteration 10080 || Loss: 18.8537 || 10iter: 3.7038 sec.\n",
      "Iteration 10090 || Loss: 15.7502 || 10iter: 3.6964 sec.\n",
      "Iteration 10100 || Loss: 22.2734 || 10iter: 3.7385 sec.\n",
      "Iteration 10110 || Loss: 15.2075 || 10iter: 3.7507 sec.\n",
      "Iteration 10120 || Loss: 19.0088 || 10iter: 3.6799 sec.\n",
      "Iteration 10130 || Loss: 24.5870 || 10iter: 3.6543 sec.\n",
      "Iteration 10140 || Loss: 18.1947 || 10iter: 3.5111 sec.\n",
      "Iteration 10150 || Loss: 7.8722 || 10iter: 3.0836 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:3460.5444 ||Epoch_VAL_Loss:1090.9392\n",
      "timer:  89.2603 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 51/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10160 || Loss: 27.3802 || 10iter: 7.0599 sec.\n",
      "Iteration 10170 || Loss: 12.8594 || 10iter: 3.6875 sec.\n",
      "Iteration 10180 || Loss: 18.7432 || 10iter: 3.7634 sec.\n",
      "Iteration 10190 || Loss: 12.8967 || 10iter: 3.6801 sec.\n",
      "Iteration 10200 || Loss: 20.0192 || 10iter: 3.7139 sec.\n",
      "Iteration 10210 || Loss: 17.5770 || 10iter: 3.6879 sec.\n",
      "Iteration 10220 || Loss: 18.7452 || 10iter: 3.7342 sec.\n",
      "Iteration 10230 || Loss: 11.4797 || 10iter: 3.6243 sec.\n",
      "Iteration 10240 || Loss: 15.7999 || 10iter: 3.6842 sec.\n",
      "Iteration 10250 || Loss: 17.2641 || 10iter: 3.6904 sec.\n",
      "Iteration 10260 || Loss: 20.8137 || 10iter: 3.6800 sec.\n",
      "Iteration 10270 || Loss: 16.4770 || 10iter: 3.6585 sec.\n",
      "Iteration 10280 || Loss: 19.1474 || 10iter: 3.6794 sec.\n",
      "Iteration 10290 || Loss: 16.9255 || 10iter: 3.7098 sec.\n",
      "Iteration 10300 || Loss: 25.4284 || 10iter: 3.7174 sec.\n",
      "Iteration 10310 || Loss: 13.8815 || 10iter: 3.7075 sec.\n",
      "Iteration 10320 || Loss: 21.9302 || 10iter: 3.7733 sec.\n",
      "Iteration 10330 || Loss: 16.0554 || 10iter: 3.6300 sec.\n",
      "Iteration 10340 || Loss: 14.2549 || 10iter: 3.6187 sec.\n",
      "Iteration 10350 || Loss: 15.5851 || 10iter: 3.3828 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:3431.2895 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6948 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 52/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10360 || Loss: 17.7095 || 10iter: 5.7951 sec.\n",
      "Iteration 10370 || Loss: 17.0290 || 10iter: 3.6172 sec.\n",
      "Iteration 10380 || Loss: 22.9771 || 10iter: 3.7782 sec.\n",
      "Iteration 10390 || Loss: 17.9049 || 10iter: 3.8428 sec.\n",
      "Iteration 10400 || Loss: 22.1389 || 10iter: 3.6287 sec.\n",
      "Iteration 10410 || Loss: 19.9707 || 10iter: 3.7299 sec.\n",
      "Iteration 10420 || Loss: 18.4635 || 10iter: 3.7020 sec.\n",
      "Iteration 10430 || Loss: 12.6251 || 10iter: 3.7125 sec.\n",
      "Iteration 10440 || Loss: 15.6304 || 10iter: 3.6736 sec.\n",
      "Iteration 10450 || Loss: 18.2130 || 10iter: 3.6988 sec.\n",
      "Iteration 10460 || Loss: 14.9486 || 10iter: 3.6744 sec.\n",
      "Iteration 10470 || Loss: 14.2829 || 10iter: 3.6597 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10480 || Loss: 15.0476 || 10iter: 3.7181 sec.\n",
      "Iteration 10490 || Loss: 13.3308 || 10iter: 3.7177 sec.\n",
      "Iteration 10500 || Loss: 15.1352 || 10iter: 3.6998 sec.\n",
      "Iteration 10510 || Loss: 10.5504 || 10iter: 3.7742 sec.\n",
      "Iteration 10520 || Loss: 14.2654 || 10iter: 3.7324 sec.\n",
      "Iteration 10530 || Loss: 15.0251 || 10iter: 3.6632 sec.\n",
      "Iteration 10540 || Loss: 15.9008 || 10iter: 3.6131 sec.\n",
      "Iteration 10550 || Loss: 15.7385 || 10iter: 3.3658 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:3398.9321 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6237 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 53/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10560 || Loss: 18.3004 || 10iter: 4.5645 sec.\n",
      "Iteration 10570 || Loss: 13.9067 || 10iter: 4.0636 sec.\n",
      "Iteration 10580 || Loss: 13.8171 || 10iter: 3.6400 sec.\n",
      "Iteration 10590 || Loss: 28.1352 || 10iter: 3.6973 sec.\n",
      "Iteration 10600 || Loss: 13.2282 || 10iter: 3.6958 sec.\n",
      "Iteration 10610 || Loss: 14.0883 || 10iter: 3.6847 sec.\n",
      "Iteration 10620 || Loss: 19.3089 || 10iter: 3.7708 sec.\n",
      "Iteration 10630 || Loss: 14.3230 || 10iter: 3.6960 sec.\n",
      "Iteration 10640 || Loss: 15.5368 || 10iter: 3.6627 sec.\n",
      "Iteration 10650 || Loss: 16.7632 || 10iter: 3.6805 sec.\n",
      "Iteration 10660 || Loss: 14.5521 || 10iter: 3.7942 sec.\n",
      "Iteration 10670 || Loss: 18.4662 || 10iter: 3.7486 sec.\n",
      "Iteration 10680 || Loss: 20.1651 || 10iter: 3.6060 sec.\n",
      "Iteration 10690 || Loss: 13.8036 || 10iter: 3.7616 sec.\n",
      "Iteration 10700 || Loss: 14.7675 || 10iter: 3.7068 sec.\n",
      "Iteration 10710 || Loss: 23.6594 || 10iter: 3.6746 sec.\n",
      "Iteration 10720 || Loss: 14.1556 || 10iter: 3.7491 sec.\n",
      "Iteration 10730 || Loss: 17.0572 || 10iter: 3.7245 sec.\n",
      "Iteration 10740 || Loss: 12.6271 || 10iter: 3.7152 sec.\n",
      "Iteration 10750 || Loss: 17.5968 || 10iter: 3.5325 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:3468.1848 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9947 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 54/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10760 || Loss: 19.1666 || 10iter: 3.0081 sec.\n",
      "Iteration 10770 || Loss: 14.0427 || 10iter: 4.3145 sec.\n",
      "Iteration 10780 || Loss: 18.3752 || 10iter: 3.6349 sec.\n",
      "Iteration 10790 || Loss: 17.4435 || 10iter: 3.7006 sec.\n",
      "Iteration 10800 || Loss: 13.8810 || 10iter: 3.7586 sec.\n",
      "Iteration 10810 || Loss: 18.3886 || 10iter: 3.7272 sec.\n",
      "Iteration 10820 || Loss: 17.2292 || 10iter: 3.8507 sec.\n",
      "Iteration 10830 || Loss: 20.8064 || 10iter: 3.6076 sec.\n",
      "Iteration 10840 || Loss: 15.5271 || 10iter: 3.7235 sec.\n",
      "Iteration 10850 || Loss: 13.0045 || 10iter: 3.7579 sec.\n",
      "Iteration 10860 || Loss: 18.6438 || 10iter: 3.8555 sec.\n",
      "Iteration 10870 || Loss: 25.6690 || 10iter: 3.7628 sec.\n",
      "Iteration 10880 || Loss: 15.7176 || 10iter: 3.6534 sec.\n",
      "Iteration 10890 || Loss: 19.5951 || 10iter: 3.6702 sec.\n",
      "Iteration 10900 || Loss: 15.3654 || 10iter: 3.6361 sec.\n",
      "Iteration 10910 || Loss: 15.0464 || 10iter: 3.7111 sec.\n",
      "Iteration 10920 || Loss: 11.1835 || 10iter: 3.6749 sec.\n",
      "Iteration 10930 || Loss: 18.2159 || 10iter: 3.7106 sec.\n",
      "Iteration 10940 || Loss: 17.8950 || 10iter: 3.6730 sec.\n",
      "Iteration 10950 || Loss: 13.0480 || 10iter: 3.5410 sec.\n",
      "Iteration 10960 || Loss: 14.5722 || 10iter: 3.3686 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:3448.4113 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8256 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 55/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10970 || Loss: 16.2034 || 10iter: 6.3371 sec.\n",
      "Iteration 10980 || Loss: 16.7241 || 10iter: 3.7724 sec.\n",
      "Iteration 10990 || Loss: 20.3017 || 10iter: 3.7411 sec.\n",
      "Iteration 11000 || Loss: 12.4143 || 10iter: 3.8665 sec.\n",
      "Iteration 11010 || Loss: 13.1818 || 10iter: 3.7299 sec.\n",
      "Iteration 11020 || Loss: 21.8345 || 10iter: 3.8025 sec.\n",
      "Iteration 11030 || Loss: 17.0911 || 10iter: 3.6981 sec.\n",
      "Iteration 11040 || Loss: 15.8161 || 10iter: 3.7197 sec.\n",
      "Iteration 11050 || Loss: 25.3108 || 10iter: 3.7370 sec.\n",
      "Iteration 11060 || Loss: 13.1418 || 10iter: 3.7104 sec.\n",
      "Iteration 11070 || Loss: 14.5603 || 10iter: 3.6305 sec.\n",
      "Iteration 11080 || Loss: 21.0978 || 10iter: 3.6949 sec.\n",
      "Iteration 11090 || Loss: 16.1141 || 10iter: 3.6936 sec.\n",
      "Iteration 11100 || Loss: 16.0746 || 10iter: 3.6361 sec.\n",
      "Iteration 11110 || Loss: 21.5018 || 10iter: 3.6513 sec.\n",
      "Iteration 11120 || Loss: 14.5902 || 10iter: 3.6726 sec.\n",
      "Iteration 11130 || Loss: 16.2441 || 10iter: 3.6911 sec.\n",
      "Iteration 11140 || Loss: 15.6862 || 10iter: 3.7265 sec.\n",
      "Iteration 11150 || Loss: 16.9098 || 10iter: 3.6378 sec.\n",
      "Iteration 11160 || Loss: 15.7265 || 10iter: 3.3353 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:3439.2871 ||Epoch_VAL_Loss:1074.0302\n",
      "timer:  89.5829 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 56/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11170 || Loss: 13.7217 || 10iter: 4.7513 sec.\n",
      "Iteration 11180 || Loss: 14.9553 || 10iter: 4.0086 sec.\n",
      "Iteration 11190 || Loss: 22.6050 || 10iter: 3.6948 sec.\n",
      "Iteration 11200 || Loss: 15.6110 || 10iter: 3.7462 sec.\n",
      "Iteration 11210 || Loss: 18.2516 || 10iter: 3.7633 sec.\n",
      "Iteration 11220 || Loss: 18.4288 || 10iter: 4.2761 sec.\n",
      "Iteration 11230 || Loss: 18.4647 || 10iter: 3.7926 sec.\n",
      "Iteration 11240 || Loss: 11.1218 || 10iter: 3.6641 sec.\n",
      "Iteration 11250 || Loss: 13.4085 || 10iter: 3.6661 sec.\n",
      "Iteration 11260 || Loss: 20.6827 || 10iter: 3.7391 sec.\n",
      "Iteration 11270 || Loss: 20.8219 || 10iter: 3.7543 sec.\n",
      "Iteration 11280 || Loss: 19.0398 || 10iter: 3.7281 sec.\n",
      "Iteration 11290 || Loss: 17.5815 || 10iter: 4.0090 sec.\n",
      "Iteration 11300 || Loss: 22.2043 || 10iter: 3.8427 sec.\n",
      "Iteration 11310 || Loss: 15.4457 || 10iter: 3.7432 sec.\n",
      "Iteration 11320 || Loss: 16.0117 || 10iter: 3.6750 sec.\n",
      "Iteration 11330 || Loss: 17.9345 || 10iter: 3.7634 sec.\n",
      "Iteration 11340 || Loss: 20.3336 || 10iter: 3.6702 sec.\n",
      "Iteration 11350 || Loss: 14.3177 || 10iter: 3.7937 sec.\n",
      "Iteration 11360 || Loss: 17.8878 || 10iter: 3.3944 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:3430.4870 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.9623 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 57/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11370 || Loss: 21.4344 || 10iter: 3.3752 sec.\n",
      "Iteration 11380 || Loss: 17.5105 || 10iter: 4.5875 sec.\n",
      "Iteration 11390 || Loss: 9.9622 || 10iter: 3.6635 sec.\n",
      "Iteration 11400 || Loss: 17.0527 || 10iter: 3.9356 sec.\n",
      "Iteration 11410 || Loss: 19.5915 || 10iter: 4.0742 sec.\n",
      "Iteration 11420 || Loss: 16.2416 || 10iter: 3.9654 sec.\n",
      "Iteration 11430 || Loss: 21.2311 || 10iter: 3.7207 sec.\n",
      "Iteration 11440 || Loss: 15.7948 || 10iter: 3.7563 sec.\n",
      "Iteration 11450 || Loss: 21.8788 || 10iter: 3.6757 sec.\n",
      "Iteration 11460 || Loss: 23.8512 || 10iter: 3.7323 sec.\n",
      "Iteration 11470 || Loss: 14.1366 || 10iter: 3.6126 sec.\n",
      "Iteration 11480 || Loss: 16.2738 || 10iter: 3.9156 sec.\n",
      "Iteration 11490 || Loss: 13.2494 || 10iter: 4.1449 sec.\n",
      "Iteration 11500 || Loss: 15.5730 || 10iter: 4.0148 sec.\n",
      "Iteration 11510 || Loss: 16.9785 || 10iter: 3.6870 sec.\n",
      "Iteration 11520 || Loss: 16.2497 || 10iter: 3.7149 sec.\n",
      "Iteration 11530 || Loss: 10.1664 || 10iter: 3.6848 sec.\n",
      "Iteration 11540 || Loss: 16.5567 || 10iter: 3.6903 sec.\n",
      "Iteration 11550 || Loss: 18.3845 || 10iter: 3.7050 sec.\n",
      "Iteration 11560 || Loss: 13.1446 || 10iter: 3.5177 sec.\n",
      "Iteration 11570 || Loss: 16.4103 || 10iter: 3.4266 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:3409.7854 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  79.7717 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 58/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11580 || Loss: 18.5032 || 10iter: 7.0111 sec.\n",
      "Iteration 11590 || Loss: 13.7138 || 10iter: 3.6292 sec.\n",
      "Iteration 11600 || Loss: 18.2134 || 10iter: 3.7044 sec.\n",
      "Iteration 11610 || Loss: 10.5684 || 10iter: 3.6672 sec.\n",
      "Iteration 11620 || Loss: 15.7180 || 10iter: 3.7210 sec.\n",
      "Iteration 11630 || Loss: 17.8094 || 10iter: 3.7310 sec.\n",
      "Iteration 11640 || Loss: 15.2685 || 10iter: 3.7156 sec.\n",
      "Iteration 11650 || Loss: 19.8424 || 10iter: 3.7040 sec.\n",
      "Iteration 11660 || Loss: 14.7828 || 10iter: 3.6545 sec.\n",
      "Iteration 11670 || Loss: 13.6650 || 10iter: 3.7656 sec.\n",
      "Iteration 11680 || Loss: 15.5192 || 10iter: 4.3404 sec.\n",
      "Iteration 11690 || Loss: 14.5029 || 10iter: 4.2466 sec.\n",
      "Iteration 11700 || Loss: 16.5024 || 10iter: 4.1264 sec.\n",
      "Iteration 11710 || Loss: 13.4770 || 10iter: 4.1828 sec.\n",
      "Iteration 11720 || Loss: 17.1951 || 10iter: 4.5257 sec.\n",
      "Iteration 11730 || Loss: 14.7236 || 10iter: 4.3916 sec.\n",
      "Iteration 11740 || Loss: 11.5622 || 10iter: 4.5134 sec.\n",
      "Iteration 11750 || Loss: 18.6457 || 10iter: 4.7284 sec.\n",
      "Iteration 11760 || Loss: 15.9026 || 10iter: 3.7786 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11770 || Loss: 13.1593 || 10iter: 3.3807 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:3359.0923 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  83.6792 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 59/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11780 || Loss: 20.0513 || 10iter: 5.2577 sec.\n",
      "Iteration 11790 || Loss: 15.3899 || 10iter: 4.1176 sec.\n",
      "Iteration 11800 || Loss: 17.8105 || 10iter: 3.7039 sec.\n",
      "Iteration 11810 || Loss: 19.2686 || 10iter: 3.6753 sec.\n",
      "Iteration 11820 || Loss: 19.4741 || 10iter: 3.8104 sec.\n",
      "Iteration 11830 || Loss: 16.1798 || 10iter: 3.7478 sec.\n",
      "Iteration 11840 || Loss: 14.2493 || 10iter: 3.6433 sec.\n",
      "Iteration 11850 || Loss: 17.7511 || 10iter: 3.6538 sec.\n",
      "Iteration 11860 || Loss: 12.8664 || 10iter: 3.7308 sec.\n",
      "Iteration 11870 || Loss: 13.2553 || 10iter: 3.6680 sec.\n",
      "Iteration 11880 || Loss: 13.0394 || 10iter: 3.7161 sec.\n",
      "Iteration 11890 || Loss: 13.0261 || 10iter: 3.7745 sec.\n",
      "Iteration 11900 || Loss: 16.5923 || 10iter: 3.6335 sec.\n",
      "Iteration 11910 || Loss: 15.0761 || 10iter: 3.6966 sec.\n",
      "Iteration 11920 || Loss: 21.0495 || 10iter: 3.7113 sec.\n",
      "Iteration 11930 || Loss: 17.9218 || 10iter: 3.6612 sec.\n",
      "Iteration 11940 || Loss: 17.9374 || 10iter: 3.7435 sec.\n",
      "Iteration 11950 || Loss: 19.6017 || 10iter: 3.7344 sec.\n",
      "Iteration 11960 || Loss: 11.5646 || 10iter: 3.7154 sec.\n",
      "Iteration 11970 || Loss: 17.2807 || 10iter: 3.3538 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:3295.7762 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8753 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 60/200\n",
      "-------------\n",
      "train\n",
      "Iteration 11980 || Loss: 14.6593 || 10iter: 4.1068 sec.\n",
      "Iteration 11990 || Loss: 18.0368 || 10iter: 4.3321 sec.\n",
      "Iteration 12000 || Loss: 18.2954 || 10iter: 3.6658 sec.\n",
      "Iteration 12010 || Loss: 15.4168 || 10iter: 3.6883 sec.\n",
      "Iteration 12020 || Loss: 19.4846 || 10iter: 3.6717 sec.\n",
      "Iteration 12030 || Loss: 17.1420 || 10iter: 3.7167 sec.\n",
      "Iteration 12040 || Loss: 14.9632 || 10iter: 3.7144 sec.\n",
      "Iteration 12050 || Loss: 13.0389 || 10iter: 3.6885 sec.\n",
      "Iteration 12060 || Loss: 18.4252 || 10iter: 3.6934 sec.\n",
      "Iteration 12070 || Loss: 14.5749 || 10iter: 3.6953 sec.\n",
      "Iteration 12080 || Loss: 14.5777 || 10iter: 3.7350 sec.\n",
      "Iteration 12090 || Loss: 14.0705 || 10iter: 3.6185 sec.\n",
      "Iteration 12100 || Loss: 16.3627 || 10iter: 3.7517 sec.\n",
      "Iteration 12110 || Loss: 14.3926 || 10iter: 3.6800 sec.\n",
      "Iteration 12120 || Loss: 16.0523 || 10iter: 3.7390 sec.\n",
      "Iteration 12130 || Loss: 15.9107 || 10iter: 3.7272 sec.\n",
      "Iteration 12140 || Loss: 12.7172 || 10iter: 3.8399 sec.\n",
      "Iteration 12150 || Loss: 15.6986 || 10iter: 3.7176 sec.\n",
      "Iteration 12160 || Loss: 15.7100 || 10iter: 3.7644 sec.\n",
      "Iteration 12170 || Loss: 12.4505 || 10iter: 3.5117 sec.\n",
      "Iteration 12180 || Loss: 22.7934 || 10iter: 3.0968 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:3386.3977 ||Epoch_VAL_Loss:1077.6277\n",
      "timer:  89.7518 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 61/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12190 || Loss: 18.4539 || 10iter: 7.0693 sec.\n",
      "Iteration 12200 || Loss: 14.2535 || 10iter: 3.6797 sec.\n",
      "Iteration 12210 || Loss: 18.7430 || 10iter: 3.8213 sec.\n",
      "Iteration 12220 || Loss: 15.9952 || 10iter: 3.6643 sec.\n",
      "Iteration 12230 || Loss: 16.7519 || 10iter: 3.7141 sec.\n",
      "Iteration 12240 || Loss: 17.3347 || 10iter: 3.8024 sec.\n",
      "Iteration 12250 || Loss: 20.0329 || 10iter: 3.6436 sec.\n",
      "Iteration 12260 || Loss: 21.9453 || 10iter: 3.6613 sec.\n",
      "Iteration 12270 || Loss: 19.6294 || 10iter: 3.6276 sec.\n",
      "Iteration 12280 || Loss: 17.3876 || 10iter: 3.7377 sec.\n",
      "Iteration 12290 || Loss: 20.2004 || 10iter: 3.6187 sec.\n",
      "Iteration 12300 || Loss: 26.4427 || 10iter: 3.6763 sec.\n",
      "Iteration 12310 || Loss: 21.9701 || 10iter: 3.7380 sec.\n",
      "Iteration 12320 || Loss: 16.1038 || 10iter: 3.6972 sec.\n",
      "Iteration 12330 || Loss: 15.8911 || 10iter: 3.6546 sec.\n",
      "Iteration 12340 || Loss: 11.9579 || 10iter: 3.7634 sec.\n",
      "Iteration 12350 || Loss: 19.0056 || 10iter: 3.6533 sec.\n",
      "Iteration 12360 || Loss: 17.8673 || 10iter: 3.6675 sec.\n",
      "Iteration 12370 || Loss: 15.6237 || 10iter: 3.5936 sec.\n",
      "Iteration 12380 || Loss: 16.3460 || 10iter: 3.3612 sec.\n",
      "-------------\n",
      "epoch 61 || Epoch_TRAIN_Loss:3358.5946 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6653 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 62/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12390 || Loss: 13.6364 || 10iter: 6.0407 sec.\n",
      "Iteration 12400 || Loss: 15.3624 || 10iter: 3.6775 sec.\n",
      "Iteration 12410 || Loss: 17.1477 || 10iter: 3.7395 sec.\n",
      "Iteration 12420 || Loss: 17.8930 || 10iter: 3.7522 sec.\n",
      "Iteration 12430 || Loss: 14.8431 || 10iter: 3.6733 sec.\n",
      "Iteration 12440 || Loss: 14.5318 || 10iter: 3.7482 sec.\n",
      "Iteration 12450 || Loss: 14.8075 || 10iter: 3.7599 sec.\n",
      "Iteration 12460 || Loss: 17.3461 || 10iter: 3.6609 sec.\n",
      "Iteration 12470 || Loss: 22.1206 || 10iter: 3.7923 sec.\n",
      "Iteration 12480 || Loss: 12.7660 || 10iter: 3.7616 sec.\n",
      "Iteration 12490 || Loss: 13.1732 || 10iter: 3.6376 sec.\n",
      "Iteration 12500 || Loss: 18.5130 || 10iter: 3.6482 sec.\n",
      "Iteration 12510 || Loss: 12.3787 || 10iter: 3.7842 sec.\n",
      "Iteration 12520 || Loss: 13.3756 || 10iter: 3.7170 sec.\n",
      "Iteration 12530 || Loss: 13.8575 || 10iter: 3.7326 sec.\n",
      "Iteration 12540 || Loss: 16.4967 || 10iter: 3.5836 sec.\n",
      "Iteration 12550 || Loss: 17.2046 || 10iter: 3.7009 sec.\n",
      "Iteration 12560 || Loss: 13.3954 || 10iter: 3.6546 sec.\n",
      "Iteration 12570 || Loss: 23.4106 || 10iter: 3.7063 sec.\n",
      "Iteration 12580 || Loss: 18.7170 || 10iter: 3.3563 sec.\n",
      "-------------\n",
      "epoch 62 || Epoch_TRAIN_Loss:3334.6031 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.9369 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 63/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12590 || Loss: 13.4083 || 10iter: 4.8314 sec.\n",
      "Iteration 12600 || Loss: 15.5169 || 10iter: 3.7548 sec.\n",
      "Iteration 12610 || Loss: 19.6194 || 10iter: 3.6265 sec.\n",
      "Iteration 12620 || Loss: 13.1803 || 10iter: 3.7336 sec.\n",
      "Iteration 12630 || Loss: 15.9075 || 10iter: 3.6126 sec.\n",
      "Iteration 12640 || Loss: 17.0538 || 10iter: 3.7682 sec.\n",
      "Iteration 12650 || Loss: 12.1192 || 10iter: 3.8856 sec.\n",
      "Iteration 12660 || Loss: 14.6866 || 10iter: 3.7003 sec.\n",
      "Iteration 12670 || Loss: 19.6733 || 10iter: 3.6895 sec.\n",
      "Iteration 12680 || Loss: 15.3177 || 10iter: 3.6938 sec.\n",
      "Iteration 12690 || Loss: 12.8735 || 10iter: 3.6932 sec.\n",
      "Iteration 12700 || Loss: 17.3691 || 10iter: 3.7066 sec.\n",
      "Iteration 12710 || Loss: 15.6844 || 10iter: 3.7035 sec.\n",
      "Iteration 12720 || Loss: 13.4689 || 10iter: 3.6652 sec.\n",
      "Iteration 12730 || Loss: 13.1606 || 10iter: 3.6923 sec.\n",
      "Iteration 12740 || Loss: 14.7051 || 10iter: 3.7111 sec.\n",
      "Iteration 12750 || Loss: 16.9949 || 10iter: 3.7126 sec.\n",
      "Iteration 12760 || Loss: 16.7740 || 10iter: 3.7096 sec.\n",
      "Iteration 12770 || Loss: 15.8994 || 10iter: 3.7087 sec.\n",
      "Iteration 12780 || Loss: 17.8089 || 10iter: 3.4354 sec.\n",
      "-------------\n",
      "epoch 63 || Epoch_TRAIN_Loss:3328.2597 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8271 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 64/200\n",
      "-------------\n",
      "train\n",
      "Iteration 12790 || Loss: 17.3468 || 10iter: 2.8774 sec.\n",
      "Iteration 12800 || Loss: 17.4844 || 10iter: 4.5942 sec.\n",
      "Iteration 12810 || Loss: 17.6654 || 10iter: 3.6020 sec.\n",
      "Iteration 12820 || Loss: 14.8364 || 10iter: 3.7281 sec.\n",
      "Iteration 12830 || Loss: 15.1694 || 10iter: 3.7727 sec.\n",
      "Iteration 12840 || Loss: 15.2144 || 10iter: 3.7100 sec.\n",
      "Iteration 12850 || Loss: 15.8062 || 10iter: 3.7016 sec.\n",
      "Iteration 12860 || Loss: 17.5483 || 10iter: 3.6008 sec.\n",
      "Iteration 12870 || Loss: 14.5541 || 10iter: 3.6116 sec.\n",
      "Iteration 12880 || Loss: 15.1153 || 10iter: 3.6860 sec.\n",
      "Iteration 12890 || Loss: 14.2998 || 10iter: 3.7278 sec.\n",
      "Iteration 12900 || Loss: 12.1144 || 10iter: 3.6951 sec.\n",
      "Iteration 12910 || Loss: 17.1696 || 10iter: 3.7127 sec.\n",
      "Iteration 12920 || Loss: 14.5812 || 10iter: 3.7309 sec.\n",
      "Iteration 12930 || Loss: 16.7990 || 10iter: 3.6966 sec.\n",
      "Iteration 12940 || Loss: 21.6894 || 10iter: 3.6208 sec.\n",
      "Iteration 12950 || Loss: 15.3449 || 10iter: 3.7427 sec.\n",
      "Iteration 12960 || Loss: 22.8121 || 10iter: 3.7194 sec.\n",
      "Iteration 12970 || Loss: 21.0440 || 10iter: 3.7340 sec.\n",
      "Iteration 12980 || Loss: 15.8781 || 10iter: 3.5732 sec.\n",
      "Iteration 12990 || Loss: 19.0414 || 10iter: 3.3423 sec.\n",
      "-------------\n",
      "epoch 64 || Epoch_TRAIN_Loss:3360.7907 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6497 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 65/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13000 || Loss: 19.0765 || 10iter: 6.2618 sec.\n",
      "Iteration 13010 || Loss: 17.4051 || 10iter: 3.7779 sec.\n",
      "Iteration 13020 || Loss: 13.2015 || 10iter: 3.6913 sec.\n",
      "Iteration 13030 || Loss: 14.5121 || 10iter: 3.7564 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13040 || Loss: 14.4235 || 10iter: 3.7192 sec.\n",
      "Iteration 13050 || Loss: 13.7528 || 10iter: 3.7523 sec.\n",
      "Iteration 13060 || Loss: 16.3154 || 10iter: 3.6990 sec.\n",
      "Iteration 13070 || Loss: 14.0896 || 10iter: 3.6995 sec.\n",
      "Iteration 13080 || Loss: 16.7728 || 10iter: 3.6286 sec.\n",
      "Iteration 13090 || Loss: 10.7634 || 10iter: 3.7171 sec.\n",
      "Iteration 13100 || Loss: 12.5050 || 10iter: 3.7209 sec.\n",
      "Iteration 13110 || Loss: 20.6924 || 10iter: 3.7600 sec.\n",
      "Iteration 13120 || Loss: 16.7491 || 10iter: 4.0064 sec.\n",
      "Iteration 13130 || Loss: 14.7157 || 10iter: 3.6367 sec.\n",
      "Iteration 13140 || Loss: 15.7824 || 10iter: 3.7233 sec.\n",
      "Iteration 13150 || Loss: 15.7547 || 10iter: 3.6895 sec.\n",
      "Iteration 13160 || Loss: 16.3591 || 10iter: 3.7655 sec.\n",
      "Iteration 13170 || Loss: 17.4687 || 10iter: 3.6627 sec.\n",
      "Iteration 13180 || Loss: 12.8630 || 10iter: 3.6264 sec.\n",
      "Iteration 13190 || Loss: 14.1964 || 10iter: 3.3797 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 65 || Epoch_TRAIN_Loss:3311.8594 ||Epoch_VAL_Loss:1058.6844\n",
      "timer:  89.7574 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 66/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13200 || Loss: 19.2419 || 10iter: 5.0835 sec.\n",
      "Iteration 13210 || Loss: 15.7881 || 10iter: 3.8934 sec.\n",
      "Iteration 13220 || Loss: 12.9446 || 10iter: 3.6997 sec.\n",
      "Iteration 13230 || Loss: 14.2704 || 10iter: 3.6837 sec.\n",
      "Iteration 13240 || Loss: 13.2712 || 10iter: 3.6587 sec.\n",
      "Iteration 13250 || Loss: 18.0928 || 10iter: 3.7180 sec.\n",
      "Iteration 13260 || Loss: 15.1028 || 10iter: 3.7351 sec.\n",
      "Iteration 13270 || Loss: 17.9216 || 10iter: 3.6172 sec.\n",
      "Iteration 13280 || Loss: 19.1828 || 10iter: 3.6657 sec.\n",
      "Iteration 13290 || Loss: 15.4212 || 10iter: 3.7081 sec.\n",
      "Iteration 13300 || Loss: 15.6287 || 10iter: 3.6957 sec.\n",
      "Iteration 13310 || Loss: 13.4698 || 10iter: 3.6538 sec.\n",
      "Iteration 13320 || Loss: 15.4882 || 10iter: 3.6438 sec.\n",
      "Iteration 13330 || Loss: 16.0186 || 10iter: 3.7598 sec.\n",
      "Iteration 13340 || Loss: 13.5548 || 10iter: 3.6767 sec.\n",
      "Iteration 13350 || Loss: 16.7637 || 10iter: 3.6701 sec.\n",
      "Iteration 13360 || Loss: 16.2873 || 10iter: 3.7756 sec.\n",
      "Iteration 13370 || Loss: 16.4784 || 10iter: 3.6931 sec.\n",
      "Iteration 13380 || Loss: 13.3280 || 10iter: 3.7374 sec.\n",
      "Iteration 13390 || Loss: 16.0917 || 10iter: 3.4021 sec.\n",
      "-------------\n",
      "epoch 66 || Epoch_TRAIN_Loss:3331.8192 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.6768 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 67/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13400 || Loss: 11.8229 || 10iter: 3.5142 sec.\n",
      "Iteration 13410 || Loss: 15.3822 || 10iter: 4.3507 sec.\n",
      "Iteration 13420 || Loss: 13.4812 || 10iter: 3.7028 sec.\n",
      "Iteration 13430 || Loss: 14.7954 || 10iter: 3.7135 sec.\n",
      "Iteration 13440 || Loss: 12.5955 || 10iter: 3.7053 sec.\n",
      "Iteration 13450 || Loss: 11.5560 || 10iter: 3.7676 sec.\n",
      "Iteration 13460 || Loss: 14.6462 || 10iter: 3.6394 sec.\n",
      "Iteration 13470 || Loss: 15.7934 || 10iter: 3.7318 sec.\n",
      "Iteration 13480 || Loss: 12.7106 || 10iter: 3.6411 sec.\n",
      "Iteration 13490 || Loss: 15.6662 || 10iter: 3.7350 sec.\n",
      "Iteration 13500 || Loss: 16.4653 || 10iter: 3.7202 sec.\n",
      "Iteration 13510 || Loss: 17.0419 || 10iter: 3.6650 sec.\n",
      "Iteration 13520 || Loss: 12.4997 || 10iter: 3.6687 sec.\n",
      "Iteration 13530 || Loss: 21.9577 || 10iter: 3.7311 sec.\n",
      "Iteration 13540 || Loss: 10.9727 || 10iter: 3.7066 sec.\n",
      "Iteration 13550 || Loss: 14.9516 || 10iter: 3.8098 sec.\n",
      "Iteration 13560 || Loss: 15.0583 || 10iter: 3.9270 sec.\n",
      "Iteration 13570 || Loss: 16.2099 || 10iter: 3.7247 sec.\n",
      "Iteration 13580 || Loss: 15.9262 || 10iter: 3.6432 sec.\n",
      "Iteration 13590 || Loss: 14.0797 || 10iter: 3.5398 sec.\n",
      "Iteration 13600 || Loss: 18.5312 || 10iter: 3.3444 sec.\n",
      "-------------\n",
      "epoch 67 || Epoch_TRAIN_Loss:3259.2064 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  78.1284 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 68/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13610 || Loss: 18.6123 || 10iter: 6.9172 sec.\n",
      "Iteration 13620 || Loss: 17.1725 || 10iter: 3.6267 sec.\n",
      "Iteration 13630 || Loss: 14.3062 || 10iter: 3.6872 sec.\n",
      "Iteration 13640 || Loss: 15.1917 || 10iter: 3.6840 sec.\n",
      "Iteration 13650 || Loss: 18.7357 || 10iter: 3.7803 sec.\n",
      "Iteration 13660 || Loss: 15.3570 || 10iter: 3.7510 sec.\n",
      "Iteration 13670 || Loss: 17.1869 || 10iter: 3.5762 sec.\n",
      "Iteration 13680 || Loss: 14.7643 || 10iter: 3.6729 sec.\n",
      "Iteration 13690 || Loss: 14.2988 || 10iter: 3.6777 sec.\n",
      "Iteration 13700 || Loss: 16.0022 || 10iter: 3.6913 sec.\n",
      "Iteration 13710 || Loss: 18.9239 || 10iter: 3.7748 sec.\n",
      "Iteration 13720 || Loss: 16.1715 || 10iter: 3.7518 sec.\n",
      "Iteration 13730 || Loss: 20.7166 || 10iter: 3.7384 sec.\n",
      "Iteration 13740 || Loss: 18.2658 || 10iter: 3.6610 sec.\n",
      "Iteration 13750 || Loss: 13.0895 || 10iter: 3.6605 sec.\n",
      "Iteration 13760 || Loss: 16.0214 || 10iter: 3.7195 sec.\n",
      "Iteration 13770 || Loss: 16.4985 || 10iter: 3.7034 sec.\n",
      "Iteration 13780 || Loss: 22.0614 || 10iter: 3.6935 sec.\n",
      "Iteration 13790 || Loss: 13.4395 || 10iter: 3.6105 sec.\n",
      "Iteration 13800 || Loss: 16.8131 || 10iter: 3.3447 sec.\n",
      "-------------\n",
      "epoch 68 || Epoch_TRAIN_Loss:3353.5435 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.8613 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 69/200\n",
      "-------------\n",
      "train\n",
      "Iteration 13810 || Loss: 16.5579 || 10iter: 5.5785 sec.\n",
      "Iteration 13820 || Loss: 13.9753 || 10iter: 3.7032 sec.\n",
      "Iteration 13830 || Loss: 13.9513 || 10iter: 3.6922 sec.\n",
      "Iteration 13840 || Loss: 20.7878 || 10iter: 3.7286 sec.\n",
      "Iteration 13850 || Loss: 12.6600 || 10iter: 3.7395 sec.\n",
      "Iteration 13860 || Loss: 20.1957 || 10iter: 3.7029 sec.\n",
      "Iteration 13870 || Loss: 17.7373 || 10iter: 3.6978 sec.\n",
      "Iteration 13880 || Loss: 14.0428 || 10iter: 3.6233 sec.\n",
      "Iteration 13890 || Loss: 13.6763 || 10iter: 3.6806 sec.\n",
      "Iteration 13900 || Loss: 17.9217 || 10iter: 3.6916 sec.\n",
      "Iteration 13910 || Loss: 18.1845 || 10iter: 3.6808 sec.\n",
      "Iteration 13920 || Loss: 18.6011 || 10iter: 3.7989 sec.\n",
      "Iteration 13930 || Loss: 13.3719 || 10iter: 3.6809 sec.\n",
      "Iteration 13940 || Loss: 19.7057 || 10iter: 3.6136 sec.\n",
      "Iteration 13950 || Loss: 15.1729 || 10iter: 3.7077 sec.\n",
      "Iteration 13960 || Loss: 14.8962 || 10iter: 3.7628 sec.\n",
      "Iteration 13970 || Loss: 21.9315 || 10iter: 3.7611 sec.\n",
      "Iteration 13980 || Loss: 12.8583 || 10iter: 3.6650 sec.\n",
      "Iteration 13990 || Loss: 20.5225 || 10iter: 3.6765 sec.\n",
      "Iteration 14000 || Loss: 15.6206 || 10iter: 3.3946 sec.\n",
      "-------------\n",
      "epoch 69 || Epoch_TRAIN_Loss:3277.0824 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  77.7468 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 70/200\n",
      "-------------\n",
      "train\n",
      "Iteration 14010 || Loss: 13.8138 || 10iter: 4.1904 sec.\n",
      "Iteration 14020 || Loss: 16.0858 || 10iter: 4.5193 sec.\n",
      "Iteration 14030 || Loss: 16.0317 || 10iter: 3.8572 sec.\n",
      "Iteration 14040 || Loss: 13.6608 || 10iter: 3.7161 sec.\n",
      "Iteration 14050 || Loss: 16.7950 || 10iter: 3.6870 sec.\n",
      "Iteration 14060 || Loss: 15.0164 || 10iter: 3.7554 sec.\n",
      "Iteration 14070 || Loss: 14.2231 || 10iter: 3.7271 sec.\n",
      "Iteration 14080 || Loss: 22.8733 || 10iter: 3.7075 sec.\n",
      "Iteration 14090 || Loss: 12.2812 || 10iter: 3.7796 sec.\n",
      "Iteration 14100 || Loss: 19.3738 || 10iter: 3.7472 sec.\n",
      "Iteration 14110 || Loss: 13.8720 || 10iter: 3.7745 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
