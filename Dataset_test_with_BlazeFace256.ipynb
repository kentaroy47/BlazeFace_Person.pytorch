{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000009\n",
      "000017\n",
      "000021\n",
      "000023\n",
      "000030\n",
      "000032\n",
      "000035\n",
      "000041\n",
      "000048\n",
      "000050\n",
      "000051\n",
      "000066\n",
      "000073\n",
      "000081\n",
      "000083\n",
      "000089\n",
      "000101\n",
      "000104\n",
      "000110\n",
      "000113\n",
      "000125\n",
      "000129\n",
      "000131\n",
      "000133\n",
      "000138\n",
      "000146\n",
      "000150\n",
      "000159\n",
      "000162\n",
      "000163\n",
      "000164\n",
      "000165\n",
      "000169\n",
      "000170\n",
      "000171\n",
      "000173\n",
      "000174\n",
      "000177\n",
      "000190\n",
      "000192\n",
      "000193\n",
      "000194\n",
      "000200\n",
      "000210\n",
      "000218\n",
      "000220\n",
      "000222\n",
      "000229\n",
      "000232\n",
      "000245\n",
      "000251\n",
      "000257\n",
      "000259\n",
      "000269\n",
      "000275\n",
      "000276\n",
      "000278\n",
      "000282\n",
      "000285\n",
      "000288\n",
      "000298\n",
      "000302\n",
      "000305\n",
      "000308\n",
      "000320\n",
      "000321\n",
      "000322\n",
      "000323\n",
      "000328\n",
      "000331\n",
      "000337\n",
      "000338\n",
      "000352\n",
      "000359\n",
      "000367\n",
      "000372\n",
      "000374\n",
      "000382\n",
      "000394\n",
      "000406\n",
      "000407\n",
      "000411\n",
      "000419\n",
      "000428\n",
      "000433\n",
      "000435\n",
      "000438\n",
      "000443\n",
      "000446\n",
      "000448\n",
      "000463\n",
      "000468\n",
      "000470\n",
      "000476\n",
      "000477\n",
      "000480\n",
      "000482\n",
      "000483\n",
      "000498\n",
      "000499\n",
      "000500\n",
      "000515\n",
      "000516\n",
      "000518\n",
      "000520\n",
      "000523\n",
      "000524\n",
      "000525\n",
      "000526\n",
      "000530\n",
      "000531\n",
      "000535\n",
      "000541\n",
      "000545\n",
      "000554\n",
      "000555\n",
      "000579\n",
      "000583\n",
      "000589\n",
      "000591\n",
      "000597\n",
      "000612\n",
      "000613\n",
      "000625\n",
      "000626\n",
      "000628\n",
      "000633\n",
      "000648\n",
      "000654\n",
      "000677\n",
      "000684\n",
      "000688\n",
      "000690\n",
      "000694\n",
      "000695\n",
      "000702\n",
      "000709\n",
      "000717\n",
      "000726\n",
      "000731\n",
      "000733\n",
      "000739\n",
      "000742\n",
      "000750\n",
      "000752\n",
      "000753\n",
      "000755\n",
      "000760\n",
      "000770\n",
      "000777\n",
      "000782\n",
      "000786\n",
      "000793\n",
      "000797\n",
      "000799\n",
      "000802\n",
      "000805\n",
      "000806\n",
      "000810\n",
      "000812\n",
      "000814\n",
      "000816\n",
      "000828\n",
      "000829\n",
      "000834\n",
      "000842\n",
      "000843\n",
      "000847\n",
      "000848\n",
      "000851\n",
      "000854\n",
      "000855\n",
      "000859\n",
      "000860\n",
      "000862\n",
      "000865\n",
      "000874\n",
      "000878\n",
      "000879\n",
      "000885\n",
      "000892\n",
      "000895\n",
      "000898\n",
      "000902\n",
      "000903\n",
      "000904\n",
      "000906\n",
      "000911\n",
      "000915\n",
      "000918\n",
      "000920\n",
      "000926\n",
      "000937\n",
      "000943\n",
      "000948\n",
      "000949\n",
      "000966\n",
      "000967\n",
      "000971\n",
      "000982\n",
      "000987\n",
      "000991\n",
      "000996\n",
      "000999\n",
      "001001\n",
      "001011\n",
      "001014\n",
      "001017\n",
      "001024\n",
      "001028\n",
      "001036\n",
      "001042\n",
      "001050\n",
      "001057\n",
      "001060\n",
      "001061\n",
      "001064\n",
      "001066\n",
      "001071\n",
      "001072\n",
      "001079\n",
      "001084\n",
      "001091\n",
      "001092\n",
      "001097\n",
      "001101\n",
      "001109\n",
      "001113\n",
      "001125\n",
      "001129\n",
      "001137\n",
      "001140\n",
      "001145\n",
      "001147\n",
      "001151\n",
      "001152\n",
      "001164\n",
      "001168\n",
      "001170\n",
      "001171\n",
      "001175\n",
      "001185\n",
      "001186\n",
      "001201\n",
      "001206\n",
      "001212\n",
      "001221\n",
      "001224\n",
      "001229\n",
      "001234\n",
      "001236\n",
      "001240\n",
      "001241\n",
      "001248\n",
      "001254\n",
      "001259\n",
      "001265\n",
      "001266\n",
      "001269\n",
      "001272\n",
      "001279\n",
      "001281\n",
      "001284\n",
      "001287\n",
      "001292\n",
      "001298\n",
      "001304\n",
      "001309\n",
      "001310\n",
      "001311\n",
      "001315\n",
      "001325\n",
      "001327\n",
      "001330\n",
      "001333\n",
      "001337\n",
      "001346\n",
      "001350\n",
      "001352\n",
      "001362\n",
      "001378\n",
      "001388\n",
      "001390\n",
      "001393\n",
      "001405\n",
      "001406\n",
      "001408\n",
      "001409\n",
      "001414\n",
      "001420\n",
      "001421\n",
      "001426\n",
      "001427\n",
      "001430\n",
      "001434\n",
      "001445\n",
      "001450\n",
      "001451\n",
      "001455\n",
      "001460\n",
      "001463\n",
      "001472\n",
      "001475\n",
      "001479\n",
      "001480\n",
      "001484\n",
      "001485\n",
      "001493\n",
      "001498\n",
      "001499\n",
      "001501\n",
      "001504\n",
      "001509\n",
      "001510\n",
      "001514\n",
      "001521\n",
      "001523\n",
      "001524\n",
      "001526\n",
      "001531\n",
      "001532\n",
      "001536\n",
      "001537\n",
      "001544\n",
      "001548\n",
      "001554\n",
      "001557\n",
      "001561\n",
      "001563\n",
      "001571\n",
      "001577\n",
      "001579\n",
      "001580\n",
      "001586\n",
      "001593\n",
      "001594\n",
      "001603\n",
      "001608\n",
      "001611\n",
      "001617\n",
      "001622\n",
      "001627\n",
      "001628\n",
      "001630\n",
      "001640\n",
      "001649\n",
      "001651\n",
      "001654\n",
      "001678\n",
      "001682\n",
      "001686\n",
      "001689\n",
      "001690\n",
      "001691\n",
      "001711\n",
      "001717\n",
      "001721\n",
      "001723\n",
      "001725\n",
      "001726\n",
      "001727\n",
      "001729\n",
      "001730\n",
      "001749\n",
      "001754\n",
      "001756\n",
      "001758\n",
      "001759\n",
      "001766\n",
      "001772\n",
      "001775\n",
      "001784\n",
      "001785\n",
      "001789\n",
      "001793\n",
      "001795\n",
      "001797\n",
      "001799\n",
      "001800\n",
      "001801\n",
      "001806\n",
      "001807\n",
      "001809\n",
      "001810\n",
      "001828\n",
      "001830\n",
      "001832\n",
      "001833\n",
      "001836\n",
      "001837\n",
      "001841\n",
      "001843\n",
      "001849\n",
      "001854\n",
      "001861\n",
      "001864\n",
      "001881\n",
      "001887\n",
      "001894\n",
      "001899\n",
      "001902\n",
      "001903\n",
      "001904\n",
      "001918\n",
      "001920\n",
      "001927\n",
      "001930\n",
      "001932\n",
      "001938\n",
      "001941\n",
      "001944\n",
      "001945\n",
      "001950\n",
      "001958\n",
      "001963\n",
      "001977\n",
      "001978\n",
      "001980\n",
      "001989\n",
      "001999\n",
      "002000\n",
      "002012\n",
      "002022\n",
      "002024\n",
      "002030\n",
      "002051\n",
      "002055\n",
      "002061\n",
      "002069\n",
      "002082\n",
      "002090\n",
      "002091\n",
      "002096\n",
      "002104\n",
      "002108\n",
      "002114\n",
      "002116\n",
      "002117\n",
      "002120\n",
      "002142\n",
      "002145\n",
      "002156\n",
      "002163\n",
      "002169\n",
      "002170\n",
      "002171\n",
      "002172\n",
      "002174\n",
      "002176\n",
      "002180\n",
      "002183\n",
      "002186\n",
      "002187\n",
      "002190\n",
      "002193\n",
      "002194\n",
      "002213\n",
      "002214\n",
      "002219\n",
      "002220\n",
      "002221\n",
      "002224\n",
      "002226\n",
      "002233\n",
      "002238\n",
      "002241\n",
      "002244\n",
      "002256\n",
      "002261\n",
      "002265\n",
      "002272\n",
      "002273\n",
      "002277\n",
      "002281\n",
      "002285\n",
      "002288\n",
      "002290\n",
      "002293\n",
      "002302\n",
      "002306\n",
      "002307\n",
      "002311\n",
      "002324\n",
      "002329\n",
      "002332\n",
      "002337\n",
      "002342\n",
      "002361\n",
      "002362\n",
      "002367\n",
      "002372\n",
      "002378\n",
      "002382\n",
      "002384\n",
      "002385\n",
      "002387\n",
      "002401\n",
      "002403\n",
      "002405\n",
      "002410\n",
      "002413\n",
      "002419\n",
      "002425\n",
      "002433\n",
      "002443\n",
      "002444\n",
      "002448\n",
      "002450\n",
      "002458\n",
      "002459\n",
      "002460\n",
      "002472\n",
      "002476\n",
      "002477\n",
      "002481\n",
      "002483\n",
      "002491\n",
      "002497\n",
      "002501\n",
      "002504\n",
      "002514\n",
      "002518\n",
      "002520\n",
      "002524\n",
      "002529\n",
      "002533\n",
      "002539\n",
      "002540\n",
      "002542\n",
      "002545\n",
      "002554\n",
      "002555\n",
      "002558\n",
      "002561\n",
      "002563\n",
      "002564\n",
      "002565\n",
      "002566\n",
      "002572\n",
      "002584\n",
      "002585\n",
      "002589\n",
      "002595\n",
      "002611\n",
      "002613\n",
      "002625\n",
      "002632\n",
      "002633\n",
      "002636\n",
      "002637\n",
      "002641\n",
      "002643\n",
      "002649\n",
      "002657\n",
      "002658\n",
      "002662\n",
      "002668\n",
      "002669\n",
      "002675\n",
      "002678\n",
      "002689\n",
      "002691\n",
      "002693\n",
      "002696\n",
      "002710\n",
      "002713\n",
      "002714\n",
      "002715\n",
      "002723\n",
      "002727\n",
      "002730\n",
      "002735\n",
      "002741\n",
      "002744\n",
      "002751\n",
      "002755\n",
      "002759\n",
      "002767\n",
      "002776\n",
      "002786\n",
      "002795\n",
      "002798\n",
      "002803\n",
      "002804\n",
      "002807\n",
      "002815\n",
      "002826\n",
      "002827\n",
      "002834\n",
      "002838\n",
      "002841\n",
      "002842\n",
      "002855\n",
      "002858\n",
      "002859\n",
      "002864\n",
      "002867\n",
      "002870\n",
      "002879\n",
      "002881\n",
      "002884\n",
      "002886\n",
      "002889\n",
      "002899\n",
      "002901\n",
      "002912\n",
      "002913\n",
      "002914\n",
      "002915\n",
      "002916\n",
      "002931\n",
      "002932\n",
      "002933\n",
      "002935\n",
      "002939\n",
      "002940\n",
      "002954\n",
      "002958\n",
      "002960\n",
      "002962\n",
      "002965\n",
      "002975\n",
      "002976\n",
      "002987\n",
      "002988\n",
      "002990\n",
      "002992\n",
      "003002\n",
      "003003\n",
      "003017\n",
      "003023\n",
      "003024\n",
      "003027\n",
      "003031\n",
      "003034\n",
      "003039\n",
      "003042\n",
      "003044\n",
      "003057\n",
      "003058\n",
      "003063\n",
      "003064\n",
      "003065\n",
      "003077\n",
      "003078\n",
      "003086\n",
      "003088\n",
      "003089\n",
      "003100\n",
      "003102\n",
      "003106\n",
      "003107\n",
      "003118\n",
      "003121\n",
      "003122\n",
      "003124\n",
      "003129\n",
      "003134\n",
      "003138\n",
      "003140\n",
      "003145\n",
      "003147\n",
      "003150\n",
      "003159\n",
      "003162\n",
      "003170\n",
      "003175\n",
      "003181\n",
      "003185\n",
      "003188\n",
      "003189\n",
      "003205\n",
      "003210\n",
      "003213\n",
      "003218\n",
      "003228\n",
      "003236\n",
      "003247\n",
      "003253\n",
      "003259\n",
      "003260\n",
      "003261\n",
      "003270\n",
      "003280\n",
      "003282\n",
      "003284\n",
      "003290\n",
      "003292\n",
      "003296\n",
      "003301\n",
      "003303\n",
      "003311\n",
      "003313\n",
      "003320\n",
      "003327\n",
      "003331\n",
      "003335\n",
      "003344\n",
      "003350\n",
      "003355\n",
      "003360\n",
      "003362\n",
      "003367\n",
      "003369\n",
      "003373\n",
      "003376\n",
      "003380\n",
      "003382\n",
      "003386\n",
      "003390\n",
      "003391\n",
      "003395\n",
      "003396\n",
      "003397\n",
      "003403\n",
      "003406\n",
      "003407\n",
      "003412\n",
      "003416\n",
      "003417\n",
      "003422\n",
      "003424\n",
      "003429\n",
      "003433\n",
      "003435\n",
      "003436\n",
      "003450\n",
      "003453\n",
      "003455\n",
      "003462\n",
      "003465\n",
      "003466\n",
      "003468\n",
      "003469\n",
      "003470\n",
      "003484\n",
      "003487\n",
      "003492\n",
      "003496\n",
      "003510\n",
      "003518\n",
      "003521\n",
      "003528\n",
      "003529\n",
      "003530\n",
      "003536\n",
      "003539\n",
      "003548\n",
      "003550\n",
      "003556\n",
      "003566\n",
      "003567\n",
      "003575\n",
      "003576\n",
      "003577\n",
      "003580\n",
      "003585\n",
      "003589\n",
      "003603\n",
      "003611\n",
      "003618\n",
      "003621\n",
      "003625\n",
      "003628\n",
      "003638\n",
      "003644\n",
      "003645\n",
      "003648\n",
      "003651\n",
      "003654\n",
      "003655\n",
      "003662\n",
      "003663\n",
      "003664\n",
      "003669\n",
      "003678\n",
      "003685\n",
      "003688\n",
      "003690\n",
      "003691\n",
      "003694\n",
      "003696\n",
      "003699\n",
      "003700\n",
      "003703\n",
      "003706\n",
      "003708\n",
      "003711\n",
      "003717\n",
      "003732\n",
      "003735\n",
      "003740\n",
      "003749\n",
      "003751\n",
      "003759\n",
      "003760\n",
      "003779\n",
      "003780\n",
      "003781\n",
      "003797\n",
      "003811\n",
      "003817\n",
      "003820\n",
      "003824\n",
      "003826\n",
      "003827\n",
      "003838\n",
      "003847\n",
      "003849\n",
      "003855\n",
      "003856\n",
      "003859\n",
      "003860\n",
      "003863\n",
      "003865\n",
      "003868\n",
      "003869\n",
      "003877\n",
      "003879\n",
      "003889\n",
      "003891\n",
      "003899\n",
      "003913\n",
      "003915\n",
      "003923\n",
      "003932\n",
      "003939\n",
      "003945\n",
      "003946\n",
      "003948\n",
      "003953\n",
      "003960\n",
      "003961\n",
      "003963\n",
      "003966\n",
      "003970\n",
      "003973\n",
      "003974\n",
      "003986\n",
      "003992\n",
      "003994\n",
      "003996\n",
      "004008\n",
      "004010\n",
      "004012\n",
      "004014\n",
      "004016\n",
      "004017\n",
      "004019\n",
      "004020\n",
      "004028\n",
      "004031\n",
      "004033\n",
      "004039\n",
      "004046\n",
      "004047\n",
      "004051\n",
      "004058\n",
      "004060\n",
      "004067\n",
      "004073\n",
      "004075\n",
      "004077\n",
      "004085\n",
      "004089\n",
      "004091\n",
      "004092\n",
      "004110\n",
      "004111\n",
      "004117\n",
      "004121\n",
      "004122\n",
      "004133\n",
      "004136\n",
      "004137\n",
      "004138\n",
      "004141\n",
      "004142\n",
      "004145\n",
      "004158\n",
      "004168\n",
      "004169\n",
      "004170\n",
      "004191\n",
      "004195\n",
      "004201\n",
      "004205\n",
      "004209\n",
      "004223\n",
      "004224\n",
      "004229\n",
      "004230\n",
      "004231\n",
      "004232\n",
      "004237\n",
      "004244\n",
      "004246\n",
      "004257\n",
      "004258\n",
      "004259\n",
      "004265\n",
      "004269\n",
      "004270\n",
      "004273\n",
      "004275\n",
      "004279\n",
      "004281\n",
      "004287\n",
      "004291\n",
      "004292\n",
      "004296\n",
      "004298\n",
      "004304\n",
      "004321\n",
      "004322\n",
      "004323\n",
      "004327\n",
      "004329\n",
      "004339\n",
      "004346\n",
      "004356\n",
      "004359\n",
      "004360\n",
      "004361\n",
      "004370\n",
      "004372\n",
      "004380\n",
      "004384\n",
      "004387\n",
      "004389\n",
      "004392\n",
      "004397\n",
      "004404\n",
      "004424\n",
      "004433\n",
      "004437\n",
      "004438\n",
      "004457\n",
      "004459\n",
      "004463\n",
      "004464\n",
      "004466\n",
      "004470\n",
      "004471\n",
      "004479\n",
      "004481\n",
      "004487\n",
      "004494\n",
      "004495\n",
      "004496\n",
      "004499\n",
      "004508\n",
      "004509\n",
      "004512\n",
      "004517\n",
      "004518\n",
      "004520\n",
      "004528\n",
      "004530\n",
      "004535\n",
      "004537\n",
      "004542\n",
      "004553\n",
      "004562\n",
      "004563\n",
      "004571\n",
      "004574\n",
      "004588\n",
      "004595\n",
      "004601\n",
      "004604\n",
      "004605\n",
      "004607\n",
      "004609\n",
      "004611\n",
      "004622\n",
      "004630\n",
      "004631\n",
      "004647\n",
      "004648\n",
      "004653\n",
      "004656\n",
      "004660\n",
      "004662\n",
      "004672\n",
      "004673\n",
      "004676\n",
      "004679\n",
      "004685\n",
      "004686\n",
      "004687\n",
      "004692\n",
      "004693\n",
      "004694\n",
      "004699\n",
      "004706\n",
      "004707\n",
      "004715\n",
      "004718\n",
      "004722\n",
      "004723\n",
      "004727\n",
      "004732\n",
      "004742\n",
      "004750\n",
      "004753\n",
      "004760\n",
      "004761\n",
      "004770\n",
      "004779\n",
      "004782\n",
      "004788\n",
      "004789\n",
      "004790\n",
      "004796\n",
      "004797\n",
      "004801\n",
      "004812\n",
      "004815\n",
      "004826\n",
      "004828\n",
      "004830\n",
      "004831\n",
      "004832\n",
      "004834\n",
      "004840\n",
      "004846\n",
      "004852\n",
      "004857\n",
      "004859\n",
      "004868\n",
      "004872\n",
      "004879\n",
      "004882\n",
      "004886\n",
      "004897\n",
      "004898\n",
      "004902\n",
      "004913\n",
      "004916\n",
      "004928\n",
      "004931\n",
      "004935\n",
      "004939\n",
      "004943\n",
      "004946\n",
      "004950\n",
      "004953\n",
      "004954\n",
      "004968\n",
      "004976\n",
      "004977\n",
      "004985\n",
      "004987\n",
      "004990\n",
      "004994\n",
      "004995\n",
      "005014\n",
      "005024\n",
      "005027\n",
      "005029\n",
      "005036\n",
      "005037\n",
      "005039\n",
      "005042\n",
      "005045\n",
      "005052\n",
      "005054\n",
      "005057\n",
      "005061\n",
      "005063\n",
      "005064\n",
      "005081\n",
      "005086\n",
      "005090\n",
      "005093\n",
      "005097\n",
      "005102\n",
      "005107\n",
      "005111\n",
      "005114\n",
      "005116\n",
      "005122\n",
      "005124\n",
      "005129\n",
      "005134\n",
      "005144\n",
      "005146\n",
      "005156\n",
      "005159\n",
      "005160\n",
      "005161\n",
      "005173\n",
      "005176\n",
      "005189\n",
      "005190\n",
      "005195\n",
      "005208\n",
      "005219\n",
      "005220\n",
      "005222\n",
      "005229\n",
      "005230\n",
      "005231\n",
      "005236\n",
      "005239\n",
      "005242\n",
      "005245\n",
      "005248\n",
      "005257\n",
      "005260\n",
      "005268\n",
      "005269\n",
      "005273\n",
      "005274\n",
      "005278\n",
      "005281\n",
      "005283\n",
      "005297\n",
      "005298\n",
      "005303\n",
      "005304\n",
      "005306\n",
      "005307\n",
      "005310\n",
      "005311\n",
      "005315\n",
      "005326\n",
      "005327\n",
      "005331\n",
      "005336\n",
      "005343\n",
      "005344\n",
      "005346\n",
      "005348\n",
      "005351\n",
      "005368\n",
      "005371\n",
      "005374\n",
      "005383\n",
      "005384\n",
      "005385\n",
      "005396\n",
      "005398\n",
      "005405\n",
      "005410\n",
      "005413\n",
      "005414\n",
      "005417\n",
      "005419\n",
      "005421\n",
      "005424\n",
      "005429\n",
      "005430\n",
      "005439\n",
      "005441\n",
      "005448\n",
      "005450\n",
      "005451\n",
      "005457\n",
      "005461\n",
      "005467\n",
      "005471\n",
      "005475\n",
      "005478\n",
      "005485\n",
      "005487\n",
      "005496\n",
      "005499\n",
      "005507\n",
      "005508\n",
      "005509\n",
      "005511\n",
      "005522\n",
      "005526\n",
      "005527\n",
      "005536\n",
      "005541\n",
      "005542\n",
      "005547\n",
      "005550\n",
      "005552\n",
      "005554\n",
      "005563\n",
      "005573\n",
      "005574\n",
      "005582\n",
      "005583\n",
      "005585\n",
      "005593\n",
      "005599\n",
      "005600\n",
      "005606\n",
      "005608\n",
      "005611\n",
      "005613\n",
      "005615\n",
      "005618\n",
      "005631\n",
      "005636\n",
      "005639\n",
      "005644\n",
      "005645\n",
      "005648\n",
      "005652\n",
      "005653\n",
      "005657\n",
      "005658\n",
      "005660\n",
      "005662\n",
      "005664\n",
      "005669\n",
      "005672\n",
      "005680\n",
      "005682\n",
      "005693\n",
      "005695\n",
      "005700\n",
      "005702\n",
      "005704\n",
      "005714\n",
      "005716\n",
      "005719\n",
      "005728\n",
      "005729\n",
      "005731\n",
      "005732\n",
      "005738\n",
      "005740\n",
      "005741\n",
      "005747\n",
      "005757\n",
      "005761\n",
      "005764\n",
      "005765\n",
      "005768\n",
      "005773\n",
      "005780\n",
      "005784\n",
      "005786\n",
      "005788\n",
      "005790\n",
      "005796\n",
      "005799\n",
      "005811\n",
      "005814\n",
      "005818\n",
      "005824\n",
      "005826\n",
      "005829\n",
      "005836\n",
      "005840\n",
      "005845\n",
      "005850\n",
      "005852\n",
      "005856\n",
      "005859\n",
      "005863\n",
      "005873\n",
      "005881\n",
      "005884\n",
      "005885\n",
      "005889\n",
      "005903\n",
      "005905\n",
      "005906\n",
      "005908\n",
      "005910\n",
      "005919\n",
      "005923\n",
      "005928\n",
      "005930\n",
      "005938\n",
      "005940\n",
      "005948\n",
      "005951\n",
      "005956\n",
      "005961\n",
      "005971\n",
      "005975\n",
      "005979\n",
      "005984\n",
      "005985\n",
      "005988\n",
      "005989\n",
      "005990\n",
      "005991\n",
      "005992\n",
      "006004\n",
      "006005\n",
      "006011\n",
      "006028\n",
      "006029\n",
      "006033\n",
      "006038\n",
      "006041\n",
      "006043\n",
      "006071\n",
      "006073\n",
      "006095\n",
      "006103\n",
      "006104\n",
      "006117\n",
      "006128\n",
      "006130\n",
      "006133\n",
      "006140\n",
      "006148\n",
      "006150\n",
      "006151\n",
      "006156\n",
      "006158\n",
      "006161\n",
      "006163\n",
      "006171\n",
      "006172\n",
      "006175\n",
      "006176\n",
      "006177\n",
      "006179\n",
      "006180\n",
      "006185\n",
      "006187\n",
      "006198\n",
      "006209\n",
      "006210\n",
      "006221\n",
      "006225\n",
      "006229\n",
      "006233\n",
      "006235\n",
      "006236\n",
      "006238\n",
      "006240\n",
      "006243\n",
      "006259\n",
      "006272\n",
      "006276\n",
      "006282\n",
      "006284\n",
      "006285\n",
      "006286\n",
      "006289\n",
      "006296\n",
      "006300\n",
      "006320\n",
      "006339\n",
      "006341\n",
      "006344\n",
      "006346\n",
      "006351\n",
      "006352\n",
      "006355\n",
      "006369\n",
      "006381\n",
      "006387\n",
      "006391\n",
      "006392\n",
      "006395\n",
      "006398\n",
      "006409\n",
      "006427\n",
      "006428\n",
      "006433\n",
      "006440\n",
      "006442\n",
      "006444\n",
      "006445\n",
      "006450\n",
      "006455\n",
      "006459\n",
      "006463\n",
      "006465\n",
      "006475\n",
      "006476\n",
      "006482\n",
      "006483\n",
      "006486\n",
      "006488\n",
      "006492\n",
      "006497\n",
      "006503\n",
      "006506\n",
      "006519\n",
      "006529\n",
      "006532\n",
      "006536\n",
      "006542\n",
      "006550\n",
      "006551\n",
      "006560\n",
      "006572\n",
      "006576\n",
      "006578\n",
      "006583\n",
      "006584\n",
      "006585\n",
      "006587\n",
      "006588\n",
      "006599\n",
      "006602\n",
      "006609\n",
      "006610\n",
      "006611\n",
      "006612\n",
      "006617\n",
      "006618\n",
      "006621\n",
      "006625\n",
      "006626\n",
      "006631\n",
      "006637\n",
      "006645\n",
      "006647\n",
      "006648\n",
      "006658\n",
      "006661\n",
      "006664\n",
      "006666\n",
      "006667\n",
      "006668\n",
      "006670\n",
      "006677\n",
      "006682\n",
      "006690\n",
      "006695\n",
      "006696\n",
      "006697\n",
      "006698\n",
      "006699\n",
      "006703\n",
      "006707\n",
      "006709\n",
      "006714\n",
      "006718\n",
      "006722\n",
      "006727\n",
      "006731\n",
      "006735\n",
      "006738\n",
      "006739\n",
      "006748\n",
      "006753\n",
      "006762\n",
      "006766\n",
      "006768\n",
      "006772\n",
      "006777\n",
      "006782\n",
      "006783\n",
      "006784\n",
      "006786\n",
      "006789\n",
      "006797\n",
      "006800\n",
      "006802\n",
      "006803\n",
      "006808\n",
      "006810\n",
      "006821\n",
      "006824\n",
      "006833\n",
      "006835\n",
      "006836\n",
      "006838\n",
      "006840\n",
      "006841\n",
      "006847\n",
      "006850\n",
      "006855\n",
      "006858\n",
      "006862\n",
      "006864\n",
      "006866\n",
      "006868\n",
      "006874\n",
      "006876\n",
      "006880\n",
      "006887\n",
      "006896\n",
      "006903\n",
      "006908\n",
      "006910\n",
      "006911\n",
      "006912\n",
      "006919\n",
      "006924\n",
      "006932\n",
      "006934\n",
      "006939\n",
      "006943\n",
      "006945\n",
      "006948\n",
      "006950\n",
      "006956\n",
      "006958\n",
      "006959\n",
      "006963\n",
      "006965\n",
      "006968\n",
      "006981\n",
      "006983\n",
      "006987\n",
      "006988\n",
      "006989\n",
      "006990\n",
      "007004\n",
      "007007\n",
      "007016\n",
      "007020\n",
      "007023\n",
      "007031\n",
      "007035\n",
      "007036\n",
      "007040\n",
      "007045\n",
      "007046\n",
      "007048\n",
      "007049\n",
      "007052\n",
      "007054\n",
      "007059\n",
      "007073\n",
      "007079\n",
      "007080\n",
      "007093\n",
      "007108\n",
      "007109\n",
      "007114\n",
      "007117\n",
      "007121\n",
      "007122\n",
      "007128\n",
      "007129\n",
      "007138\n",
      "007139\n",
      "007140\n",
      "007147\n",
      "007148\n",
      "007150\n",
      "007153\n",
      "007154\n",
      "007159\n",
      "007163\n",
      "007167\n",
      "007172\n",
      "007182\n",
      "007184\n",
      "007187\n",
      "007189\n",
      "007200\n",
      "007208\n",
      "007213\n",
      "007214\n",
      "007216\n",
      "007227\n",
      "007234\n",
      "007236\n",
      "007243\n",
      "007244\n",
      "007256\n",
      "007260\n",
      "007261\n",
      "007266\n",
      "007270\n",
      "007271\n",
      "007276\n",
      "007284\n",
      "007292\n",
      "007296\n",
      "007298\n",
      "007305\n",
      "007308\n",
      "007314\n",
      "007318\n",
      "007325\n",
      "007330\n",
      "007336\n",
      "007346\n",
      "007350\n",
      "007359\n",
      "007369\n",
      "007375\n",
      "007381\n",
      "007388\n",
      "007398\n",
      "007408\n",
      "007410\n",
      "007413\n",
      "007414\n",
      "007436\n",
      "007438\n",
      "007445\n",
      "007446\n",
      "007448\n",
      "007449\n",
      "007457\n",
      "007466\n",
      "007475\n",
      "007480\n",
      "007482\n",
      "007483\n",
      "007484\n",
      "007493\n",
      "007497\n",
      "007498\n",
      "007511\n",
      "007526\n",
      "007535\n",
      "007540\n",
      "007543\n",
      "007547\n",
      "007558\n",
      "007559\n",
      "007563\n",
      "007566\n",
      "007568\n",
      "007570\n",
      "007571\n",
      "007572\n",
      "007576\n",
      "007578\n",
      "007579\n",
      "007586\n",
      "007590\n",
      "007592\n",
      "007594\n",
      "007605\n",
      "007606\n",
      "007611\n",
      "007618\n",
      "007619\n",
      "007629\n",
      "007631\n",
      "007639\n",
      "007647\n",
      "007649\n",
      "007653\n",
      "007656\n",
      "007657\n",
      "007668\n",
      "007675\n",
      "007683\n",
      "007687\n",
      "007694\n",
      "007697\n",
      "007699\n",
      "007709\n",
      "007712\n",
      "007718\n",
      "007723\n",
      "007727\n",
      "007735\n",
      "007745\n",
      "007746\n",
      "007748\n",
      "007751\n",
      "007760\n",
      "007763\n",
      "007768\n",
      "007777\n",
      "007786\n",
      "007793\n",
      "007795\n",
      "007799\n",
      "007809\n",
      "007810\n",
      "007813\n",
      "007814\n",
      "007815\n",
      "007821\n",
      "007836\n",
      "007841\n",
      "007856\n",
      "007872\n",
      "007877\n",
      "007878\n",
      "007884\n",
      "007889\n",
      "007897\n",
      "007901\n",
      "007902\n",
      "007910\n",
      "007911\n",
      "007915\n",
      "007919\n",
      "007928\n",
      "007931\n",
      "007932\n",
      "007940\n",
      "007943\n",
      "007946\n",
      "007953\n",
      "007963\n",
      "007964\n",
      "007970\n",
      "007971\n",
      "007979\n",
      "007984\n",
      "007987\n",
      "007996\n",
      "007997\n",
      "007998\n",
      "008002\n",
      "008008\n",
      "008019\n",
      "008023\n",
      "008024\n",
      "008026\n",
      "008033\n",
      "008040\n",
      "008042\n",
      "008043\n",
      "008048\n",
      "008053\n",
      "008069\n",
      "008072\n",
      "008079\n",
      "008082\n",
      "008083\n",
      "008086\n",
      "008091\n",
      "008096\n",
      "008103\n",
      "008105\n",
      "008108\n",
      "008122\n",
      "008125\n",
      "008127\n",
      "008130\n",
      "008132\n",
      "008137\n",
      "008139\n",
      "008142\n",
      "008160\n",
      "008163\n",
      "008164\n",
      "008171\n",
      "008175\n",
      "008177\n",
      "008180\n",
      "008188\n",
      "008190\n",
      "008199\n",
      "008204\n",
      "008209\n",
      "008211\n",
      "008216\n",
      "008218\n",
      "008220\n",
      "008224\n",
      "008225\n",
      "008232\n",
      "008241\n",
      "008248\n",
      "008253\n",
      "008261\n",
      "008263\n",
      "008272\n",
      "008279\n",
      "008281\n",
      "008284\n",
      "008285\n",
      "008293\n",
      "008294\n",
      "008301\n",
      "008302\n",
      "008307\n",
      "008312\n",
      "008313\n",
      "008315\n",
      "008327\n",
      "008332\n",
      "008341\n",
      "008360\n",
      "008368\n",
      "008374\n",
      "008376\n",
      "008381\n",
      "008385\n",
      "008387\n",
      "008390\n",
      "008391\n",
      "008397\n",
      "008398\n",
      "008410\n",
      "008413\n",
      "008430\n",
      "008433\n",
      "008437\n",
      "008444\n",
      "008445\n",
      "008449\n",
      "008452\n",
      "008453\n",
      "008461\n",
      "008467\n",
      "008470\n",
      "008475\n",
      "008482\n",
      "008485\n",
      "008503\n",
      "008509\n",
      "008513\n",
      "008514\n",
      "008526\n",
      "008529\n",
      "008541\n",
      "008550\n",
      "008559\n",
      "008562\n",
      "008573\n",
      "008576\n",
      "008582\n",
      "008585\n",
      "008587\n",
      "008592\n",
      "008596\n",
      "008602\n",
      "008604\n",
      "008612\n",
      "008617\n",
      "008618\n",
      "008628\n",
      "008638\n",
      "008644\n",
      "008647\n",
      "008663\n",
      "008665\n",
      "008676\n",
      "008690\n",
      "008692\n",
      "008698\n",
      "008699\n",
      "008701\n",
      "008702\n",
      "008706\n",
      "008718\n",
      "008728\n",
      "008731\n",
      "008738\n",
      "008739\n",
      "008742\n",
      "008744\n",
      "008748\n",
      "008752\n",
      "008766\n",
      "008768\n",
      "008769\n",
      "008771\n",
      "008775\n",
      "008794\n",
      "008801\n",
      "008805\n",
      "008810\n",
      "008813\n",
      "008817\n",
      "008833\n",
      "008836\n",
      "008838\n",
      "008862\n",
      "008865\n",
      "008867\n",
      "008872\n",
      "008876\n",
      "008879\n",
      "008884\n",
      "008886\n",
      "008891\n",
      "008892\n",
      "008900\n",
      "008911\n",
      "008920\n",
      "008921\n",
      "008931\n",
      "008932\n",
      "008943\n",
      "008944\n",
      "008953\n",
      "008960\n",
      "008961\n",
      "008965\n",
      "008975\n",
      "008979\n",
      "008983\n",
      "009000\n",
      "009005\n",
      "009020\n",
      "009022\n",
      "009024\n",
      "009034\n",
      "009036\n",
      "009045\n",
      "009049\n",
      "009064\n",
      "009068\n",
      "009073\n",
      "009078\n",
      "009079\n",
      "009087\n",
      "009089\n",
      "009091\n",
      "009114\n",
      "009123\n",
      "009128\n",
      "009136\n",
      "009141\n",
      "009153\n",
      "009160\n",
      "009161\n",
      "009174\n",
      "009180\n",
      "009184\n",
      "009193\n",
      "009197\n",
      "009202\n",
      "009208\n",
      "009209\n",
      "009214\n",
      "009224\n",
      "009230\n",
      "009236\n",
      "009238\n",
      "009239\n",
      "009242\n",
      "009251\n",
      "009259\n",
      "009273\n",
      "009283\n",
      "009286\n",
      "009287\n",
      "009288\n",
      "009290\n",
      "009291\n",
      "009295\n",
      "009296\n",
      "009299\n",
      "009303\n",
      "009306\n",
      "009308\n",
      "009312\n",
      "009316\n",
      "009318\n",
      "009327\n",
      "009330\n",
      "009331\n",
      "009336\n",
      "009342\n",
      "009343\n",
      "009349\n",
      "009358\n",
      "009362\n",
      "009368\n",
      "009374\n",
      "009375\n",
      "009377\n",
      "009382\n",
      "009393\n",
      "009394\n",
      "009407\n",
      "009410\n",
      "009411\n",
      "009412\n",
      "009413\n",
      "009414\n",
      "009418\n",
      "009420\n",
      "009421\n",
      "009424\n",
      "009433\n",
      "009434\n",
      "009437\n",
      "009440\n",
      "009448\n",
      "009456\n",
      "009460\n",
      "009468\n",
      "009469\n",
      "009477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009479\n",
      "009481\n",
      "009490\n",
      "009494\n",
      "009496\n",
      "009497\n",
      "009499\n",
      "009500\n",
      "009504\n",
      "009512\n",
      "009517\n",
      "009518\n",
      "009520\n",
      "009524\n",
      "009526\n",
      "009531\n",
      "009532\n",
      "009537\n",
      "009541\n",
      "009542\n",
      "009546\n",
      "009551\n",
      "009557\n",
      "009558\n",
      "009565\n",
      "009567\n",
      "009568\n",
      "009573\n",
      "009577\n",
      "009579\n",
      "009585\n",
      "009591\n",
      "009596\n",
      "009600\n",
      "009603\n",
      "009609\n",
      "009611\n",
      "009613\n",
      "009614\n",
      "009617\n",
      "009618\n",
      "009638\n",
      "009641\n",
      "009647\n",
      "009649\n",
      "009654\n",
      "009655\n",
      "009656\n",
      "009659\n",
      "009666\n",
      "009668\n",
      "009671\n",
      "009676\n",
      "009684\n",
      "009687\n",
      "009691\n",
      "009693\n",
      "009698\n",
      "009703\n",
      "009711\n",
      "009712\n",
      "009713\n",
      "009717\n",
      "009718\n",
      "009721\n",
      "009726\n",
      "009732\n",
      "009734\n",
      "009738\n",
      "009747\n",
      "009754\n",
      "009755\n",
      "009756\n",
      "009762\n",
      "009767\n",
      "009773\n",
      "009776\n",
      "009780\n",
      "009781\n",
      "009789\n",
      "009792\n",
      "009796\n",
      "009800\n",
      "009809\n",
      "009813\n",
      "009822\n",
      "009828\n",
      "009841\n",
      "009845\n",
      "009848\n",
      "009851\n",
      "009859\n",
      "009867\n",
      "009868\n",
      "009869\n",
      "009874\n",
      "009877\n",
      "009878\n",
      "009879\n",
      "009880\n",
      "009881\n",
      "009882\n",
      "009900\n",
      "009902\n",
      "009917\n",
      "009918\n",
      "009926\n",
      "009935\n",
      "009942\n",
      "009944\n",
      "009946\n",
      "009947\n",
      "009949\n",
      "009950\n",
      "009954\n",
      "009958\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 256  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "64\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5719, 0.8359, 0.7700, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazeFace(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (4): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (5): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (6): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (7): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "    )\n",
      "    (8): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (9): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BlazeFaceExtra(\n",
      "  (features): Sequential(\n",
      "    (0): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BlazeBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (act): ReLU(inplace)\n",
      "      (conv2): Sequential(\n",
      "        (0): ReLU(inplace)\n",
      "        (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.blazeface import SSD256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 256,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16],  # DBOXの大きさを決める\n",
    "    'min_sizes': [16, 32],  # DBOXの大きさを決める\n",
    "    'max_sizes': [32, 100],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD256(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD256(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra2): BlazeFaceExtra2(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-2\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface256_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n",
      "Iteration 10 || Loss: 81.5671 || 10iter: 24.0458 sec.\n",
      "Iteration 20 || Loss: 28.4457 || 10iter: 10.1055 sec.\n",
      "Iteration 30 || Loss: 30.6882 || 10iter: 8.7572 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:8738.2591 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  48.1610 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n",
      "Iteration 40 || Loss: 20.8605 || 10iter: 17.0132 sec.\n",
      "Iteration 50 || Loss: 27.1527 || 10iter: 9.3169 sec.\n",
      "Iteration 60 || Loss: 25.2783 || 10iter: 11.0028 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:790.0189 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.7755 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 70 || Loss: 18.5457 || 10iter: 10.1124 sec.\n",
      "Iteration 80 || Loss: 18.0264 || 10iter: 9.8165 sec.\n",
      "Iteration 90 || Loss: 24.9605 || 10iter: 8.0947 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:746.5560 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.1531 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 100 || Loss: 32.8353 || 10iter: 8.3875 sec.\n",
      "Iteration 110 || Loss: 21.9698 || 10iter: 11.2297 sec.\n",
      "Iteration 120 || Loss: 18.5704 || 10iter: 10.4564 sec.\n",
      "Iteration 130 || Loss: 23.5595 || 10iter: 10.1704 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:773.2974 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.9813 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 140 || Loss: 28.5998 || 10iter: 13.4943 sec.\n",
      "Iteration 150 || Loss: 20.4550 || 10iter: 14.0263 sec.\n",
      "Iteration 160 || Loss: 25.0546 || 10iter: 9.3195 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:738.4855 ||Epoch_VAL_Loss:305.9486\n",
      "timer:  54.9088 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 170 || Loss: 23.5330 || 10iter: 14.6882 sec.\n",
      "Iteration 180 || Loss: 18.3640 || 10iter: 9.7030 sec.\n",
      "Iteration 190 || Loss: 28.3247 || 10iter: 11.6481 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:721.7672 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.5571 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 200 || Loss: 29.0080 || 10iter: 7.3622 sec.\n",
      "Iteration 210 || Loss: 18.1276 || 10iter: 10.2145 sec.\n",
      "Iteration 220 || Loss: 25.2973 || 10iter: 7.7799 sec.\n",
      "Iteration 230 || Loss: 27.4437 || 10iter: 11.6853 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:751.9180 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.8332 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 240 || Loss: 19.4987 || 10iter: 17.5289 sec.\n",
      "Iteration 250 || Loss: 25.7390 || 10iter: 11.9236 sec.\n",
      "Iteration 260 || Loss: 27.4303 || 10iter: 10.3032 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:770.9382 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.2737 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 270 || Loss: 24.0438 || 10iter: 11.7014 sec.\n",
      "Iteration 280 || Loss: 22.6021 || 10iter: 13.9566 sec.\n",
      "Iteration 290 || Loss: 20.6935 || 10iter: 9.7769 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:758.4291 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.3143 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 300 || Loss: 21.8832 || 10iter: 11.0007 sec.\n",
      "Iteration 310 || Loss: 17.5450 || 10iter: 14.8367 sec.\n",
      "Iteration 320 || Loss: 20.7715 || 10iter: 10.0769 sec.\n",
      "Iteration 330 || Loss: 20.5516 || 10iter: 9.7783 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:702.4159 ||Epoch_VAL_Loss:299.8591\n",
      "timer:  61.0122 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 340 || Loss: 21.2513 || 10iter: 15.6944 sec.\n",
      "Iteration 350 || Loss: 28.3819 || 10iter: 8.7453 sec.\n",
      "Iteration 360 || Loss: 27.7173 || 10iter: 10.7012 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:719.4366 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.8246 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 370 || Loss: 18.6857 || 10iter: 13.7317 sec.\n",
      "Iteration 380 || Loss: 20.3178 || 10iter: 11.9079 sec.\n",
      "Iteration 390 || Loss: 22.6041 || 10iter: 10.7564 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:733.5396 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.4662 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 400 || Loss: 26.2453 || 10iter: 9.2811 sec.\n",
      "Iteration 410 || Loss: 28.1815 || 10iter: 15.1025 sec.\n",
      "Iteration 420 || Loss: 20.4479 || 10iter: 10.3374 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:735.3687 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.2580 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n",
      "Iteration 430 || Loss: 14.9744 || 10iter: 7.6022 sec.\n",
      "Iteration 440 || Loss: 22.6594 || 10iter: 16.8125 sec.\n",
      "Iteration 450 || Loss: 17.1654 || 10iter: 10.5518 sec.\n",
      "Iteration 460 || Loss: 19.3954 || 10iter: 8.9487 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:718.6224 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.3382 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 470 || Loss: 18.2073 || 10iter: 20.7473 sec.\n",
      "Iteration 480 || Loss: 18.1907 || 10iter: 9.5117 sec.\n",
      "Iteration 490 || Loss: 19.6907 || 10iter: 8.0847 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:693.8531 ||Epoch_VAL_Loss:284.3854\n",
      "timer:  58.5352 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 16/200\n",
      "-------------\n",
      "train\n",
      "Iteration 500 || Loss: 20.1124 || 10iter: 10.6810 sec.\n",
      "Iteration 510 || Loss: 20.8303 || 10iter: 12.3674 sec.\n",
      "Iteration 520 || Loss: 14.6516 || 10iter: 10.9306 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:686.9088 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.7933 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 17/200\n",
      "-------------\n",
      "train\n",
      "Iteration 530 || Loss: 23.9144 || 10iter: 7.7007 sec.\n",
      "Iteration 540 || Loss: 23.0223 || 10iter: 15.7849 sec.\n",
      "Iteration 550 || Loss: 23.7815 || 10iter: 10.9838 sec.\n",
      "Iteration 560 || Loss: 20.3786 || 10iter: 8.5585 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:710.2866 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.9775 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 18/200\n",
      "-------------\n",
      "train\n",
      "Iteration 570 || Loss: 26.3968 || 10iter: 22.6904 sec.\n",
      "Iteration 580 || Loss: 19.1429 || 10iter: 9.7145 sec.\n",
      "Iteration 590 || Loss: 25.1002 || 10iter: 9.7374 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:697.7946 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.7873 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 19/200\n",
      "-------------\n",
      "train\n",
      "Iteration 600 || Loss: 19.7159 || 10iter: 14.7435 sec.\n",
      "Iteration 610 || Loss: 20.9810 || 10iter: 9.5122 sec.\n",
      "Iteration 620 || Loss: 23.8621 || 10iter: 8.4473 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:685.3024 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.0966 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 20/200\n",
      "-------------\n",
      "train\n",
      "Iteration 630 || Loss: 19.0037 || 10iter: 14.4432 sec.\n",
      "Iteration 640 || Loss: 20.0932 || 10iter: 11.5840 sec.\n",
      "Iteration 650 || Loss: 20.4915 || 10iter: 9.0833 sec.\n",
      "Iteration 660 || Loss: 23.9294 || 10iter: 10.7340 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:708.8876 ||Epoch_VAL_Loss:281.3080\n",
      "timer:  57.3785 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 21/200\n",
      "-------------\n",
      "train\n",
      "Iteration 670 || Loss: 19.6584 || 10iter: 23.0111 sec.\n",
      "Iteration 680 || Loss: 16.9528 || 10iter: 11.0960 sec.\n",
      "Iteration 690 || Loss: 17.5892 || 10iter: 8.4902 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:694.1108 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.9728 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 22/200\n",
      "-------------\n",
      "train\n",
      "Iteration 700 || Loss: 19.7869 || 10iter: 18.7259 sec.\n",
      "Iteration 710 || Loss: 20.6826 || 10iter: 10.7826 sec.\n",
      "Iteration 720 || Loss: 23.1447 || 10iter: 9.9850 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:681.4753 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.5152 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 23/200\n",
      "-------------\n",
      "train\n",
      "Iteration 730 || Loss: 22.2561 || 10iter: 11.5236 sec.\n",
      "Iteration 740 || Loss: 16.4658 || 10iter: 9.7142 sec.\n",
      "Iteration 750 || Loss: 23.8778 || 10iter: 8.2798 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:696.7585 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.9637 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 24/200\n",
      "-------------\n",
      "train\n",
      "Iteration 760 || Loss: 17.6139 || 10iter: 8.4903 sec.\n",
      "Iteration 770 || Loss: 19.8080 || 10iter: 12.4670 sec.\n",
      "Iteration 780 || Loss: 19.6482 || 10iter: 8.8518 sec.\n",
      "Iteration 790 || Loss: 18.6550 || 10iter: 10.6777 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:644.4044 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.2582 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 25/200\n",
      "-------------\n",
      "train\n",
      "Iteration 800 || Loss: 25.0550 || 10iter: 14.2509 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 810 || Loss: 22.4748 || 10iter: 11.5128 sec.\n",
      "Iteration 820 || Loss: 23.0246 || 10iter: 10.6205 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:711.7038 ||Epoch_VAL_Loss:269.0385\n",
      "timer:  52.7412 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 26/200\n",
      "-------------\n",
      "train\n",
      "Iteration 830 || Loss: 22.3911 || 10iter: 16.0342 sec.\n",
      "Iteration 840 || Loss: 16.9133 || 10iter: 10.9981 sec.\n",
      "Iteration 850 || Loss: 17.6606 || 10iter: 9.9709 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:661.2042 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.7276 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 27/200\n",
      "-------------\n",
      "train\n",
      "Iteration 860 || Loss: 19.7520 || 10iter: 8.1320 sec.\n",
      "Iteration 870 || Loss: 21.2943 || 10iter: 10.1756 sec.\n",
      "Iteration 880 || Loss: 20.0147 || 10iter: 8.1477 sec.\n",
      "Iteration 890 || Loss: 22.7548 || 10iter: 10.7322 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:689.7718 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.1446 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 28/200\n",
      "-------------\n",
      "train\n",
      "Iteration 900 || Loss: 19.8972 || 10iter: 17.4439 sec.\n",
      "Iteration 910 || Loss: 21.9177 || 10iter: 10.5071 sec.\n",
      "Iteration 920 || Loss: 21.9560 || 10iter: 10.2540 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:654.1652 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.9992 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 29/200\n",
      "-------------\n",
      "train\n",
      "Iteration 930 || Loss: 21.9065 || 10iter: 11.2816 sec.\n",
      "Iteration 940 || Loss: 19.6571 || 10iter: 13.9605 sec.\n",
      "Iteration 950 || Loss: 16.1184 || 10iter: 10.1798 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:706.6385 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.2739 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 30/200\n",
      "-------------\n",
      "train\n",
      "Iteration 960 || Loss: 18.5460 || 10iter: 10.7679 sec.\n",
      "Iteration 970 || Loss: 21.0211 || 10iter: 15.0875 sec.\n",
      "Iteration 980 || Loss: 17.8112 || 10iter: 10.5984 sec.\n",
      "Iteration 990 || Loss: 23.6983 || 10iter: 9.0703 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:672.7247 ||Epoch_VAL_Loss:273.3046\n",
      "timer:  61.2541 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 31/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1000 || Loss: 18.4454 || 10iter: 15.5387 sec.\n",
      "Iteration 1010 || Loss: 17.6662 || 10iter: 8.1720 sec.\n",
      "Iteration 1020 || Loss: 16.2443 || 10iter: 11.0520 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:643.2818 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.3782 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 32/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1030 || Loss: 22.2016 || 10iter: 13.8052 sec.\n",
      "Iteration 1040 || Loss: 28.4451 || 10iter: 12.7835 sec.\n",
      "Iteration 1050 || Loss: 18.7918 || 10iter: 10.1378 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:668.4755 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.8348 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 33/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1060 || Loss: 23.6591 || 10iter: 9.9725 sec.\n",
      "Iteration 1070 || Loss: 21.9694 || 10iter: 15.1022 sec.\n",
      "Iteration 1080 || Loss: 24.6157 || 10iter: 9.7616 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:672.3957 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.5223 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 34/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1090 || Loss: 21.0767 || 10iter: 10.7860 sec.\n",
      "Iteration 1100 || Loss: 17.8215 || 10iter: 14.4800 sec.\n",
      "Iteration 1110 || Loss: 17.1232 || 10iter: 10.1209 sec.\n",
      "Iteration 1120 || Loss: 22.8321 || 10iter: 9.6512 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:627.6765 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.3728 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 35/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1130 || Loss: 20.3222 || 10iter: 17.9093 sec.\n",
      "Iteration 1140 || Loss: 20.2047 || 10iter: 9.4329 sec.\n",
      "Iteration 1150 || Loss: 26.3953 || 10iter: 7.9009 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:666.1141 ||Epoch_VAL_Loss:273.7678\n",
      "timer:  55.9056 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 36/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1160 || Loss: 20.5568 || 10iter: 10.0183 sec.\n",
      "Iteration 1170 || Loss: 18.0972 || 10iter: 13.4757 sec.\n",
      "Iteration 1180 || Loss: 15.5931 || 10iter: 10.8111 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:670.7125 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.1476 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 37/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1190 || Loss: 18.4953 || 10iter: 7.6055 sec.\n",
      "Iteration 1200 || Loss: 16.2989 || 10iter: 15.6841 sec.\n",
      "Iteration 1210 || Loss: 24.1159 || 10iter: 11.3616 sec.\n",
      "Iteration 1220 || Loss: 15.6535 || 10iter: 8.5337 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:631.3179 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.1089 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 38/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1230 || Loss: 16.0391 || 10iter: 22.7221 sec.\n",
      "Iteration 1240 || Loss: 21.9108 || 10iter: 10.3647 sec.\n",
      "Iteration 1250 || Loss: 20.9803 || 10iter: 9.8833 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:623.5586 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.2623 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 39/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1260 || Loss: 22.2973 || 10iter: 14.9400 sec.\n",
      "Iteration 1270 || Loss: 19.5164 || 10iter: 9.1224 sec.\n",
      "Iteration 1280 || Loss: 20.2140 || 10iter: 8.3511 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:660.6394 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.9800 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 40/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1290 || Loss: 23.4669 || 10iter: 13.2398 sec.\n",
      "Iteration 1300 || Loss: 18.8729 || 10iter: 12.7394 sec.\n",
      "Iteration 1310 || Loss: 20.3646 || 10iter: 8.9343 sec.\n",
      "Iteration 1320 || Loss: 14.7444 || 10iter: 10.8743 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:646.4395 ||Epoch_VAL_Loss:295.7297\n",
      "timer:  57.1763 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 41/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1330 || Loss: 16.8668 || 10iter: 22.2062 sec.\n",
      "Iteration 1340 || Loss: 21.3482 || 10iter: 11.2566 sec.\n",
      "Iteration 1350 || Loss: 13.8924 || 10iter: 8.3699 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:620.7000 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.0205 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 42/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1360 || Loss: 15.0791 || 10iter: 20.2544 sec.\n",
      "Iteration 1370 || Loss: 15.9841 || 10iter: 10.7993 sec.\n",
      "Iteration 1380 || Loss: 18.1674 || 10iter: 9.5390 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:606.9316 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.1297 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 43/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1390 || Loss: 17.7420 || 10iter: 12.4546 sec.\n",
      "Iteration 1400 || Loss: 20.7183 || 10iter: 9.3699 sec.\n",
      "Iteration 1410 || Loss: 15.2255 || 10iter: 8.4001 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:629.9578 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.7015 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 44/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1420 || Loss: 16.2434 || 10iter: 9.1528 sec.\n",
      "Iteration 1430 || Loss: 19.8409 || 10iter: 12.1592 sec.\n",
      "Iteration 1440 || Loss: 16.4060 || 10iter: 10.0785 sec.\n",
      "Iteration 1450 || Loss: 17.3544 || 10iter: 10.5973 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:636.8090 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.7803 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 45/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1460 || Loss: 16.8815 || 10iter: 13.2937 sec.\n",
      "Iteration 1470 || Loss: 17.5527 || 10iter: 13.4939 sec.\n",
      "Iteration 1480 || Loss: 16.4183 || 10iter: 9.7206 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:637.1823 ||Epoch_VAL_Loss:269.0472\n",
      "timer:  54.1937 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 46/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1490 || Loss: 18.9165 || 10iter: 15.0155 sec.\n",
      "Iteration 1500 || Loss: 18.6442 || 10iter: 10.1263 sec.\n",
      "Iteration 1510 || Loss: 18.3856 || 10iter: 10.7890 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:630.6135 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.3142 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 47/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1520 || Loss: 19.3332 || 10iter: 7.9918 sec.\n",
      "Iteration 1530 || Loss: 20.1737 || 10iter: 10.1078 sec.\n",
      "Iteration 1540 || Loss: 27.8459 || 10iter: 7.7678 sec.\n",
      "Iteration 1550 || Loss: 19.6410 || 10iter: 11.1638 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:618.4600 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.7432 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 48/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1560 || Loss: 21.6805 || 10iter: 17.2538 sec.\n",
      "Iteration 1570 || Loss: 16.4943 || 10iter: 10.8460 sec.\n",
      "Iteration 1580 || Loss: 14.4257 || 10iter: 10.2858 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:598.8278 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.8128 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 49/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1590 || Loss: 20.1070 || 10iter: 11.7893 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1600 || Loss: 20.9633 || 10iter: 13.3503 sec.\n",
      "Iteration 1610 || Loss: 19.3398 || 10iter: 10.7308 sec.\n",
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:637.2569 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7186 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 50/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1620 || Loss: 14.0267 || 10iter: 10.2344 sec.\n",
      "Iteration 1630 || Loss: 19.4465 || 10iter: 15.6448 sec.\n",
      "Iteration 1640 || Loss: 18.8537 || 10iter: 10.6346 sec.\n",
      "Iteration 1650 || Loss: 17.4327 || 10iter: 8.7033 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:615.1622 ||Epoch_VAL_Loss:270.1684\n",
      "timer:  61.0433 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 51/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1660 || Loss: 17.5467 || 10iter: 15.8594 sec.\n",
      "Iteration 1670 || Loss: 23.0072 || 10iter: 8.3667 sec.\n",
      "Iteration 1680 || Loss: 15.3119 || 10iter: 11.4726 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:632.0865 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.4340 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 52/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1690 || Loss: 18.3011 || 10iter: 13.5620 sec.\n",
      "Iteration 1700 || Loss: 15.9890 || 10iter: 11.6627 sec.\n",
      "Iteration 1710 || Loss: 20.3961 || 10iter: 11.1863 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:623.3521 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.4740 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 53/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1720 || Loss: 16.7240 || 10iter: 9.7646 sec.\n",
      "Iteration 1730 || Loss: 15.9332 || 10iter: 14.9551 sec.\n",
      "Iteration 1740 || Loss: 16.3083 || 10iter: 10.1891 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:599.7465 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.5701 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 54/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1750 || Loss: 18.7628 || 10iter: 8.0661 sec.\n",
      "Iteration 1760 || Loss: 17.4382 || 10iter: 16.3517 sec.\n",
      "Iteration 1770 || Loss: 23.0962 || 10iter: 11.0460 sec.\n",
      "Iteration 1780 || Loss: 15.5162 || 10iter: 8.7356 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:642.8359 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.3369 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 55/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1790 || Loss: 16.7208 || 10iter: 19.9798 sec.\n",
      "Iteration 1800 || Loss: 20.0829 || 10iter: 10.1503 sec.\n",
      "Iteration 1810 || Loss: 21.0598 || 10iter: 8.2211 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:622.9690 ||Epoch_VAL_Loss:270.2380\n",
      "timer:  58.7124 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 56/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1820 || Loss: 23.9840 || 10iter: 11.7100 sec.\n",
      "Iteration 1830 || Loss: 18.6637 || 10iter: 10.7802 sec.\n",
      "Iteration 1840 || Loss: 18.5396 || 10iter: 11.8255 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:608.8983 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.3368 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 57/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1850 || Loss: 22.8504 || 10iter: 7.5138 sec.\n",
      "Iteration 1860 || Loss: 18.9004 || 10iter: 15.2317 sec.\n",
      "Iteration 1870 || Loss: 18.0248 || 10iter: 11.0970 sec.\n",
      "Iteration 1880 || Loss: 20.0154 || 10iter: 8.4862 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:592.8367 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.9939 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 58/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1890 || Loss: 15.7024 || 10iter: 23.5298 sec.\n",
      "Iteration 1900 || Loss: 21.4798 || 10iter: 11.2879 sec.\n",
      "Iteration 1910 || Loss: 20.9477 || 10iter: 8.4834 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:609.5702 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  48.1093 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 59/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1920 || Loss: 20.7606 || 10iter: 16.7192 sec.\n",
      "Iteration 1930 || Loss: 19.0992 || 10iter: 9.4933 sec.\n",
      "Iteration 1940 || Loss: 17.0816 || 10iter: 8.1742 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:593.1350 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.1135 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 60/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1950 || Loss: 25.9046 || 10iter: 13.4531 sec.\n",
      "Iteration 1960 || Loss: 11.6545 || 10iter: 12.6744 sec.\n",
      "Iteration 1970 || Loss: 18.8600 || 10iter: 9.2742 sec.\n",
      "Iteration 1980 || Loss: 16.4274 || 10iter: 10.5049 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:598.6219 ||Epoch_VAL_Loss:268.5962\n",
      "timer:  56.8201 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 61/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1990 || Loss: 18.7315 || 10iter: 22.8204 sec.\n",
      "Iteration 2000 || Loss: 21.2832 || 10iter: 10.7369 sec.\n",
      "Iteration 2010 || Loss: 24.0962 || 10iter: 8.6544 sec.\n",
      "-------------\n",
      "epoch 61 || Epoch_TRAIN_Loss:611.9952 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.5402 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 62/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2020 || Loss: 20.5889 || 10iter: 20.6290 sec.\n",
      "Iteration 2030 || Loss: 17.9957 || 10iter: 9.9008 sec.\n",
      "Iteration 2040 || Loss: 17.3195 || 10iter: 9.9297 sec.\n",
      "-------------\n",
      "epoch 62 || Epoch_TRAIN_Loss:630.0186 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.7222 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 63/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2050 || Loss: 20.9373 || 10iter: 11.5133 sec.\n",
      "Iteration 2060 || Loss: 14.1244 || 10iter: 9.5763 sec.\n",
      "Iteration 2070 || Loss: 18.4310 || 10iter: 8.6715 sec.\n",
      "-------------\n",
      "epoch 63 || Epoch_TRAIN_Loss:575.1751 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.1050 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 64/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2080 || Loss: 20.8845 || 10iter: 8.9779 sec.\n",
      "Iteration 2090 || Loss: 18.8194 || 10iter: 11.9483 sec.\n",
      "Iteration 2100 || Loss: 17.4274 || 10iter: 9.2747 sec.\n",
      "Iteration 2110 || Loss: 18.0547 || 10iter: 10.9353 sec.\n",
      "-------------\n",
      "epoch 64 || Epoch_TRAIN_Loss:601.7501 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.9336 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 65/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2120 || Loss: 22.0441 || 10iter: 13.6359 sec.\n",
      "Iteration 2130 || Loss: 22.0188 || 10iter: 12.3905 sec.\n",
      "Iteration 2140 || Loss: 18.0922 || 10iter: 10.4979 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 65 || Epoch_TRAIN_Loss:609.1431 ||Epoch_VAL_Loss:274.5836\n",
      "timer:  54.3990 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 66/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2150 || Loss: 16.4071 || 10iter: 16.3714 sec.\n",
      "Iteration 2160 || Loss: 15.2555 || 10iter: 9.4632 sec.\n",
      "Iteration 2170 || Loss: 16.2114 || 10iter: 10.1409 sec.\n",
      "-------------\n",
      "epoch 66 || Epoch_TRAIN_Loss:576.3801 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.0508 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 67/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2180 || Loss: 17.5801 || 10iter: 8.2782 sec.\n",
      "Iteration 2190 || Loss: 23.0016 || 10iter: 10.0087 sec.\n",
      "Iteration 2200 || Loss: 20.5877 || 10iter: 8.5481 sec.\n",
      "Iteration 2210 || Loss: 15.5649 || 10iter: 10.5090 sec.\n",
      "-------------\n",
      "epoch 67 || Epoch_TRAIN_Loss:619.1160 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.1457 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 68/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2220 || Loss: 16.7036 || 10iter: 16.5111 sec.\n",
      "Iteration 2230 || Loss: 15.6254 || 10iter: 11.9199 sec.\n",
      "Iteration 2240 || Loss: 14.2078 || 10iter: 10.2112 sec.\n",
      "-------------\n",
      "epoch 68 || Epoch_TRAIN_Loss:573.5902 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.0529 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 69/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2250 || Loss: 16.6998 || 10iter: 11.5831 sec.\n",
      "Iteration 2260 || Loss: 20.4216 || 10iter: 14.4666 sec.\n",
      "Iteration 2270 || Loss: 14.6740 || 10iter: 9.7905 sec.\n",
      "-------------\n",
      "epoch 69 || Epoch_TRAIN_Loss:622.1452 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7678 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 70/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2280 || Loss: 18.1678 || 10iter: 10.8106 sec.\n",
      "Iteration 2290 || Loss: 18.0177 || 10iter: 16.4877 sec.\n",
      "Iteration 2300 || Loss: 22.2379 || 10iter: 10.0053 sec.\n",
      "Iteration 2310 || Loss: 19.9675 || 10iter: 8.9938 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 70 || Epoch_TRAIN_Loss:570.0288 ||Epoch_VAL_Loss:266.1434\n",
      "timer:  62.1283 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 71/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2320 || Loss: 16.5503 || 10iter: 16.0499 sec.\n",
      "Iteration 2330 || Loss: 17.6519 || 10iter: 8.6946 sec.\n",
      "Iteration 2340 || Loss: 23.8778 || 10iter: 10.8042 sec.\n",
      "-------------\n",
      "epoch 71 || Epoch_TRAIN_Loss:584.3742 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.2162 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 72/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2350 || Loss: 18.0090 || 10iter: 13.1034 sec.\n",
      "Iteration 2360 || Loss: 16.6556 || 10iter: 13.0266 sec.\n",
      "Iteration 2370 || Loss: 14.9157 || 10iter: 10.4509 sec.\n",
      "-------------\n",
      "epoch 72 || Epoch_TRAIN_Loss:564.7047 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.6535 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 73/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2380 || Loss: 19.7360 || 10iter: 10.4045 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2390 || Loss: 17.3252 || 10iter: 14.8271 sec.\n",
      "Iteration 2400 || Loss: 17.5437 || 10iter: 9.9154 sec.\n",
      "-------------\n",
      "epoch 73 || Epoch_TRAIN_Loss:596.1089 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.6380 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 74/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2410 || Loss: 20.4745 || 10iter: 10.6523 sec.\n",
      "Iteration 2420 || Loss: 17.5412 || 10iter: 16.2489 sec.\n",
      "Iteration 2430 || Loss: 17.4865 || 10iter: 8.9898 sec.\n",
      "Iteration 2440 || Loss: 18.0117 || 10iter: 9.6704 sec.\n",
      "-------------\n",
      "epoch 74 || Epoch_TRAIN_Loss:570.8829 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.8353 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 75/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2450 || Loss: 22.5227 || 10iter: 19.4072 sec.\n",
      "Iteration 2460 || Loss: 17.9949 || 10iter: 9.2984 sec.\n",
      "Iteration 2470 || Loss: 15.7306 || 10iter: 7.8510 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 75 || Epoch_TRAIN_Loss:581.2257 ||Epoch_VAL_Loss:3740.8610\n",
      "timer:  56.9266 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 76/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2480 || Loss: 18.0294 || 10iter: 10.4370 sec.\n",
      "Iteration 2490 || Loss: 18.2907 || 10iter: 12.5552 sec.\n",
      "Iteration 2500 || Loss: 13.5945 || 10iter: 11.1662 sec.\n",
      "-------------\n",
      "epoch 76 || Epoch_TRAIN_Loss:571.2223 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.9280 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 77/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2510 || Loss: 15.6737 || 10iter: 7.2876 sec.\n",
      "Iteration 2520 || Loss: 18.2602 || 10iter: 16.2030 sec.\n",
      "Iteration 2530 || Loss: 17.3435 || 10iter: 10.4861 sec.\n",
      "Iteration 2540 || Loss: 18.1033 || 10iter: 8.7862 sec.\n",
      "-------------\n",
      "epoch 77 || Epoch_TRAIN_Loss:551.3812 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.6260 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 78/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2550 || Loss: 21.3360 || 10iter: 22.9912 sec.\n",
      "Iteration 2560 || Loss: 18.3450 || 10iter: 9.8895 sec.\n",
      "Iteration 2570 || Loss: 18.1709 || 10iter: 9.9041 sec.\n",
      "-------------\n",
      "epoch 78 || Epoch_TRAIN_Loss:572.0885 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.1252 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 79/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2580 || Loss: 17.0287 || 10iter: 14.7257 sec.\n",
      "Iteration 2590 || Loss: 17.3171 || 10iter: 9.5143 sec.\n",
      "Iteration 2600 || Loss: 14.4617 || 10iter: 8.5969 sec.\n",
      "-------------\n",
      "epoch 79 || Epoch_TRAIN_Loss:577.1577 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.1191 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 80/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2610 || Loss: 20.5003 || 10iter: 12.9018 sec.\n",
      "Iteration 2620 || Loss: 16.7939 || 10iter: 11.3625 sec.\n",
      "Iteration 2630 || Loss: 19.3493 || 10iter: 9.4791 sec.\n",
      "Iteration 2640 || Loss: 17.4076 || 10iter: 10.4218 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 80 || Epoch_TRAIN_Loss:552.9580 ||Epoch_VAL_Loss:261.2603\n",
      "timer:  54.6368 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 81/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2650 || Loss: 19.4100 || 10iter: 23.6620 sec.\n",
      "Iteration 2660 || Loss: 17.8329 || 10iter: 10.7422 sec.\n",
      "Iteration 2670 || Loss: 17.2143 || 10iter: 9.4432 sec.\n",
      "-------------\n",
      "epoch 81 || Epoch_TRAIN_Loss:555.1579 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.3443 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 82/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2680 || Loss: 17.2572 || 10iter: 17.2335 sec.\n",
      "Iteration 2690 || Loss: 13.8004 || 10iter: 9.8729 sec.\n",
      "Iteration 2700 || Loss: 14.2111 || 10iter: 11.2207 sec.\n",
      "-------------\n",
      "epoch 82 || Epoch_TRAIN_Loss:588.5551 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.9239 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 83/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2710 || Loss: 21.1528 || 10iter: 10.0419 sec.\n",
      "Iteration 2720 || Loss: 20.5984 || 10iter: 9.6997 sec.\n",
      "Iteration 2730 || Loss: 17.9721 || 10iter: 8.3813 sec.\n",
      "-------------\n",
      "epoch 83 || Epoch_TRAIN_Loss:575.6883 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.0032 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 84/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2740 || Loss: 18.6087 || 10iter: 7.2821 sec.\n",
      "Iteration 2750 || Loss: 20.4158 || 10iter: 11.4862 sec.\n",
      "Iteration 2760 || Loss: 17.8235 || 10iter: 11.8412 sec.\n",
      "Iteration 2770 || Loss: 17.5213 || 10iter: 9.8489 sec.\n",
      "-------------\n",
      "epoch 84 || Epoch_TRAIN_Loss:572.8706 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.1066 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 85/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2780 || Loss: 18.0287 || 10iter: 14.4214 sec.\n",
      "Iteration 2790 || Loss: 16.5413 || 10iter: 14.3100 sec.\n",
      "Iteration 2800 || Loss: 20.6363 || 10iter: 8.8498 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 85 || Epoch_TRAIN_Loss:566.7225 ||Epoch_VAL_Loss:264.2065\n",
      "timer:  57.4931 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 86/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2810 || Loss: 17.1736 || 10iter: 13.0193 sec.\n",
      "Iteration 2820 || Loss: 15.0289 || 10iter: 10.2057 sec.\n",
      "Iteration 2830 || Loss: 13.2935 || 10iter: 12.7265 sec.\n",
      "-------------\n",
      "epoch 86 || Epoch_TRAIN_Loss:559.2890 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.0390 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 87/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2840 || Loss: 19.5219 || 10iter: 6.2490 sec.\n",
      "Iteration 2850 || Loss: 17.2265 || 10iter: 11.2926 sec.\n",
      "Iteration 2860 || Loss: 18.5634 || 10iter: 10.4713 sec.\n",
      "Iteration 2870 || Loss: 16.4950 || 10iter: 10.0340 sec.\n",
      "-------------\n",
      "epoch 87 || Epoch_TRAIN_Loss:534.0607 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.8625 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 88/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2880 || Loss: 14.3590 || 10iter: 14.6865 sec.\n",
      "Iteration 2890 || Loss: 17.5715 || 10iter: 13.4579 sec.\n",
      "Iteration 2900 || Loss: 20.9718 || 10iter: 9.1451 sec.\n",
      "-------------\n",
      "epoch 88 || Epoch_TRAIN_Loss:551.6022 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.6168 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 89/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2910 || Loss: 17.2458 || 10iter: 13.6493 sec.\n",
      "Iteration 2920 || Loss: 14.3593 || 10iter: 14.4049 sec.\n",
      "Iteration 2930 || Loss: 18.5953 || 10iter: 9.3416 sec.\n",
      "-------------\n",
      "epoch 89 || Epoch_TRAIN_Loss:562.9321 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.4866 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 90/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2940 || Loss: 20.3100 || 10iter: 14.3333 sec.\n",
      "Iteration 2950 || Loss: 18.4824 || 10iter: 14.1980 sec.\n",
      "Iteration 2960 || Loss: 17.8828 || 10iter: 8.6978 sec.\n",
      "Iteration 2970 || Loss: 19.0334 || 10iter: 10.8138 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 90 || Epoch_TRAIN_Loss:567.4713 ||Epoch_VAL_Loss:279.9915\n",
      "timer:  60.2272 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 91/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2980 || Loss: 17.7501 || 10iter: 16.3193 sec.\n",
      "Iteration 2990 || Loss: 21.9166 || 10iter: 11.3988 sec.\n",
      "Iteration 3000 || Loss: 18.7529 || 10iter: 9.9069 sec.\n",
      "-------------\n",
      "epoch 91 || Epoch_TRAIN_Loss:552.7326 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.2354 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 92/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3010 || Loss: 18.0900 || 10iter: 12.8179 sec.\n",
      "Iteration 3020 || Loss: 17.8105 || 10iter: 14.8247 sec.\n",
      "Iteration 3030 || Loss: 15.7360 || 10iter: 9.5288 sec.\n",
      "-------------\n",
      "epoch 92 || Epoch_TRAIN_Loss:548.9054 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.1903 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 93/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3040 || Loss: 16.9446 || 10iter: 13.0023 sec.\n",
      "Iteration 3050 || Loss: 18.3991 || 10iter: 14.7587 sec.\n",
      "Iteration 3060 || Loss: 16.1781 || 10iter: 9.7104 sec.\n",
      "-------------\n",
      "epoch 93 || Epoch_TRAIN_Loss:569.5853 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.8649 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 94/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3070 || Loss: 17.1264 || 10iter: 9.9003 sec.\n",
      "Iteration 3080 || Loss: 16.4090 || 10iter: 14.1833 sec.\n",
      "Iteration 3090 || Loss: 13.5662 || 10iter: 8.6733 sec.\n",
      "Iteration 3100 || Loss: 14.9783 || 10iter: 11.4934 sec.\n",
      "-------------\n",
      "epoch 94 || Epoch_TRAIN_Loss:576.9363 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.9581 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 95/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3110 || Loss: 18.4172 || 10iter: 15.4783 sec.\n",
      "Iteration 3120 || Loss: 16.0840 || 10iter: 9.6369 sec.\n",
      "Iteration 3130 || Loss: 18.4099 || 10iter: 8.3663 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 95 || Epoch_TRAIN_Loss:557.8120 ||Epoch_VAL_Loss:260.9364\n",
      "timer:  52.7195 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 96/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3140 || Loss: 15.8243 || 10iter: 12.4310 sec.\n",
      "Iteration 3150 || Loss: 17.9294 || 10iter: 14.6384 sec.\n",
      "Iteration 3160 || Loss: 17.7848 || 10iter: 10.1272 sec.\n",
      "-------------\n",
      "epoch 96 || Epoch_TRAIN_Loss:555.9275 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.9617 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 97/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3170 || Loss: 17.0434 || 10iter: 11.4927 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3180 || Loss: 13.6904 || 10iter: 15.3308 sec.\n",
      "Iteration 3190 || Loss: 15.7092 || 10iter: 9.4292 sec.\n",
      "Iteration 3200 || Loss: 16.7830 || 10iter: 10.7257 sec.\n",
      "-------------\n",
      "epoch 97 || Epoch_TRAIN_Loss:542.5671 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  48.0180 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 98/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3210 || Loss: 17.7641 || 10iter: 18.8460 sec.\n",
      "Iteration 3220 || Loss: 21.1642 || 10iter: 9.8546 sec.\n",
      "Iteration 3230 || Loss: 14.3739 || 10iter: 10.9097 sec.\n",
      "-------------\n",
      "epoch 98 || Epoch_TRAIN_Loss:556.2729 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.2607 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 99/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3240 || Loss: 16.5628 || 10iter: 11.2453 sec.\n",
      "Iteration 3250 || Loss: 14.6437 || 10iter: 9.6203 sec.\n",
      "Iteration 3260 || Loss: 14.3833 || 10iter: 9.9722 sec.\n",
      "-------------\n",
      "epoch 99 || Epoch_TRAIN_Loss:554.9253 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.5946 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 100/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3270 || Loss: 21.2495 || 10iter: 10.7097 sec.\n",
      "Iteration 3280 || Loss: 15.9312 || 10iter: 9.2943 sec.\n",
      "Iteration 3290 || Loss: 19.9408 || 10iter: 12.5110 sec.\n",
      "Iteration 3300 || Loss: 13.4816 || 10iter: 9.0704 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 100 || Epoch_TRAIN_Loss:554.5734 ||Epoch_VAL_Loss:255.8929\n",
      "timer:  52.9437 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 101/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3310 || Loss: 17.3073 || 10iter: 22.6434 sec.\n",
      "Iteration 3320 || Loss: 15.8163 || 10iter: 9.3932 sec.\n",
      "Iteration 3330 || Loss: 17.0472 || 10iter: 11.1323 sec.\n",
      "-------------\n",
      "epoch 101 || Epoch_TRAIN_Loss:557.5946 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.7402 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 102/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3340 || Loss: 15.3879 || 10iter: 14.2211 sec.\n",
      "Iteration 3350 || Loss: 14.6784 || 10iter: 11.1906 sec.\n",
      "Iteration 3360 || Loss: 17.5791 || 10iter: 10.5303 sec.\n",
      "-------------\n",
      "epoch 102 || Epoch_TRAIN_Loss:543.2982 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.3456 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 103/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3370 || Loss: 17.5899 || 10iter: 9.1459 sec.\n",
      "Iteration 3380 || Loss: 19.2083 || 10iter: 9.8789 sec.\n",
      "Iteration 3390 || Loss: 16.5344 || 10iter: 9.5736 sec.\n",
      "-------------\n",
      "epoch 103 || Epoch_TRAIN_Loss:552.8392 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.4605 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 104/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3400 || Loss: 16.8895 || 10iter: 7.3009 sec.\n",
      "Iteration 3410 || Loss: 17.2232 || 10iter: 10.6076 sec.\n",
      "Iteration 3420 || Loss: 18.6540 || 10iter: 13.2455 sec.\n",
      "Iteration 3430 || Loss: 17.0832 || 10iter: 8.9455 sec.\n",
      "-------------\n",
      "epoch 104 || Epoch_TRAIN_Loss:578.3918 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.6845 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 105/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3440 || Loss: 21.4485 || 10iter: 14.7412 sec.\n",
      "Iteration 3450 || Loss: 13.4818 || 10iter: 14.1396 sec.\n",
      "Iteration 3460 || Loss: 17.2514 || 10iter: 8.9073 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 105 || Epoch_TRAIN_Loss:569.1179 ||Epoch_VAL_Loss:260.3785\n",
      "timer:  57.3253 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 106/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3470 || Loss: 16.5188 || 10iter: 12.6919 sec.\n",
      "Iteration 3480 || Loss: 15.2626 || 10iter: 9.8470 sec.\n",
      "Iteration 3490 || Loss: 17.3328 || 10iter: 12.0593 sec.\n",
      "-------------\n",
      "epoch 106 || Epoch_TRAIN_Loss:562.7965 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.8323 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 107/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3500 || Loss: 14.5116 || 10iter: 6.9168 sec.\n",
      "Iteration 3510 || Loss: 11.7372 || 10iter: 10.8045 sec.\n",
      "Iteration 3520 || Loss: 15.5479 || 10iter: 8.5073 sec.\n",
      "Iteration 3530 || Loss: 21.2809 || 10iter: 10.7330 sec.\n",
      "-------------\n",
      "epoch 107 || Epoch_TRAIN_Loss:545.9896 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  37.8395 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 108/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3540 || Loss: 14.2844 || 10iter: 15.9125 sec.\n",
      "Iteration 3550 || Loss: 17.1086 || 10iter: 12.8601 sec.\n",
      "Iteration 3560 || Loss: 15.6714 || 10iter: 9.8344 sec.\n",
      "-------------\n",
      "epoch 108 || Epoch_TRAIN_Loss:537.7984 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.9681 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 109/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3570 || Loss: 17.5606 || 10iter: 11.9638 sec.\n",
      "Iteration 3580 || Loss: 14.6555 || 10iter: 14.7282 sec.\n",
      "Iteration 3590 || Loss: 15.7251 || 10iter: 9.8209 sec.\n",
      "-------------\n",
      "epoch 109 || Epoch_TRAIN_Loss:548.0670 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.7062 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 110/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3600 || Loss: 15.5272 || 10iter: 13.2687 sec.\n",
      "Iteration 3610 || Loss: 19.1519 || 10iter: 14.1058 sec.\n",
      "Iteration 3620 || Loss: 19.8510 || 10iter: 9.6896 sec.\n",
      "Iteration 3630 || Loss: 17.8696 || 10iter: 9.5315 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 110 || Epoch_TRAIN_Loss:561.8018 ||Epoch_VAL_Loss:271.3215\n",
      "timer:  61.2418 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 111/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3640 || Loss: 14.4713 || 10iter: 16.4133 sec.\n",
      "Iteration 3650 || Loss: 16.4050 || 10iter: 8.6936 sec.\n",
      "Iteration 3660 || Loss: 14.7202 || 10iter: 10.8156 sec.\n",
      "-------------\n",
      "epoch 111 || Epoch_TRAIN_Loss:543.6034 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  38.6747 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 112/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3670 || Loss: 20.1764 || 10iter: 13.1727 sec.\n",
      "Iteration 3680 || Loss: 12.9346 || 10iter: 13.7206 sec.\n",
      "Iteration 3690 || Loss: 21.6185 || 10iter: 9.7847 sec.\n",
      "-------------\n",
      "epoch 112 || Epoch_TRAIN_Loss:572.7056 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7912 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 113/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3700 || Loss: 16.9546 || 10iter: 10.3449 sec.\n",
      "Iteration 3710 || Loss: 20.4039 || 10iter: 14.7528 sec.\n",
      "Iteration 3720 || Loss: 15.2058 || 10iter: 9.9798 sec.\n",
      "-------------\n",
      "epoch 113 || Epoch_TRAIN_Loss:528.4661 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.6062 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 114/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3730 || Loss: 14.1623 || 10iter: 9.1583 sec.\n",
      "Iteration 3740 || Loss: 16.8751 || 10iter: 16.0238 sec.\n",
      "Iteration 3750 || Loss: 19.6480 || 10iter: 9.8920 sec.\n",
      "Iteration 3760 || Loss: 14.7926 || 10iter: 9.4470 sec.\n",
      "-------------\n",
      "epoch 114 || Epoch_TRAIN_Loss:539.4703 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.9397 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 115/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3770 || Loss: 13.6192 || 10iter: 18.8783 sec.\n",
      "Iteration 3780 || Loss: 17.1609 || 10iter: 9.7925 sec.\n",
      "Iteration 3790 || Loss: 15.3942 || 10iter: 7.7917 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 115 || Epoch_TRAIN_Loss:547.0737 ||Epoch_VAL_Loss:255.4700\n",
      "timer:  56.5047 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 116/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3800 || Loss: 13.9573 || 10iter: 10.9617 sec.\n",
      "Iteration 3810 || Loss: 17.6616 || 10iter: 11.6445 sec.\n",
      "Iteration 3820 || Loss: 17.4359 || 10iter: 11.2800 sec.\n",
      "-------------\n",
      "epoch 116 || Epoch_TRAIN_Loss:528.9617 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.8740 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 117/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3830 || Loss: 18.8456 || 10iter: 6.7419 sec.\n",
      "Iteration 3840 || Loss: 14.5365 || 10iter: 16.2090 sec.\n",
      "Iteration 3850 || Loss: 21.0740 || 10iter: 11.2048 sec.\n",
      "Iteration 3860 || Loss: 16.1729 || 10iter: 8.5594 sec.\n",
      "-------------\n",
      "epoch 117 || Epoch_TRAIN_Loss:550.5301 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.5164 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 118/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3870 || Loss: 18.2110 || 10iter: 22.2282 sec.\n",
      "Iteration 3880 || Loss: 17.0255 || 10iter: 11.2626 sec.\n",
      "Iteration 3890 || Loss: 15.4214 || 10iter: 8.8588 sec.\n",
      "-------------\n",
      "epoch 118 || Epoch_TRAIN_Loss:549.5760 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.2368 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 119/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3900 || Loss: 21.5498 || 10iter: 16.4407 sec.\n",
      "Iteration 3910 || Loss: 17.4079 || 10iter: 10.0579 sec.\n",
      "Iteration 3920 || Loss: 15.3898 || 10iter: 8.2327 sec.\n",
      "-------------\n",
      "epoch 119 || Epoch_TRAIN_Loss:536.4702 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.4070 sec.\n",
      "lr is: 0.01\n",
      "-------------\n",
      "Epoch 120/200\n",
      "-------------\n",
      "train\n",
      "Iteration 3930 || Loss: 18.2331 || 10iter: 14.0611 sec.\n",
      "Iteration 3940 || Loss: 17.5535 || 10iter: 12.8154 sec.\n",
      "Iteration 3950 || Loss: 13.2076 || 10iter: 8.7575 sec.\n",
      "Iteration 3960 || Loss: 17.8760 || 10iter: 10.7929 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 120 || Epoch_TRAIN_Loss:545.1069 ||Epoch_VAL_Loss:262.2627\n",
      "timer:  58.4840 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 121/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3970 || Loss: 13.9775 || 10iter: 20.7118 sec.\n",
      "Iteration 3980 || Loss: 16.9664 || 10iter: 12.9422 sec.\n",
      "Iteration 3990 || Loss: 13.0430 || 10iter: 8.4477 sec.\n",
      "-------------\n",
      "epoch 121 || Epoch_TRAIN_Loss:526.5011 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.6458 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 122/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4000 || Loss: 16.9235 || 10iter: 20.1007 sec.\n",
      "Iteration 4010 || Loss: 14.6651 || 10iter: 11.3972 sec.\n",
      "Iteration 4020 || Loss: 18.0612 || 10iter: 8.9391 sec.\n",
      "-------------\n",
      "epoch 122 || Epoch_TRAIN_Loss:510.4020 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.4316 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 123/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4030 || Loss: 14.3651 || 10iter: 13.1026 sec.\n",
      "Iteration 4040 || Loss: 16.9409 || 10iter: 9.7502 sec.\n",
      "Iteration 4050 || Loss: 17.5814 || 10iter: 8.4886 sec.\n",
      "-------------\n",
      "epoch 123 || Epoch_TRAIN_Loss:510.1609 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.6128 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 124/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4060 || Loss: 19.0459 || 10iter: 10.0724 sec.\n",
      "Iteration 4070 || Loss: 15.9778 || 10iter: 13.3869 sec.\n",
      "Iteration 4080 || Loss: 17.7108 || 10iter: 8.5629 sec.\n",
      "Iteration 4090 || Loss: 13.3467 || 10iter: 11.1939 sec.\n",
      "-------------\n",
      "epoch 124 || Epoch_TRAIN_Loss:518.4106 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.9934 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 125/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4100 || Loss: 14.7906 || 10iter: 15.4592 sec.\n",
      "Iteration 4110 || Loss: 16.1575 || 10iter: 11.5041 sec.\n",
      "Iteration 4120 || Loss: 15.7035 || 10iter: 11.0309 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 125 || Epoch_TRAIN_Loss:517.4053 ||Epoch_VAL_Loss:248.1188\n",
      "timer:  53.9221 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 126/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4130 || Loss: 16.2620 || 10iter: 16.3520 sec.\n",
      "Iteration 4140 || Loss: 12.4715 || 10iter: 10.7219 sec.\n",
      "Iteration 4150 || Loss: 14.8994 || 10iter: 9.8220 sec.\n",
      "-------------\n",
      "epoch 126 || Epoch_TRAIN_Loss:520.9974 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.0401 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 127/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4160 || Loss: 17.5117 || 10iter: 9.4487 sec.\n",
      "Iteration 4170 || Loss: 13.0692 || 10iter: 9.6165 sec.\n",
      "Iteration 4180 || Loss: 16.9928 || 10iter: 8.8323 sec.\n",
      "Iteration 4190 || Loss: 11.9230 || 10iter: 10.3029 sec.\n",
      "-------------\n",
      "epoch 127 || Epoch_TRAIN_Loss:521.0093 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.2556 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 128/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4200 || Loss: 17.9421 || 10iter: 18.2326 sec.\n",
      "Iteration 4210 || Loss: 16.2775 || 10iter: 9.7204 sec.\n",
      "Iteration 4220 || Loss: 13.5752 || 10iter: 10.6736 sec.\n",
      "-------------\n",
      "epoch 128 || Epoch_TRAIN_Loss:522.9096 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.1709 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 129/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4230 || Loss: 14.4011 || 10iter: 11.0196 sec.\n",
      "Iteration 4240 || Loss: 21.0536 || 10iter: 11.8138 sec.\n",
      "Iteration 4250 || Loss: 14.0278 || 10iter: 11.4611 sec.\n",
      "-------------\n",
      "epoch 129 || Epoch_TRAIN_Loss:521.7417 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.3869 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 130/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4260 || Loss: 15.8494 || 10iter: 8.5330 sec.\n",
      "Iteration 4270 || Loss: 16.6877 || 10iter: 14.0325 sec.\n",
      "Iteration 4280 || Loss: 14.7086 || 10iter: 11.3157 sec.\n",
      "Iteration 4290 || Loss: 15.4249 || 10iter: 8.3772 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 130 || Epoch_TRAIN_Loss:517.6440 ||Epoch_VAL_Loss:249.4641\n",
      "timer:  57.2545 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 131/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4300 || Loss: 13.5634 || 10iter: 17.6292 sec.\n",
      "Iteration 4310 || Loss: 14.7699 || 10iter: 9.0763 sec.\n",
      "Iteration 4320 || Loss: 14.3618 || 10iter: 9.0452 sec.\n",
      "-------------\n",
      "epoch 131 || Epoch_TRAIN_Loss:504.5309 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.0169 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 132/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4330 || Loss: 13.7888 || 10iter: 16.8715 sec.\n",
      "Iteration 4340 || Loss: 11.8826 || 10iter: 9.3039 sec.\n",
      "Iteration 4350 || Loss: 16.8412 || 10iter: 10.8795 sec.\n",
      "-------------\n",
      "epoch 132 || Epoch_TRAIN_Loss:487.8723 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.7784 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 133/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4360 || Loss: 17.3927 || 10iter: 10.7101 sec.\n",
      "Iteration 4370 || Loss: 17.3043 || 10iter: 11.2362 sec.\n",
      "Iteration 4380 || Loss: 15.6091 || 10iter: 12.1615 sec.\n",
      "-------------\n",
      "epoch 133 || Epoch_TRAIN_Loss:501.1538 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.9957 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 134/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4390 || Loss: 18.8583 || 10iter: 5.3681 sec.\n",
      "Iteration 4400 || Loss: 18.9775 || 10iter: 15.7058 sec.\n",
      "Iteration 4410 || Loss: 14.3905 || 10iter: 11.4256 sec.\n",
      "Iteration 4420 || Loss: 18.0703 || 10iter: 8.5906 sec.\n",
      "-------------\n",
      "epoch 134 || Epoch_TRAIN_Loss:514.4116 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.6613 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 135/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4430 || Loss: 13.1018 || 10iter: 20.5771 sec.\n",
      "Iteration 4440 || Loss: 16.9907 || 10iter: 12.5500 sec.\n",
      "Iteration 4450 || Loss: 14.3978 || 10iter: 8.3157 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 135 || Epoch_TRAIN_Loss:506.6110 ||Epoch_VAL_Loss:249.2189\n",
      "timer:  58.1873 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 136/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4460 || Loss: 16.9650 || 10iter: 15.0540 sec.\n",
      "Iteration 4470 || Loss: 14.5145 || 10iter: 10.1642 sec.\n",
      "Iteration 4480 || Loss: 14.7598 || 10iter: 6.7304 sec.\n",
      "-------------\n",
      "epoch 136 || Epoch_TRAIN_Loss:497.0790 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  36.9931 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 137/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4490 || Loss: 16.7221 || 10iter: 5.6937 sec.\n",
      "Iteration 4500 || Loss: 14.0104 || 10iter: 8.1363 sec.\n",
      "Iteration 4510 || Loss: 16.8800 || 10iter: 6.6167 sec.\n",
      "Iteration 4520 || Loss: 12.9664 || 10iter: 6.4295 sec.\n",
      "-------------\n",
      "epoch 137 || Epoch_TRAIN_Loss:521.3580 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.4284 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 138/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4530 || Loss: 15.0840 || 10iter: 12.2513 sec.\n",
      "Iteration 4540 || Loss: 17.6555 || 10iter: 6.6243 sec.\n",
      "Iteration 4550 || Loss: 13.5660 || 10iter: 6.3406 sec.\n",
      "-------------\n",
      "epoch 138 || Epoch_TRAIN_Loss:509.6997 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  27.6771 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 139/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4560 || Loss: 17.2009 || 10iter: 10.2007 sec.\n",
      "Iteration 4570 || Loss: 20.8295 || 10iter: 7.1397 sec.\n",
      "Iteration 4580 || Loss: 14.6657 || 10iter: 6.4956 sec.\n",
      "-------------\n",
      "epoch 139 || Epoch_TRAIN_Loss:524.8704 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  28.5030 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 140/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4590 || Loss: 14.8297 || 10iter: 7.9481 sec.\n",
      "Iteration 4600 || Loss: 12.4713 || 10iter: 9.0031 sec.\n",
      "Iteration 4610 || Loss: 18.4401 || 10iter: 7.0471 sec.\n",
      "Iteration 4620 || Loss: 13.9413 || 10iter: 6.5609 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 140 || Epoch_TRAIN_Loss:493.3514 ||Epoch_VAL_Loss:245.7038\n",
      "timer:  45.1490 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 141/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4630 || Loss: 14.4134 || 10iter: 18.2151 sec.\n",
      "Iteration 4640 || Loss: 13.7732 || 10iter: 12.5093 sec.\n",
      "Iteration 4650 || Loss: 18.1454 || 10iter: 9.5560 sec.\n",
      "-------------\n",
      "epoch 141 || Epoch_TRAIN_Loss:525.9169 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.7260 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 142/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4660 || Loss: 18.0344 || 10iter: 12.8539 sec.\n",
      "Iteration 4670 || Loss: 17.9860 || 10iter: 15.3854 sec.\n",
      "Iteration 4680 || Loss: 13.8443 || 10iter: 9.7596 sec.\n",
      "-------------\n",
      "epoch 142 || Epoch_TRAIN_Loss:507.6306 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.9800 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 143/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4690 || Loss: 17.3120 || 10iter: 13.4864 sec.\n",
      "Iteration 4700 || Loss: 16.7654 || 10iter: 14.2373 sec.\n",
      "Iteration 4710 || Loss: 15.2042 || 10iter: 9.7391 sec.\n",
      "-------------\n",
      "epoch 143 || Epoch_TRAIN_Loss:506.4519 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.2533 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 144/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4720 || Loss: 15.2941 || 10iter: 9.6952 sec.\n",
      "Iteration 4730 || Loss: 15.3357 || 10iter: 13.6251 sec.\n",
      "Iteration 4740 || Loss: 13.8428 || 10iter: 8.6514 sec.\n",
      "Iteration 4750 || Loss: 18.2399 || 10iter: 8.1015 sec.\n",
      "-------------\n",
      "epoch 144 || Epoch_TRAIN_Loss:491.7210 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7285 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 145/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4760 || Loss: 13.6692 || 10iter: 21.4209 sec.\n",
      "Iteration 4770 || Loss: 14.6492 || 10iter: 11.5358 sec.\n",
      "Iteration 4780 || Loss: 14.7407 || 10iter: 9.3157 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 145 || Epoch_TRAIN_Loss:510.8362 ||Epoch_VAL_Loss:247.6289\n",
      "timer:  60.6802 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 146/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4790 || Loss: 14.0819 || 10iter: 12.1420 sec.\n",
      "Iteration 4800 || Loss: 17.4267 || 10iter: 14.8064 sec.\n",
      "Iteration 4810 || Loss: 16.3825 || 10iter: 10.5899 sec.\n",
      "-------------\n",
      "epoch 146 || Epoch_TRAIN_Loss:521.3581 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.4529 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 147/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4820 || Loss: 15.9556 || 10iter: 12.4755 sec.\n",
      "Iteration 4830 || Loss: 14.2589 || 10iter: 14.3981 sec.\n",
      "Iteration 4840 || Loss: 15.0441 || 10iter: 8.6102 sec.\n",
      "Iteration 4850 || Loss: 16.3986 || 10iter: 10.3595 sec.\n",
      "-------------\n",
      "epoch 147 || Epoch_TRAIN_Loss:527.8767 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.8753 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 148/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4860 || Loss: 13.6720 || 10iter: 18.5831 sec.\n",
      "Iteration 4870 || Loss: 13.2258 || 10iter: 9.4667 sec.\n",
      "Iteration 4880 || Loss: 12.2710 || 10iter: 7.3691 sec.\n",
      "-------------\n",
      "epoch 148 || Epoch_TRAIN_Loss:502.5972 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.4916 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 149/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4890 || Loss: 12.8132 || 10iter: 17.5976 sec.\n",
      "Iteration 4900 || Loss: 17.9506 || 10iter: 10.7860 sec.\n",
      "Iteration 4910 || Loss: 12.0543 || 10iter: 9.4304 sec.\n",
      "-------------\n",
      "epoch 149 || Epoch_TRAIN_Loss:513.9900 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.5685 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 150/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4920 || Loss: 19.0600 || 10iter: 11.3153 sec.\n",
      "Iteration 4930 || Loss: 14.2562 || 10iter: 9.4812 sec.\n",
      "Iteration 4940 || Loss: 15.3864 || 10iter: 12.4698 sec.\n",
      "Iteration 4950 || Loss: 19.1245 || 10iter: 9.1726 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 150 || Epoch_TRAIN_Loss:497.5291 ||Epoch_VAL_Loss:244.8019\n",
      "timer:  53.1381 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 151/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4960 || Loss: 18.0290 || 10iter: 23.1269 sec.\n",
      "Iteration 4970 || Loss: 14.5958 || 10iter: 9.2327 sec.\n",
      "Iteration 4980 || Loss: 11.3110 || 10iter: 10.6149 sec.\n",
      "-------------\n",
      "epoch 151 || Epoch_TRAIN_Loss:503.4661 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.8861 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 152/200\n",
      "-------------\n",
      "train\n",
      "Iteration 4990 || Loss: 15.5240 || 10iter: 15.4043 sec.\n",
      "Iteration 5000 || Loss: 14.3856 || 10iter: 9.5639 sec.\n",
      "Iteration 5010 || Loss: 16.2090 || 10iter: 8.5473 sec.\n",
      "-------------\n",
      "epoch 152 || Epoch_TRAIN_Loss:503.5097 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.5622 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 153/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5020 || Loss: 17.6780 || 10iter: 12.8652 sec.\n",
      "Iteration 5030 || Loss: 13.0052 || 10iter: 9.6434 sec.\n",
      "Iteration 5040 || Loss: 12.1978 || 10iter: 11.1173 sec.\n",
      "-------------\n",
      "epoch 153 || Epoch_TRAIN_Loss:493.6784 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.9725 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 154/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5050 || Loss: 16.0576 || 10iter: 6.5275 sec.\n",
      "Iteration 5060 || Loss: 15.4577 || 10iter: 10.3526 sec.\n",
      "Iteration 5070 || Loss: 16.0080 || 10iter: 13.5650 sec.\n",
      "Iteration 5080 || Loss: 14.9877 || 10iter: 9.1408 sec.\n",
      "-------------\n",
      "epoch 154 || Epoch_TRAIN_Loss:497.8752 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.1517 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 155/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5090 || Loss: 14.0080 || 10iter: 14.2725 sec.\n",
      "Iteration 5100 || Loss: 15.0786 || 10iter: 14.4335 sec.\n",
      "Iteration 5110 || Loss: 17.5410 || 10iter: 9.0581 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 155 || Epoch_TRAIN_Loss:499.0564 ||Epoch_VAL_Loss:247.2053\n",
      "timer:  56.9672 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 156/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5120 || Loss: 17.8140 || 10iter: 13.4019 sec.\n",
      "Iteration 5130 || Loss: 11.9800 || 10iter: 9.7622 sec.\n",
      "Iteration 5140 || Loss: 18.9091 || 10iter: 7.8039 sec.\n",
      "-------------\n",
      "epoch 156 || Epoch_TRAIN_Loss:521.0638 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  39.6228 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 157/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5150 || Loss: 20.1495 || 10iter: 10.5397 sec.\n",
      "Iteration 5160 || Loss: 16.1875 || 10iter: 11.3599 sec.\n",
      "Iteration 5170 || Loss: 16.7891 || 10iter: 9.2496 sec.\n",
      "Iteration 5180 || Loss: 13.9802 || 10iter: 10.7238 sec.\n",
      "-------------\n",
      "epoch 157 || Epoch_TRAIN_Loss:516.2359 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.7068 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 158/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5190 || Loss: 14.0367 || 10iter: 15.1939 sec.\n",
      "Iteration 5200 || Loss: 15.5742 || 10iter: 13.3983 sec.\n",
      "Iteration 5210 || Loss: 16.5688 || 10iter: 9.8289 sec.\n",
      "-------------\n",
      "epoch 158 || Epoch_TRAIN_Loss:490.9029 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7033 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 159/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5220 || Loss: 14.3928 || 10iter: 12.3587 sec.\n",
      "Iteration 5230 || Loss: 14.0069 || 10iter: 14.5089 sec.\n",
      "Iteration 5240 || Loss: 16.8130 || 10iter: 9.8215 sec.\n",
      "-------------\n",
      "epoch 159 || Epoch_TRAIN_Loss:508.8607 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.6263 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 160/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5250 || Loss: 16.2190 || 10iter: 12.3168 sec.\n",
      "Iteration 5260 || Loss: 14.7451 || 10iter: 14.3680 sec.\n",
      "Iteration 5270 || Loss: 18.1571 || 10iter: 9.7372 sec.\n",
      "Iteration 5280 || Loss: 17.2789 || 10iter: 8.4047 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 160 || Epoch_TRAIN_Loss:498.7476 ||Epoch_VAL_Loss:246.1348\n",
      "timer:  56.8192 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 161/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5290 || Loss: 15.7501 || 10iter: 21.7104 sec.\n",
      "Iteration 5300 || Loss: 13.8191 || 10iter: 9.0903 sec.\n",
      "Iteration 5310 || Loss: 13.0057 || 10iter: 10.7059 sec.\n",
      "-------------\n",
      "epoch 161 || Epoch_TRAIN_Loss:486.8773 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.1770 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 162/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5320 || Loss: 13.3117 || 10iter: 13.5871 sec.\n",
      "Iteration 5330 || Loss: 14.4777 || 10iter: 12.0251 sec.\n",
      "Iteration 5340 || Loss: 20.3207 || 10iter: 11.0727 sec.\n",
      "-------------\n",
      "epoch 162 || Epoch_TRAIN_Loss:505.3357 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7961 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 163/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5350 || Loss: 13.2296 || 10iter: 9.5623 sec.\n",
      "Iteration 5360 || Loss: 14.7015 || 10iter: 15.1023 sec.\n",
      "Iteration 5370 || Loss: 14.5237 || 10iter: 10.2508 sec.\n",
      "-------------\n",
      "epoch 163 || Epoch_TRAIN_Loss:500.9965 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.5818 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 164/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5380 || Loss: 14.5428 || 10iter: 8.1088 sec.\n",
      "Iteration 5390 || Loss: 15.8399 || 10iter: 16.1072 sec.\n",
      "Iteration 5400 || Loss: 15.5233 || 10iter: 10.3817 sec.\n",
      "Iteration 5410 || Loss: 14.3739 || 10iter: 8.7062 sec.\n",
      "-------------\n",
      "epoch 164 || Epoch_TRAIN_Loss:515.3760 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.7128 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 165/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5420 || Loss: 15.6897 || 10iter: 15.2990 sec.\n",
      "Iteration 5430 || Loss: 19.0374 || 10iter: 14.2163 sec.\n",
      "Iteration 5440 || Loss: 15.2853 || 10iter: 8.9532 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 165 || Epoch_TRAIN_Loss:503.0523 ||Epoch_VAL_Loss:245.0745\n",
      "timer:  57.9772 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 166/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5450 || Loss: 13.6295 || 10iter: 13.1223 sec.\n",
      "Iteration 5460 || Loss: 15.8260 || 10iter: 10.5974 sec.\n",
      "Iteration 5470 || Loss: 13.6612 || 10iter: 11.9465 sec.\n",
      "-------------\n",
      "epoch 166 || Epoch_TRAIN_Loss:502.9528 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.7090 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 167/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5480 || Loss: 15.0818 || 10iter: 6.9885 sec.\n",
      "Iteration 5490 || Loss: 15.9102 || 10iter: 12.6969 sec.\n",
      "Iteration 5500 || Loss: 15.2453 || 10iter: 11.8783 sec.\n",
      "Iteration 5510 || Loss: 15.3299 || 10iter: 8.8762 sec.\n",
      "-------------\n",
      "epoch 167 || Epoch_TRAIN_Loss:497.1092 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.1651 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 168/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5520 || Loss: 14.1516 || 10iter: 22.2795 sec.\n",
      "Iteration 5530 || Loss: 13.1590 || 10iter: 11.6738 sec.\n",
      "Iteration 5540 || Loss: 14.3260 || 10iter: 8.1888 sec.\n",
      "-------------\n",
      "epoch 168 || Epoch_TRAIN_Loss:500.5898 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.7080 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 169/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5550 || Loss: 13.3402 || 10iter: 12.8609 sec.\n",
      "Iteration 5560 || Loss: 15.0820 || 10iter: 14.8367 sec.\n",
      "Iteration 5570 || Loss: 13.6989 || 10iter: 9.6025 sec.\n",
      "-------------\n",
      "epoch 169 || Epoch_TRAIN_Loss:502.5866 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.2681 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 170/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5580 || Loss: 19.9757 || 10iter: 13.0584 sec.\n",
      "Iteration 5590 || Loss: 14.3168 || 10iter: 14.9969 sec.\n",
      "Iteration 5600 || Loss: 19.7331 || 10iter: 10.1278 sec.\n",
      "Iteration 5610 || Loss: 15.9746 || 10iter: 9.7071 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 170 || Epoch_TRAIN_Loss:513.2119 ||Epoch_VAL_Loss:249.7699\n",
      "timer:  62.7161 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 171/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5620 || Loss: 13.5127 || 10iter: 17.8377 sec.\n",
      "Iteration 5630 || Loss: 14.3498 || 10iter: 13.4415 sec.\n",
      "Iteration 5640 || Loss: 12.2965 || 10iter: 9.1253 sec.\n",
      "-------------\n",
      "epoch 171 || Epoch_TRAIN_Loss:494.6920 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.8588 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 172/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5650 || Loss: 17.1335 || 10iter: 17.5890 sec.\n",
      "Iteration 5660 || Loss: 18.7972 || 10iter: 13.3950 sec.\n",
      "Iteration 5670 || Loss: 11.8614 || 10iter: 8.9473 sec.\n",
      "-------------\n",
      "epoch 172 || Epoch_TRAIN_Loss:512.4548 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.0275 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 173/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5680 || Loss: 16.5337 || 10iter: 9.4908 sec.\n",
      "Iteration 5690 || Loss: 14.5343 || 10iter: 15.1640 sec.\n",
      "Iteration 5700 || Loss: 15.6599 || 10iter: 10.8222 sec.\n",
      "-------------\n",
      "epoch 173 || Epoch_TRAIN_Loss:501.5762 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.1219 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 174/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5710 || Loss: 12.0867 || 10iter: 9.4161 sec.\n",
      "Iteration 5720 || Loss: 13.8213 || 10iter: 16.5146 sec.\n",
      "Iteration 5730 || Loss: 14.8532 || 10iter: 10.2777 sec.\n",
      "Iteration 5740 || Loss: 15.0083 || 10iter: 8.8068 sec.\n",
      "-------------\n",
      "epoch 174 || Epoch_TRAIN_Loss:520.7507 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.3821 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 175/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5750 || Loss: 15.7936 || 10iter: 20.6637 sec.\n",
      "Iteration 5760 || Loss: 13.7382 || 10iter: 9.4560 sec.\n",
      "Iteration 5770 || Loss: 15.1362 || 10iter: 10.6745 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 175 || Epoch_TRAIN_Loss:493.3176 ||Epoch_VAL_Loss:243.1059\n",
      "timer:  57.4360 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 176/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5780 || Loss: 15.1575 || 10iter: 15.3330 sec.\n",
      "Iteration 5790 || Loss: 15.4981 || 10iter: 14.1182 sec.\n",
      "Iteration 5800 || Loss: 10.9777 || 10iter: 9.3747 sec.\n",
      "-------------\n",
      "epoch 176 || Epoch_TRAIN_Loss:480.1628 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.6337 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 177/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5810 || Loss: 15.1580 || 10iter: 7.6996 sec.\n",
      "Iteration 5820 || Loss: 16.0549 || 10iter: 15.3349 sec.\n",
      "Iteration 5830 || Loss: 14.0046 || 10iter: 11.6171 sec.\n",
      "Iteration 5840 || Loss: 16.3596 || 10iter: 8.7259 sec.\n",
      "-------------\n",
      "epoch 177 || Epoch_TRAIN_Loss:489.6415 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.1845 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 178/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5850 || Loss: 15.0694 || 10iter: 23.3044 sec.\n",
      "Iteration 5860 || Loss: 15.4241 || 10iter: 10.4905 sec.\n",
      "Iteration 5870 || Loss: 14.3777 || 10iter: 8.7454 sec.\n",
      "-------------\n",
      "epoch 178 || Epoch_TRAIN_Loss:505.0624 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.1541 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 179/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5880 || Loss: 14.1244 || 10iter: 17.2303 sec.\n",
      "Iteration 5890 || Loss: 14.7579 || 10iter: 10.1664 sec.\n",
      "Iteration 5900 || Loss: 12.3245 || 10iter: 10.5781 sec.\n",
      "-------------\n",
      "epoch 179 || Epoch_TRAIN_Loss:509.8681 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  45.0448 sec.\n",
      "lr is: 0.001\n",
      "-------------\n",
      "Epoch 180/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5910 || Loss: 15.3893 || 10iter: 10.0617 sec.\n",
      "Iteration 5920 || Loss: 16.3473 || 10iter: 10.5958 sec.\n",
      "Iteration 5930 || Loss: 16.9465 || 10iter: 12.8420 sec.\n",
      "Iteration 5940 || Loss: 16.6431 || 10iter: 8.7613 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 180 || Epoch_TRAIN_Loss:492.0204 ||Epoch_VAL_Loss:242.5796\n",
      "timer:  52.2315 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 181/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5950 || Loss: 13.1536 || 10iter: 19.4000 sec.\n",
      "Iteration 5960 || Loss: 12.9958 || 10iter: 13.4681 sec.\n",
      "Iteration 5970 || Loss: 15.9613 || 10iter: 8.7853 sec.\n",
      "-------------\n",
      "epoch 181 || Epoch_TRAIN_Loss:501.6170 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.1589 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 182/200\n",
      "-------------\n",
      "train\n",
      "Iteration 5980 || Loss: 13.4558 || 10iter: 19.4861 sec.\n",
      "Iteration 5990 || Loss: 15.8882 || 10iter: 12.7303 sec.\n",
      "Iteration 6000 || Loss: 14.2417 || 10iter: 8.3500 sec.\n",
      "-------------\n",
      "epoch 182 || Epoch_TRAIN_Loss:513.7282 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.3715 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 183/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6010 || Loss: 12.5530 || 10iter: 14.3635 sec.\n",
      "Iteration 6020 || Loss: 12.1990 || 10iter: 10.7366 sec.\n",
      "Iteration 6030 || Loss: 16.8423 || 10iter: 10.4639 sec.\n",
      "-------------\n",
      "epoch 183 || Epoch_TRAIN_Loss:506.9800 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  44.6314 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 184/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6040 || Loss: 12.5369 || 10iter: 6.6265 sec.\n",
      "Iteration 6050 || Loss: 12.0739 || 10iter: 10.2201 sec.\n",
      "Iteration 6060 || Loss: 15.8576 || 10iter: 13.4516 sec.\n",
      "Iteration 6070 || Loss: 12.6577 || 10iter: 9.1049 sec.\n",
      "-------------\n",
      "epoch 184 || Epoch_TRAIN_Loss:505.9709 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.0774 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 185/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6080 || Loss: 18.0468 || 10iter: 14.2646 sec.\n",
      "Iteration 6090 || Loss: 17.2188 || 10iter: 9.6483 sec.\n",
      "Iteration 6100 || Loss: 14.4037 || 10iter: 11.6932 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 185 || Epoch_TRAIN_Loss:514.4254 ||Epoch_VAL_Loss:242.4547\n",
      "timer:  50.8755 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 186/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6110 || Loss: 15.2118 || 10iter: 17.4442 sec.\n",
      "Iteration 6120 || Loss: 16.7151 || 10iter: 12.1861 sec.\n",
      "Iteration 6130 || Loss: 15.2994 || 10iter: 8.8782 sec.\n",
      "-------------\n",
      "epoch 186 || Epoch_TRAIN_Loss:507.2361 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.5508 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 187/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6140 || Loss: 13.1758 || 10iter: 9.7948 sec.\n",
      "Iteration 6150 || Loss: 12.6234 || 10iter: 11.3279 sec.\n",
      "Iteration 6160 || Loss: 10.4340 || 10iter: 10.1156 sec.\n",
      "Iteration 6170 || Loss: 14.9781 || 10iter: 10.3841 sec.\n",
      "-------------\n",
      "epoch 187 || Epoch_TRAIN_Loss:488.4791 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  42.4981 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 188/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6180 || Loss: 14.0431 || 10iter: 14.8968 sec.\n",
      "Iteration 6190 || Loss: 14.3725 || 10iter: 13.6696 sec.\n",
      "Iteration 6200 || Loss: 13.0325 || 10iter: 9.8590 sec.\n",
      "-------------\n",
      "epoch 188 || Epoch_TRAIN_Loss:488.7583 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.7558 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 189/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6210 || Loss: 16.8479 || 10iter: 12.2985 sec.\n",
      "Iteration 6220 || Loss: 16.8959 || 10iter: 9.1983 sec.\n",
      "Iteration 6230 || Loss: 13.2096 || 10iter: 13.0113 sec.\n",
      "-------------\n",
      "epoch 189 || Epoch_TRAIN_Loss:486.5992 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.0515 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 190/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6240 || Loss: 16.8526 || 10iter: 8.7836 sec.\n",
      "Iteration 6250 || Loss: 16.1997 || 10iter: 11.7387 sec.\n",
      "Iteration 6260 || Loss: 13.1999 || 10iter: 12.5652 sec.\n",
      "Iteration 6270 || Loss: 13.0438 || 10iter: 8.6996 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 190 || Epoch_TRAIN_Loss:489.8969 ||Epoch_VAL_Loss:242.4634\n",
      "timer:  56.3236 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 191/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6280 || Loss: 17.4833 || 10iter: 19.4059 sec.\n",
      "Iteration 6290 || Loss: 13.2259 || 10iter: 10.9921 sec.\n",
      "Iteration 6300 || Loss: 14.6837 || 10iter: 10.6509 sec.\n",
      "-------------\n",
      "epoch 191 || Epoch_TRAIN_Loss:500.7568 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.5844 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 192/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6310 || Loss: 15.4515 || 10iter: 12.3609 sec.\n",
      "Iteration 6320 || Loss: 15.2903 || 10iter: 14.1024 sec.\n",
      "Iteration 6330 || Loss: 14.0355 || 10iter: 9.9561 sec.\n",
      "-------------\n",
      "epoch 192 || Epoch_TRAIN_Loss:502.1288 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.3985 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 193/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6340 || Loss: 14.9446 || 10iter: 10.3124 sec.\n",
      "Iteration 6350 || Loss: 19.1058 || 10iter: 9.0337 sec.\n",
      "Iteration 6360 || Loss: 15.3531 || 10iter: 12.9847 sec.\n",
      "-------------\n",
      "epoch 193 || Epoch_TRAIN_Loss:498.3725 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.6998 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 194/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6370 || Loss: 16.9619 || 10iter: 5.0523 sec.\n",
      "Iteration 6380 || Loss: 17.9271 || 10iter: 13.0958 sec.\n",
      "Iteration 6390 || Loss: 16.5877 || 10iter: 12.8005 sec.\n",
      "Iteration 6400 || Loss: 13.1628 || 10iter: 9.1485 sec.\n",
      "-------------\n",
      "epoch 194 || Epoch_TRAIN_Loss:496.2443 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.6786 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 195/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6410 || Loss: 13.2891 || 10iter: 18.1550 sec.\n",
      "Iteration 6420 || Loss: 12.0449 || 10iter: 13.9978 sec.\n",
      "Iteration 6430 || Loss: 13.6067 || 10iter: 8.6420 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 195 || Epoch_TRAIN_Loss:483.4290 ||Epoch_VAL_Loss:242.8790\n",
      "timer:  61.6523 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 196/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6440 || Loss: 14.4422 || 10iter: 10.6578 sec.\n",
      "Iteration 6450 || Loss: 13.1332 || 10iter: 12.2377 sec.\n",
      "Iteration 6460 || Loss: 12.4888 || 10iter: 11.2230 sec.\n",
      "-------------\n",
      "epoch 196 || Epoch_TRAIN_Loss:488.9891 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.0048 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 197/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6470 || Loss: 18.1698 || 10iter: 7.6870 sec.\n",
      "Iteration 6480 || Loss: 12.9150 || 10iter: 10.9021 sec.\n",
      "Iteration 6490 || Loss: 16.7006 || 10iter: 11.9810 sec.\n",
      "Iteration 6500 || Loss: 11.5684 || 10iter: 9.7926 sec.\n",
      "-------------\n",
      "epoch 197 || Epoch_TRAIN_Loss:502.5151 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.0978 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 198/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6510 || Loss: 16.2141 || 10iter: 14.3703 sec.\n",
      "Iteration 6520 || Loss: 15.5294 || 10iter: 14.1981 sec.\n",
      "Iteration 6530 || Loss: 16.8043 || 10iter: 9.2757 sec.\n",
      "-------------\n",
      "epoch 198 || Epoch_TRAIN_Loss:499.0684 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  41.1703 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 199/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6540 || Loss: 14.2551 || 10iter: 13.4134 sec.\n",
      "Iteration 6550 || Loss: 13.7654 || 10iter: 14.9421 sec.\n",
      "Iteration 6560 || Loss: 21.5047 || 10iter: 9.4096 sec.\n",
      "-------------\n",
      "epoch 199 || Epoch_TRAIN_Loss:517.2394 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  43.5931 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 200/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6570 || Loss: 15.4647 || 10iter: 14.0916 sec.\n",
      "Iteration 6580 || Loss: 14.1316 || 10iter: 14.4087 sec.\n",
      "Iteration 6590 || Loss: 13.0355 || 10iter: 9.2150 sec.\n",
      "Iteration 6600 || Loss: 16.2373 || 10iter: 10.3194 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 200 || Epoch_TRAIN_Loss:492.5249 ||Epoch_VAL_Loss:241.7389\n",
      "timer:  62.6264 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 201/200\n",
      "-------------\n",
      "train\n",
      "Iteration 6610 || Loss: 12.0007 || 10iter: 17.2401 sec.\n",
      "Iteration 6620 || Loss: 14.6886 || 10iter: 9.5325 sec.\n",
      "Iteration 6630 || Loss: 14.2703 || 10iter: 11.1728 sec.\n",
      "-------------\n",
      "epoch 201 || Epoch_TRAIN_Loss:505.9907 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  40.6758 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
