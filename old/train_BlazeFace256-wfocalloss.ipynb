{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "os.makedirs(\"log\", exist_ok=True)\n",
    "input_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up person only VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val not found\n",
      "trainlist:  6469\n",
      "vallist:  2097\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "vocpath = os.path.join(\"..\", \"VOCdevkit\", \"VOC2007\")\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath, cls=\"person\")\n",
    "\n",
    "# extend with VOC2012\n",
    "vocpath = \"../VOCdevkit/VOC2012\"\n",
    "train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath, cls=\"person\", VOC2012=True)\n",
    "\n",
    "train_img_list.extend(train_img_list2)\n",
    "train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "# make Dataset\n",
    "voc_classes = ['person']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "\n",
    "print(\"trainlist: \", len(train_img_list))\n",
    "print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-45.5372, -81.0999, -99.0463,  ..., -76.5134, -64.2030, -64.1938],\n",
       "          [-58.1552, -68.6390, -72.4191,  ..., -65.0008,  -0.5020, -56.6581],\n",
       "          [-70.5009, -86.7548, -91.1066,  ..., -66.4612, -56.4577, -46.4708],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[ -8.1659, -45.8792, -64.0686,  ..., -64.9146, -42.8002, -31.0119],\n",
       "          [-22.4291, -34.9771, -38.3957,  ..., -46.8317,  23.1840, -19.9719],\n",
       "          [-38.7112, -54.4861, -55.6691,  ..., -39.4456, -27.3896, -16.1881],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[-57.7041, -83.0186, -96.4361,  ..., -78.8120, -49.6942, -40.5570],\n",
       "          [-59.3244, -60.4205, -64.0729,  ..., -54.4060,   8.0250, -47.0385],\n",
       "          [-72.7375, -85.3823, -92.5723,  ..., -68.3897, -51.9992, -34.6697],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]]),\n",
       " array([[0.26695842, 0.26829268, 0.43982495, 0.61707317, 0.        ],\n",
       "        [0.56236324, 0.41463415, 0.65426696, 0.73170732, 0.        ],\n",
       "        [0.50328228, 0.40731707, 0.58862144, 0.72682927, 0.        ]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "32\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# check operation\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])  # iter\n",
    "images, targets = next(batch_iterator)  # get first element\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # check targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7582, 0.5593, 0.9674, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.blazeface import SSD256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda:0\n",
      "set weights!\n"
     ]
    }
   ],
   "source": [
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 2,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 256,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [16, 8],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16],  # DBOXの大きさを決める\n",
    "    'min_sizes': [16, 32],  # DBOXの大きさを決める\n",
    "    'max_sizes': [32, 100],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD256(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSDのweightsを設定\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# set inits for loc and conf\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "print(\"set weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD256(\n",
      "  (blaze): BlazeFace(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (6): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "      (8): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48)\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (9): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra): BlazeFaceExtra(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)\n",
      "          (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (extra2): BlazeFaceExtra2(\n",
      "    (features): Sequential(\n",
      "      (0): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BlazeBlock(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (act): ReLU(inplace=True)\n",
      "        (conv2): Sequential(\n",
      "          (0): ReLU(inplace=True)\n",
      "          (1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)\n",
      "          (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(96, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device, focal=True)\n",
    "\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = 1e-4\n",
    "    for i,lr_decay_epoch in enumerate([120,180]):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('train')\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('val')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log/log_output_focal.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weights/blazeface256_focal_' +\n",
    "                       str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device： cuda:0\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 1/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/BlazeFace_Person.pytorch/utils/focalloss.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 || Loss: 3159.7498 || 10iter: 12.5271 sec.\n",
      "Iteration 20 || Loss: 3631.0842 || 10iter: 6.5335 sec.\n",
      "Iteration 30 || Loss: 2789.6321 || 10iter: 6.5990 sec.\n",
      "Iteration 40 || Loss: 3270.4949 || 10iter: 6.7237 sec.\n",
      "Iteration 50 || Loss: 3391.3289 || 10iter: 6.5214 sec.\n",
      "Iteration 60 || Loss: 3542.2898 || 10iter: 6.9441 sec.\n",
      "Iteration 70 || Loss: 3469.5723 || 10iter: 6.4193 sec.\n",
      "Iteration 80 || Loss: 3409.4910 || 10iter: 6.7932 sec.\n",
      "Iteration 90 || Loss: 2698.3599 || 10iter: 6.9819 sec.\n",
      "Iteration 100 || Loss: 3397.1628 || 10iter: 6.8270 sec.\n",
      "Iteration 110 || Loss: 3273.5764 || 10iter: 7.1126 sec.\n",
      "Iteration 120 || Loss: 2289.5771 || 10iter: 7.0161 sec.\n",
      "Iteration 130 || Loss: 3439.6101 || 10iter: 6.6665 sec.\n",
      "Iteration 140 || Loss: 2720.1807 || 10iter: 6.1559 sec.\n",
      "Iteration 150 || Loss: 3203.8037 || 10iter: 6.1873 sec.\n",
      "Iteration 160 || Loss: 2252.1763 || 10iter: 7.7409 sec.\n",
      "Iteration 170 || Loss: 3687.1772 || 10iter: 7.3464 sec.\n",
      "Iteration 180 || Loss: 2489.1870 || 10iter: 6.6487 sec.\n",
      "Iteration 190 || Loss: 2555.5957 || 10iter: 6.4510 sec.\n",
      "Iteration 200 || Loss: 2583.0801 || 10iter: 6.2855 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:564009.0042 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  142.5695 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 2/200\n",
      "-------------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/BlazeFace_Person.pytorch/utils/focalloss.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210 || Loss: 2655.8091 || 10iter: 9.3385 sec.\n",
      "Iteration 220 || Loss: 1911.7057 || 10iter: 6.3987 sec.\n",
      "Iteration 230 || Loss: 2042.7052 || 10iter: 6.5660 sec.\n",
      "Iteration 240 || Loss: 1861.6342 || 10iter: 6.3684 sec.\n",
      "Iteration 250 || Loss: 1559.8262 || 10iter: 6.5851 sec.\n",
      "Iteration 260 || Loss: 2036.4868 || 10iter: 6.2675 sec.\n",
      "Iteration 270 || Loss: 2183.6946 || 10iter: 6.7817 sec.\n",
      "Iteration 280 || Loss: 1995.1785 || 10iter: 6.6071 sec.\n",
      "Iteration 290 || Loss: 1704.7499 || 10iter: 6.6313 sec.\n",
      "Iteration 300 || Loss: 2065.6289 || 10iter: 6.5683 sec.\n",
      "Iteration 310 || Loss: 1564.0509 || 10iter: 6.8948 sec.\n",
      "Iteration 320 || Loss: 1433.5189 || 10iter: 6.5765 sec.\n",
      "Iteration 330 || Loss: 1857.1807 || 10iter: 6.1599 sec.\n",
      "Iteration 340 || Loss: 1899.5281 || 10iter: 6.2952 sec.\n",
      "Iteration 350 || Loss: 1843.9053 || 10iter: 6.5094 sec.\n",
      "Iteration 360 || Loss: 1492.8733 || 10iter: 5.8008 sec.\n",
      "Iteration 370 || Loss: 1563.2986 || 10iter: 8.8031 sec.\n",
      "Iteration 380 || Loss: 1883.1088 || 10iter: 6.5307 sec.\n",
      "Iteration 390 || Loss: 1270.3921 || 10iter: 6.4122 sec.\n",
      "Iteration 400 || Loss: 1290.6611 || 10iter: 6.2270 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:393787.1851 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  137.6729 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 3/200\n",
      "-------------\n",
      "train\n",
      "Iteration 410 || Loss: 1623.5658 || 10iter: 7.8124 sec.\n",
      "Iteration 420 || Loss: 1414.0707 || 10iter: 6.9979 sec.\n",
      "Iteration 430 || Loss: 1605.4969 || 10iter: 6.2170 sec.\n",
      "Iteration 440 || Loss: 1360.8505 || 10iter: 6.2297 sec.\n",
      "Iteration 450 || Loss: 1071.7529 || 10iter: 6.6134 sec.\n",
      "Iteration 460 || Loss: 1168.1649 || 10iter: 7.1096 sec.\n",
      "Iteration 470 || Loss: 1009.3795 || 10iter: 6.7641 sec.\n",
      "Iteration 480 || Loss: 969.5190 || 10iter: 6.2833 sec.\n",
      "Iteration 490 || Loss: 1703.4850 || 10iter: 5.8957 sec.\n",
      "Iteration 500 || Loss: 1088.9885 || 10iter: 7.0757 sec.\n",
      "Iteration 510 || Loss: 1643.6038 || 10iter: 6.1401 sec.\n",
      "Iteration 520 || Loss: 1023.1964 || 10iter: 6.6437 sec.\n",
      "Iteration 530 || Loss: 1381.4030 || 10iter: 6.5686 sec.\n",
      "Iteration 540 || Loss: 1322.6588 || 10iter: 6.3975 sec.\n",
      "Iteration 550 || Loss: 1376.8781 || 10iter: 6.6088 sec.\n",
      "Iteration 560 || Loss: 1152.7505 || 10iter: 6.3852 sec.\n",
      "Iteration 570 || Loss: 1359.6150 || 10iter: 6.0596 sec.\n",
      "Iteration 580 || Loss: 1111.5887 || 10iter: 5.4740 sec.\n",
      "Iteration 590 || Loss: 1322.0073 || 10iter: 5.1187 sec.\n",
      "Iteration 600 || Loss: 1426.7828 || 10iter: 5.0129 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:260755.9044 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  131.3386 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 4/200\n",
      "-------------\n",
      "train\n",
      "Iteration 610 || Loss: 1303.0977 || 10iter: 3.0188 sec.\n",
      "Iteration 620 || Loss: 1588.9618 || 10iter: 7.4417 sec.\n",
      "Iteration 630 || Loss: 974.1871 || 10iter: 8.0832 sec.\n",
      "Iteration 640 || Loss: 1506.7378 || 10iter: 6.9410 sec.\n",
      "Iteration 650 || Loss: 996.2121 || 10iter: 6.5586 sec.\n",
      "Iteration 660 || Loss: 1347.8746 || 10iter: 6.2987 sec.\n",
      "Iteration 670 || Loss: 1684.4830 || 10iter: 6.9099 sec.\n",
      "Iteration 680 || Loss: 1100.9484 || 10iter: 6.4413 sec.\n",
      "Iteration 690 || Loss: 856.5255 || 10iter: 6.6399 sec.\n",
      "Iteration 700 || Loss: 1226.8545 || 10iter: 6.6797 sec.\n",
      "Iteration 710 || Loss: 1992.6719 || 10iter: 6.5581 sec.\n",
      "Iteration 720 || Loss: 1089.1083 || 10iter: 6.6283 sec.\n",
      "Iteration 730 || Loss: 933.2656 || 10iter: 6.1712 sec.\n",
      "Iteration 740 || Loss: 1235.0541 || 10iter: 6.3436 sec.\n",
      "Iteration 750 || Loss: 1181.5806 || 10iter: 6.4715 sec.\n",
      "Iteration 760 || Loss: 1055.2369 || 10iter: 6.3881 sec.\n",
      "Iteration 770 || Loss: 1191.7062 || 10iter: 6.5485 sec.\n",
      "Iteration 780 || Loss: 1344.3765 || 10iter: 6.7735 sec.\n",
      "Iteration 790 || Loss: 1264.4572 || 10iter: 6.5660 sec.\n",
      "Iteration 800 || Loss: 1190.8336 || 10iter: 6.1127 sec.\n",
      "Iteration 810 || Loss: 1734.0027 || 10iter: 6.1704 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:252103.0316 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  136.5823 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 5/200\n",
      "-------------\n",
      "train\n",
      "Iteration 820 || Loss: 961.1747 || 10iter: 8.4510 sec.\n",
      "Iteration 830 || Loss: 1174.3708 || 10iter: 6.6483 sec.\n",
      "Iteration 840 || Loss: 1116.7209 || 10iter: 7.9617 sec.\n",
      "Iteration 850 || Loss: 997.6584 || 10iter: 6.6977 sec.\n",
      "Iteration 860 || Loss: 1618.9291 || 10iter: 6.3339 sec.\n",
      "Iteration 870 || Loss: 990.4062 || 10iter: 6.4382 sec.\n",
      "Iteration 880 || Loss: 1489.1063 || 10iter: 6.5742 sec.\n",
      "Iteration 890 || Loss: 972.2078 || 10iter: 6.8868 sec.\n",
      "Iteration 900 || Loss: 1785.3287 || 10iter: 7.4047 sec.\n",
      "Iteration 910 || Loss: 1461.7869 || 10iter: 6.3522 sec.\n",
      "Iteration 920 || Loss: 1322.5480 || 10iter: 6.6043 sec.\n",
      "Iteration 930 || Loss: 1481.4180 || 10iter: 6.8948 sec.\n",
      "Iteration 940 || Loss: 1338.6541 || 10iter: 6.7123 sec.\n",
      "Iteration 950 || Loss: 1179.1385 || 10iter: 5.9003 sec.\n",
      "Iteration 960 || Loss: 1179.9325 || 10iter: 6.3122 sec.\n",
      "Iteration 970 || Loss: 1178.8489 || 10iter: 6.9046 sec.\n",
      "Iteration 980 || Loss: 870.3842 || 10iter: 6.5528 sec.\n",
      "Iteration 990 || Loss: 1289.2469 || 10iter: 6.2995 sec.\n",
      "Iteration 1000 || Loss: 874.6437 || 10iter: 6.2568 sec.\n",
      "Iteration 1010 || Loss: 1523.8896 || 10iter: 6.2054 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:246908.4655 ||Epoch_VAL_Loss:83234.3337\n",
      "timer:  162.5697 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 6/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1020 || Loss: 1125.1182 || 10iter: 7.9981 sec.\n",
      "Iteration 1030 || Loss: 1015.5946 || 10iter: 6.5676 sec.\n",
      "Iteration 1040 || Loss: 1270.3724 || 10iter: 6.3144 sec.\n",
      "Iteration 1050 || Loss: 1092.8461 || 10iter: 6.0482 sec.\n",
      "Iteration 1060 || Loss: 991.8150 || 10iter: 6.8243 sec.\n",
      "Iteration 1070 || Loss: 1010.8141 || 10iter: 6.4218 sec.\n",
      "Iteration 1080 || Loss: 1120.7975 || 10iter: 6.7633 sec.\n",
      "Iteration 1090 || Loss: 1107.0072 || 10iter: 6.1544 sec.\n",
      "Iteration 1100 || Loss: 1149.0121 || 10iter: 6.5102 sec.\n",
      "Iteration 1110 || Loss: 1340.1807 || 10iter: 6.6698 sec.\n",
      "Iteration 1120 || Loss: 1464.0605 || 10iter: 6.3655 sec.\n",
      "Iteration 1130 || Loss: 1711.5552 || 10iter: 7.0334 sec.\n",
      "Iteration 1140 || Loss: 1362.0917 || 10iter: 6.3213 sec.\n",
      "Iteration 1150 || Loss: 1175.4113 || 10iter: 7.0331 sec.\n",
      "Iteration 1160 || Loss: 1109.0317 || 10iter: 6.1408 sec.\n",
      "Iteration 1170 || Loss: 1024.3000 || 10iter: 6.7405 sec.\n",
      "Iteration 1180 || Loss: 1437.7289 || 10iter: 6.9594 sec.\n",
      "Iteration 1190 || Loss: 1355.2980 || 10iter: 6.5775 sec.\n",
      "Iteration 1200 || Loss: 1133.3561 || 10iter: 6.2708 sec.\n",
      "Iteration 1210 || Loss: 1223.7968 || 10iter: 6.1708 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:246198.5227 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  136.7141 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 7/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1220 || Loss: 1289.7024 || 10iter: 5.4086 sec.\n",
      "Iteration 1230 || Loss: 1223.9252 || 10iter: 7.4498 sec.\n",
      "Iteration 1240 || Loss: 1131.6752 || 10iter: 6.6449 sec.\n",
      "Iteration 1250 || Loss: 1063.5863 || 10iter: 6.7542 sec.\n",
      "Iteration 1260 || Loss: 1601.4860 || 10iter: 6.5785 sec.\n",
      "Iteration 1270 || Loss: 1088.6896 || 10iter: 6.5188 sec.\n",
      "Iteration 1280 || Loss: 1166.9174 || 10iter: 6.7517 sec.\n",
      "Iteration 1290 || Loss: 1288.1945 || 10iter: 6.7167 sec.\n",
      "Iteration 1300 || Loss: 1057.9554 || 10iter: 6.2530 sec.\n",
      "Iteration 1310 || Loss: 1232.9314 || 10iter: 6.4954 sec.\n",
      "Iteration 1320 || Loss: 1494.6211 || 10iter: 6.3926 sec.\n",
      "Iteration 1330 || Loss: 1737.1801 || 10iter: 6.7880 sec.\n",
      "Iteration 1340 || Loss: 1549.9154 || 10iter: 6.7085 sec.\n",
      "Iteration 1350 || Loss: 1125.5381 || 10iter: 6.7857 sec.\n",
      "Iteration 1360 || Loss: 931.9286 || 10iter: 6.7406 sec.\n",
      "Iteration 1370 || Loss: 643.9492 || 10iter: 6.7842 sec.\n",
      "Iteration 1380 || Loss: 1575.0178 || 10iter: 6.4348 sec.\n",
      "Iteration 1400 || Loss: 1298.4845 || 10iter: 6.4717 sec.\n",
      "Iteration 1410 || Loss: 1420.3203 || 10iter: 5.8475 sec.\n",
      "Iteration 1420 || Loss: 1008.4954 || 10iter: 6.4415 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:243259.7816 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  138.2134 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 8/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1430 || Loss: 1257.3663 || 10iter: 10.6791 sec.\n",
      "Iteration 1440 || Loss: 870.2339 || 10iter: 6.7476 sec.\n",
      "Iteration 1450 || Loss: 1088.0222 || 10iter: 6.6851 sec.\n",
      "Iteration 1460 || Loss: 1232.3442 || 10iter: 6.2794 sec.\n",
      "Iteration 1470 || Loss: 1581.4816 || 10iter: 6.5467 sec.\n",
      "Iteration 1480 || Loss: 1804.6241 || 10iter: 6.3821 sec.\n",
      "Iteration 1490 || Loss: 880.2369 || 10iter: 6.6010 sec.\n",
      "Iteration 1500 || Loss: 1122.3257 || 10iter: 6.2508 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1510 || Loss: 1047.0333 || 10iter: 6.5160 sec.\n",
      "Iteration 1520 || Loss: 1280.1982 || 10iter: 6.7910 sec.\n",
      "Iteration 1530 || Loss: 1459.7520 || 10iter: 6.4760 sec.\n",
      "Iteration 1540 || Loss: 1306.7867 || 10iter: 6.3842 sec.\n",
      "Iteration 1550 || Loss: 1259.7910 || 10iter: 6.4871 sec.\n",
      "Iteration 1560 || Loss: 1209.4601 || 10iter: 6.7319 sec.\n",
      "Iteration 1570 || Loss: 1147.7842 || 10iter: 6.1014 sec.\n",
      "Iteration 1580 || Loss: 1363.8795 || 10iter: 7.1123 sec.\n",
      "Iteration 1590 || Loss: 1015.3784 || 10iter: 6.7016 sec.\n",
      "Iteration 1600 || Loss: 690.8978 || 10iter: 6.7860 sec.\n",
      "Iteration 1610 || Loss: 980.3287 || 10iter: 5.9830 sec.\n",
      "Iteration 1620 || Loss: 1058.1613 || 10iter: 5.9733 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:240491.0688 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  136.2437 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 9/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1630 || Loss: 1109.8073 || 10iter: 7.6954 sec.\n",
      "Iteration 1640 || Loss: 1127.0784 || 10iter: 5.8194 sec.\n",
      "Iteration 1650 || Loss: 1313.7483 || 10iter: 5.0964 sec.\n",
      "Iteration 1660 || Loss: 955.2504 || 10iter: 5.2442 sec.\n",
      "Iteration 1670 || Loss: 1433.3246 || 10iter: 7.8248 sec.\n",
      "Iteration 1680 || Loss: 760.5253 || 10iter: 7.6859 sec.\n",
      "Iteration 1690 || Loss: 1129.3553 || 10iter: 6.2238 sec.\n",
      "Iteration 1700 || Loss: 1230.8934 || 10iter: 6.3107 sec.\n",
      "Iteration 1710 || Loss: 926.7050 || 10iter: 6.3122 sec.\n",
      "Iteration 1720 || Loss: 2054.4465 || 10iter: 6.5427 sec.\n",
      "Iteration 1730 || Loss: 1388.9365 || 10iter: 6.2711 sec.\n",
      "Iteration 1740 || Loss: 1316.4919 || 10iter: 6.3499 sec.\n",
      "Iteration 1750 || Loss: 1049.8788 || 10iter: 6.3639 sec.\n",
      "Iteration 1760 || Loss: 1191.7249 || 10iter: 6.2391 sec.\n",
      "Iteration 1770 || Loss: 1092.2784 || 10iter: 6.4451 sec.\n",
      "Iteration 1780 || Loss: 1251.7212 || 10iter: 6.2267 sec.\n",
      "Iteration 1790 || Loss: 1089.6913 || 10iter: 6.0791 sec.\n",
      "Iteration 1800 || Loss: 1679.2817 || 10iter: 6.2255 sec.\n",
      "Iteration 1810 || Loss: 1615.2211 || 10iter: 6.0791 sec.\n",
      "Iteration 1820 || Loss: 1106.5629 || 10iter: 6.1507 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:238980.7661 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  131.2140 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 10/200\n",
      "-------------\n",
      "train\n",
      "Iteration 1830 || Loss: 880.4233 || 10iter: 5.9264 sec.\n",
      "Iteration 1840 || Loss: 1565.6245 || 10iter: 7.2387 sec.\n",
      "Iteration 1850 || Loss: 1161.2168 || 10iter: 6.6913 sec.\n",
      "Iteration 1860 || Loss: 917.1055 || 10iter: 6.5809 sec.\n",
      "Iteration 1870 || Loss: 1371.9862 || 10iter: 6.3214 sec.\n",
      "Iteration 1880 || Loss: 1173.9504 || 10iter: 6.1938 sec.\n",
      "Iteration 1890 || Loss: 1129.6877 || 10iter: 8.1138 sec.\n",
      "Iteration 1900 || Loss: 1502.9065 || 10iter: 7.1501 sec.\n",
      "Iteration 1910 || Loss: 1277.2010 || 10iter: 7.1831 sec.\n",
      "Iteration 1920 || Loss: 1150.9291 || 10iter: 6.4124 sec.\n",
      "Iteration 1930 || Loss: 803.4196 || 10iter: 6.5922 sec.\n",
      "Iteration 1940 || Loss: 1188.1034 || 10iter: 6.5361 sec.\n",
      "Iteration 1950 || Loss: 1247.7206 || 10iter: 6.6180 sec.\n",
      "Iteration 1960 || Loss: 998.5214 || 10iter: 6.2332 sec.\n",
      "Iteration 1970 || Loss: 1003.4034 || 10iter: 6.7414 sec.\n",
      "Iteration 1980 || Loss: 1157.4769 || 10iter: 6.5484 sec.\n",
      "Iteration 1990 || Loss: 983.8004 || 10iter: 6.5229 sec.\n",
      "Iteration 2000 || Loss: 1573.9695 || 10iter: 6.5654 sec.\n",
      "Iteration 2010 || Loss: 989.3367 || 10iter: 6.3777 sec.\n",
      "Iteration 2020 || Loss: 1131.5763 || 10iter: 6.2453 sec.\n",
      "Iteration 2030 || Loss: 317.9428 || 10iter: 5.8736 sec.\n",
      "-------------\n",
      "val\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:239724.1010 ||Epoch_VAL_Loss:78840.6747\n",
      "timer:  164.1078 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 11/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2040 || Loss: 1175.5321 || 10iter: 9.1881 sec.\n",
      "Iteration 2050 || Loss: 1455.1707 || 10iter: 7.9921 sec.\n",
      "Iteration 2060 || Loss: 1068.6461 || 10iter: 6.9011 sec.\n",
      "Iteration 2070 || Loss: 999.6673 || 10iter: 6.2635 sec.\n",
      "Iteration 2080 || Loss: 1067.3607 || 10iter: 6.4810 sec.\n",
      "Iteration 2090 || Loss: 1052.4109 || 10iter: 6.4289 sec.\n",
      "Iteration 2100 || Loss: 866.0216 || 10iter: 6.8593 sec.\n",
      "Iteration 2110 || Loss: 939.2147 || 10iter: 6.2749 sec.\n",
      "Iteration 2120 || Loss: 1401.9325 || 10iter: 6.5396 sec.\n",
      "Iteration 2130 || Loss: 1357.3444 || 10iter: 6.2901 sec.\n",
      "Iteration 2140 || Loss: 1006.0106 || 10iter: 6.1834 sec.\n",
      "Iteration 2150 || Loss: 678.8115 || 10iter: 6.3845 sec.\n",
      "Iteration 2160 || Loss: 919.7178 || 10iter: 6.6497 sec.\n",
      "Iteration 2170 || Loss: 1470.3975 || 10iter: 7.1545 sec.\n",
      "Iteration 2180 || Loss: 1024.3530 || 10iter: 6.3837 sec.\n",
      "Iteration 2190 || Loss: 1290.3866 || 10iter: 6.4793 sec.\n",
      "Iteration 2200 || Loss: 1388.6931 || 10iter: 6.3384 sec.\n",
      "Iteration 2210 || Loss: 1350.8074 || 10iter: 6.3820 sec.\n",
      "Iteration 2220 || Loss: 1268.3483 || 10iter: 6.7095 sec.\n",
      "Iteration 2230 || Loss: 1257.0887 || 10iter: 6.0251 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:235806.0894 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  135.3949 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 12/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2240 || Loss: 1015.9033 || 10iter: 9.1248 sec.\n",
      "Iteration 2250 || Loss: 1112.6346 || 10iter: 6.0315 sec.\n",
      "Iteration 2260 || Loss: 707.6398 || 10iter: 6.7495 sec.\n",
      "Iteration 2270 || Loss: 1332.0841 || 10iter: 7.9249 sec.\n",
      "Iteration 2280 || Loss: 1233.1752 || 10iter: 7.1952 sec.\n",
      "Iteration 2290 || Loss: 1032.1595 || 10iter: 6.4626 sec.\n",
      "Iteration 2300 || Loss: 1349.9208 || 10iter: 6.8157 sec.\n",
      "Iteration 2310 || Loss: 1110.8481 || 10iter: 6.7803 sec.\n",
      "Iteration 2320 || Loss: 1040.3781 || 10iter: 6.0991 sec.\n",
      "Iteration 2330 || Loss: 1227.0398 || 10iter: 6.5379 sec.\n",
      "Iteration 2340 || Loss: 1768.5209 || 10iter: 6.6624 sec.\n",
      "Iteration 2350 || Loss: 853.1844 || 10iter: 6.8263 sec.\n",
      "Iteration 2360 || Loss: 821.5781 || 10iter: 7.3098 sec.\n",
      "Iteration 2370 || Loss: 1292.3938 || 10iter: 6.3964 sec.\n",
      "Iteration 2380 || Loss: 1129.3248 || 10iter: 6.7381 sec.\n",
      "Iteration 2390 || Loss: 770.6996 || 10iter: 7.0218 sec.\n",
      "Iteration 2400 || Loss: 940.8426 || 10iter: 6.5370 sec.\n",
      "Iteration 2410 || Loss: 1235.1458 || 10iter: 6.6744 sec.\n",
      "Iteration 2420 || Loss: 1109.4717 || 10iter: 6.4599 sec.\n",
      "Iteration 2430 || Loss: 1043.6517 || 10iter: 6.2845 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:233390.6576 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  140.0007 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 13/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2440 || Loss: 1010.1714 || 10iter: 6.9645 sec.\n",
      "Iteration 2450 || Loss: 1349.2938 || 10iter: 6.8063 sec.\n",
      "Iteration 2460 || Loss: 1420.3927 || 10iter: 5.9789 sec.\n",
      "Iteration 2470 || Loss: 959.5300 || 10iter: 7.3181 sec.\n",
      "Iteration 2480 || Loss: 980.3820 || 10iter: 7.1056 sec.\n",
      "Iteration 2490 || Loss: 672.4562 || 10iter: 7.5106 sec.\n",
      "Iteration 2500 || Loss: 1261.5819 || 10iter: 6.6965 sec.\n",
      "Iteration 2510 || Loss: 1287.1854 || 10iter: 6.2932 sec.\n",
      "Iteration 2520 || Loss: 1061.7452 || 10iter: 6.7333 sec.\n",
      "Iteration 2530 || Loss: 1312.6913 || 10iter: 6.2883 sec.\n",
      "Iteration 2540 || Loss: 1048.3724 || 10iter: 6.1659 sec.\n",
      "Iteration 2550 || Loss: 1228.1127 || 10iter: 6.6925 sec.\n",
      "Iteration 2560 || Loss: 1118.1981 || 10iter: 6.7924 sec.\n",
      "Iteration 2570 || Loss: 1476.5192 || 10iter: 6.3911 sec.\n",
      "Iteration 2580 || Loss: 886.2061 || 10iter: 6.7101 sec.\n",
      "Iteration 2590 || Loss: 679.4173 || 10iter: 6.6370 sec.\n",
      "Iteration 2600 || Loss: 1122.2241 || 10iter: 6.5473 sec.\n",
      "Iteration 2610 || Loss: 1090.0081 || 10iter: 6.5521 sec.\n",
      "Iteration 2620 || Loss: 858.9784 || 10iter: 7.0045 sec.\n",
      "Iteration 2630 || Loss: 1175.5156 || 10iter: 6.3816 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:231241.5607 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  138.8875 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 14/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2640 || Loss: 680.7405 || 10iter: 4.0073 sec.\n",
      "Iteration 2650 || Loss: 1376.8954 || 10iter: 7.9452 sec.\n",
      "Iteration 2660 || Loss: 1010.8627 || 10iter: 6.1701 sec.\n",
      "Iteration 2670 || Loss: 987.2213 || 10iter: 6.2869 sec.\n",
      "Iteration 2680 || Loss: 849.1405 || 10iter: 5.1448 sec.\n",
      "Iteration 2690 || Loss: 933.8218 || 10iter: 5.6995 sec.\n",
      "Iteration 2700 || Loss: 953.9573 || 10iter: 5.3585 sec.\n",
      "Iteration 2710 || Loss: 1227.3240 || 10iter: 5.1315 sec.\n",
      "Iteration 2720 || Loss: 902.2101 || 10iter: 5.1148 sec.\n",
      "Iteration 2730 || Loss: 1110.5537 || 10iter: 8.2260 sec.\n",
      "Iteration 2740 || Loss: 1155.7767 || 10iter: 6.4333 sec.\n",
      "Iteration 2750 || Loss: 1017.9663 || 10iter: 6.5879 sec.\n",
      "Iteration 2760 || Loss: 1106.2839 || 10iter: 6.4733 sec.\n",
      "Iteration 2770 || Loss: 1036.9688 || 10iter: 6.4048 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2780 || Loss: 1004.1494 || 10iter: 6.3606 sec.\n",
      "Iteration 2790 || Loss: 1143.2438 || 10iter: 6.4900 sec.\n",
      "Iteration 2800 || Loss: 1158.5140 || 10iter: 6.3440 sec.\n",
      "Iteration 2810 || Loss: 1368.9877 || 10iter: 6.7756 sec.\n",
      "Iteration 2820 || Loss: 1322.5387 || 10iter: 6.6574 sec.\n",
      "Iteration 2830 || Loss: 1159.4125 || 10iter: 5.8592 sec.\n",
      "Iteration 2840 || Loss: 1320.0477 || 10iter: 6.1629 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:230157.3434 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  130.4956 sec.\n",
      "lr is: 0.0001\n",
      "-------------\n",
      "Epoch 15/200\n",
      "-------------\n",
      "train\n",
      "Iteration 2850 || Loss: 1080.2767 || 10iter: 9.9476 sec.\n",
      "Iteration 2860 || Loss: 834.2479 || 10iter: 6.7234 sec.\n",
      "Iteration 2870 || Loss: 879.7550 || 10iter: 7.0464 sec.\n",
      "Iteration 2880 || Loss: 1362.1244 || 10iter: 7.0415 sec.\n",
      "Iteration 2890 || Loss: 1272.7614 || 10iter: 6.6746 sec.\n",
      "Iteration 2900 || Loss: 1005.1508 || 10iter: 6.5403 sec.\n",
      "Iteration 2910 || Loss: 1226.9225 || 10iter: 6.1776 sec.\n",
      "Iteration 2920 || Loss: 807.7581 || 10iter: 6.1233 sec.\n",
      "Iteration 2930 || Loss: 913.4117 || 10iter: 6.1270 sec.\n",
      "Iteration 2940 || Loss: 1209.7155 || 10iter: 8.6899 sec.\n",
      "Iteration 2950 || Loss: 1037.3591 || 10iter: 6.4961 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
